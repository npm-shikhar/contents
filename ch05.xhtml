<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd"
  xmlns:epub="http://www.idpf.org/2007/ops">

<head>
  <link href="Style00.css" rel="stylesheet" type="text/css" />
  <link href="Style01.css" rel="stylesheet" type="text/css" />
  <link href="Style02.css" rel="stylesheet" type="text/css" />
  <link href="Style03.css" rel="stylesheet" type="text/css" />
  <style type="text/css" title="ibis-book">
    @charset "utf-8";

    #sbo-rt-content html,
    #sbo-rt-content div,
    #sbo-rt-content div,
    #sbo-rt-content span,
    #sbo-rt-content applet,
    #sbo-rt-content object,
    #sbo-rt-content iframe,
    #sbo-rt-content h1,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5,
    #sbo-rt-content h6,
    #sbo-rt-content p,
    #sbo-rt-content blockquote,
    #sbo-rt-content pre,
    #sbo-rt-content a,
    #sbo-rt-content abbr,
    #sbo-rt-content acronym,
    #sbo-rt-content address,
    #sbo-rt-content big,
    #sbo-rt-content cite,
    #sbo-rt-content code,
    #sbo-rt-content del,
    #sbo-rt-content dfn,
    #sbo-rt-content em,
    #sbo-rt-content img,
    #sbo-rt-content ins,
    #sbo-rt-content kbd,
    #sbo-rt-content q,
    #sbo-rt-content s,
    #sbo-rt-content samp,
    #sbo-rt-content small,
    #sbo-rt-content strike,
    #sbo-rt-content strong,
    #sbo-rt-content sub,
    #sbo-rt-content sup,
    #sbo-rt-content tt,
    #sbo-rt-content var,
    #sbo-rt-content b,
    #sbo-rt-content u,
    #sbo-rt-content i,
    #sbo-rt-content center,
    #sbo-rt-content dl,
    #sbo-rt-content dt,
    #sbo-rt-content dd,
    #sbo-rt-content ol,
    #sbo-rt-content ul,
    #sbo-rt-content li,
    #sbo-rt-content fieldset,
    #sbo-rt-content form,
    #sbo-rt-content label,
    #sbo-rt-content legend,
    #sbo-rt-content table,
    #sbo-rt-content caption,
    #sbo-rt-content tdiv,
    #sbo-rt-content tfoot,
    #sbo-rt-content thead,
    #sbo-rt-content tr,
    #sbo-rt-content th,
    #sbo-rt-content td,
    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content canvas,
    #sbo-rt-content details,
    #sbo-rt-content embed,
    #sbo-rt-content figure,
    #sbo-rt-content figcaption,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content output,
    #sbo-rt-content ruby,
    #sbo-rt-content section,
    #sbo-rt-content summary,
    #sbo-rt-content time,
    #sbo-rt-content mark,
    #sbo-rt-content audio,
    #sbo-rt-content video {
      margin: 0;
      padding: 0;
      border: 0;
      font-size: 100%;
      font: inherit;
      vertical-align: baseline
    }

    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content details,
    #sbo-rt-content figcaption,
    #sbo-rt-content figure,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content section {
      display: block
    }

    #sbo-rt-content div {
      line-height: 1
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style: none
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content q {
      quotes: none
    }

    #sbo-rt-content blockquote:before,
    #sbo-rt-content blockquote:after,
    #sbo-rt-content q:before,
    #sbo-rt-content q:after {
      content: none
    }

    #sbo-rt-content table {
      border-collapse: collapse;
      border-spacing: 0
    }

    @page {
      margin: 5px !important
    }

    #sbo-rt-content p {
      margin: 10px 0 0;
      line-height: 125%;
      text-align: left
    }

    #sbo-rt-content p.byline {
      text-align: left;
      margin: -33px auto 35px;
      font-style: italic;
      font-weight: bold
    }

    #sbo-rt-content div.preface p+p.byline {
      margin: 1em 0 0 !important
    }

    #sbo-rt-content div.preface p.byline+p.byline {
      margin: 0 !important
    }

    #sbo-rt-content div.sect1&gt;

    p.byline {
      margin: -.25em 0 1em
    }

    #sbo-rt-content div.sect1&gt;

    p.byline+p.byline {
      margin-top: -1em
    }

    #sbo-rt-content em {
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content em strong,
    #sbo-rt-content strong em {
      font-weight: bold;
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content strong,
    #sbo-rt-content span.bold {
      font-weight: bold
    }

    #sbo-rt-content em.replaceable {
      font-style: italic
    }

    #sbo-rt-content strong.userinput {
      font-weight: bold;
      font-style: normal
    }

    #sbo-rt-content span.bolditalic {
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content a.ulink,
    #sbo-rt-content a.xref,
    #sbo-rt-content a.email,
    #sbo-rt-content a.link,
    #sbo-rt-content a {
      text-decoration: none;
      color: #8e0012
    }

    #sbo-rt-content span.lineannotation {
      font-style: italic;
      color: #a62a2a;
      font-family: serif
    }

    #sbo-rt-content span.underline {
      text-decoration: underline
    }

    #sbo-rt-content span.strikethrough {
      text-decoration: line-through
    }

    #sbo-rt-content span.smallcaps {
      font-variant: small-caps
    }

    #sbo-rt-content span.cursor {
      background: #000;
      color: #fff
    }

    #sbo-rt-content span.smaller {
      font-size: 75%
    }

    #sbo-rt-content .boxedtext,
    #sbo-rt-content .keycap {
      border-style: solid;
      border-width: 1px;
      border-color: #000;
      padding: 1px
    }

    #sbo-rt-content span.gray50 {
      color: #7F7F7F;
    }

    #sbo-rt-content h1,
    #sbo-rt-content div.toc-title,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5 {
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      font-weight: bold;
      text-align: left;
      page-break-after: avoid !important;
      font-family: sans-serif, "DejaVuSans"
    }

    #sbo-rt-content div.toc-title {
      font-size: 1.5em;
      margin-top: 20px !important;
      margin-bottom: 30px !important
    }

    #sbo-rt-content section[data-type="sect1"] h1 {
      font-size: 1.3em;
      color: #8e0012;
      margin: 40px 0 8px 0
    }

    #sbo-rt-content section[data-type="sect2"] h2 {
      font-size: 1.1em;
      margin: 30px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect3"] h3 {
      font-size: 1em;
      color: #555;
      margin: 20px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect4"] h4 {
      font-size: 1em;
      font-weight: normal;
      font-style: italic;
      margin: 15px 0 6px 0 !important
    }

    #sbo-rt-content section[data-type="chapter"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="preface"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="appendix"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="glossary"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="bibliography"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="index"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      line-height: 1;
      margin-bottom: 50px;
      color: #000;
      padding-bottom: 10px;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content span.label,
    #sbo-rt-content span.keep-together {
      font-size: inherit;
      font-weight: inherit
    }

    #sbo-rt-content div[data-type="part"] h1 {
      font-size: 2em;
      text-align: center;
      margin-top: 0 !important;
      margin-bottom: 50px;
      padding: 50px 0 10px 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content img.width-ninety {
      width: 90%
    }

    #sbo-rt-content img {
      max-width: 95%;
      margin: 0 auto;
      padding: 0
    }

    #sbo-rt-content div.figure {
      background-color: transparent;
      text-align: center !important;
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content figure {
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.figure h6,
    #sbo-rt-content figure h6,
    #sbo-rt-content figure figcaption {
      font-size: .9rem !important;
      text-align: center;
      font-weight: normal !important;
      font-style: italic;
      font-family: serif !important;
      text-transform: none !important;
      letter-spacing: normal !important;
      color: #000 !important;
      padding-top: 10px !important;
      page-break-before: avoid
    }

    #sbo-rt-content div.informalfigure {
      text-align: center !important;
      padding: 5px 0 !important
    }

    #sbo-rt-content div.sidebar {
      margin: 15px 0 10px 0 !important;
      border: 1px solid #DCDCDC;
      background-color: #F7F7F7;
      padding: 15px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content aside[data-type="sidebar"] {
      margin: 15px 0 10px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar-title,
    #sbo-rt-content aside[data-type="sidebar"] h5 {
      font-weight: bold;
      font-size: 1em;
      font-family: sans-serif;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar ol,
    #sbo-rt-content div.sidebar ul,
    #sbo-rt-content aside[data-type="sidebar"] ol,
    #sbo-rt-content aside[data-type="sidebar"] ul {
      margin-left: 1.25em !important
    }

    #sbo-rt-content div.sidebar div.figure p.title,
    #sbo-rt-content aside[data-type="sidebar"] figcaption,
    #sbo-rt-content div.sidebar div.informalfigure div.caption {
      font-size: 90%;
      text-align: center;
      font-weight: normal;
      font-style: italic;
      font-family: serif !important;
      color: #000;
      padding: 5px !important;
      page-break-before: avoid;
      page-break-after: avoid
    }

    #sbo-rt-content div.sidebar div.tip,
    #sbo-rt-content div.sidebar div[data-type="tip"],
    #sbo-rt-content div.sidebar div.note,
    #sbo-rt-content div.sidebar div[data-type="note"],
    #sbo-rt-content div.sidebar div.warning,
    #sbo-rt-content div.sidebar div[data-type="warning"],
    #sbo-rt-content div.sidebar div[data-type="caution"],
    #sbo-rt-content div.sidebar div[data-type="important"] {
      margin: 20px auto 20px auto !important;
      font-size: 90%;
      width: 85%
    }

    #sbo-rt-content aside[data-type="sidebar"] p.byline {
      font-size: 90%;
      font-weight: bold;
      font-style: italic;
      text-align: center;
      text-indent: 0;
      margin: 5px auto 6px;
      page-break-after: avoid
    }

    #sbo-rt-content pre {
      white-space: pre-wrap;
      font-family: "Ubuntu Mono", monospace;
      margin: 25px 0 25px 20px;
      font-size: 85%;
      display: block;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content div.note pre.programlisting,
    #sbo-rt-content div.tip pre.programlisting,
    #sbo-rt-content div.warning pre.programlisting,
    #sbo-rt-content div.caution pre.programlisting,
    #sbo-rt-content div.important pre.programlisting {
      margin-bottom: 0
    }

    #sbo-rt-content code {
      font-family: "Ubuntu Mono", monospace;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content code strong em,
    #sbo-rt-content code em strong,
    #sbo-rt-content pre em strong,
    #sbo-rt-content pre strong em,
    #sbo-rt-content strong code em code,
    #sbo-rt-content em code strong code,
    #sbo-rt-content span.bolditalic code {
      font-weight: bold;
      font-style: italic;
      font-family: "Ubuntu Mono BoldItal", monospace
    }

    #sbo-rt-content code em,
    #sbo-rt-content em code,
    #sbo-rt-content pre em,
    #sbo-rt-content em.replaceable {
      font-family: "Ubuntu Mono Ital", monospace;
      font-style: italic
    }

    #sbo-rt-content code strong,
    #sbo-rt-content strong code,
    #sbo-rt-content pre strong,
    #sbo-rt-content strong.userinput {
      font-family: "Ubuntu Mono Bold", monospace;
      font-weight: bold
    }

    #sbo-rt-content div[data-type="example"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div[data-type="example"] h1,
    #sbo-rt-content div[data-type="example"] h2,
    #sbo-rt-content div[data-type="example"] h3,
    #sbo-rt-content div[data-type="example"] h4,
    #sbo-rt-content div[data-type="example"] h5,
    #sbo-rt-content div[data-type="example"] h6 {
      font-style: italic;
      font-weight: normal;
      text-align: left !important;
      text-transform: none !important;
      font-family: serif !important;
      margin: 10px 0 5px 0 !important;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content li pre.example {
      padding: 10px 0 !important
    }

    #sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],
    #sbo-rt-content div[data-type="example"] pre[data-type="screen"] {
      margin: 0
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      margin: 50px 0 10px 0 !important;
      line-height: 1;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] h2,
    #sbo-rt-content section[data-type="titlepage"] p.subtitle,
    #sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"] {
      font-size: 1.3em;
      font-weight: normal;
      text-align: center;
      margin-top: .5em;
      color: #555
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h2[data-type="author"],
    #sbo-rt-content section[data-type="titlepage"] p.author {
      font-size: 1.3em;
      font-family: serif !important;
      font-weight: bold;
      margin: 50px 0 !important;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] p.edition {
      text-align: center;
      text-transform: uppercase;
      margin-top: 2em
    }

    #sbo-rt-content section[data-type="titlepage"] {
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"]:after {
      content: url(css_assets/titlepage_footer_ebook.png);
      margin: 0 auto;
      max-width: 80%
    }

    #sbo-rt-content div.book div.titlepage div.publishername {
      margin-top: 60%;
      margin-bottom: 20px;
      text-align: center;
      font-size: 1.25em
    }

    #sbo-rt-content div.book div.titlepage div.locations p {
      margin: 0;
      text-align: center
    }

    #sbo-rt-content div.book div.titlepage div.locations p.cities {
      font-size: 80%;
      text-align: center;
      margin-top: 5px
    }

    #sbo-rt-content section.preface[title="Dedication"]&gt;

    div.titlepage h2.title {
      text-align: center;
      text-transform: uppercase;
      font-size: 1.5em;
      margin-top: 50px;
      margin-bottom: 50px
    }

    #sbo-rt-content ul.stafflist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.stafflist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content ul.printings li {
      list-style-type: none
    }

    #sbo-rt-content section.preface[title="Dedication"] p {
      font-style: italic;
      text-align: center
    }

    #sbo-rt-content div.colophon h1.title {
      font-size: 1.3em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon h2.subtitle {
      margin: 0 !important;
      color: #000;
      font-family: serif !important;
      font-size: 1em;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.author h3.author {
      font-size: 1.1em;
      font-family: serif !important;
      margin: 10px 0 0 !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h4,
    #sbo-rt-content div.colophon div.editor h3.editor {
      color: #000;
      font-size: .8em;
      margin: 15px 0 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h3.editor {
      font-size: .8em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.publisher {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon div.publisher p,
    #sbo-rt-content div.colophon div.publisher span.publishername {
      margin: 0;
      font-size: .8em
    }

    #sbo-rt-content div.legalnotice p,
    #sbo-rt-content div.timestamp p {
      font-size: .8em
    }

    #sbo-rt-content div.timestamp p {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon[title="About the Author"] h1.title,
    #sbo-rt-content div.colophon[title="Colophon"] h1.title {
      font-size: 1.5em;
      margin: 0 !important;
      font-family: sans-serif !important
    }

    #sbo-rt-content section.chapter div.titlepage div.author {
      margin: 10px 0 10px 0
    }

    #sbo-rt-content section.chapter div.titlepage div.author div.affiliation {
      font-style: italic
    }

    #sbo-rt-content div.attribution {
      margin: 5px 0 0 50px !important
    }

    #sbo-rt-content h3.author span.orgname {
      display: none
    }

    #sbo-rt-content div.epigraph {
      margin: 10px 0 10px 20px !important;
      page-break-inside: avoid;
      font-size: 90%
    }

    #sbo-rt-content div.epigraph p {
      font-style: italic
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content div.blockquote {
      margin: 10px !important;
      page-break-inside: avoid;
      font-size: 95%
    }

    #sbo-rt-content blockquote p,
    #sbo-rt-content div.blockquote p {
      font-style: italic;
      margin: .75em 0 0 !important
    }

    #sbo-rt-content blockquote div.attribution,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      margin: 5px 0 10px 30px !important;
      text-align: right;
      width: 80%
    }

    #sbo-rt-content blockquote div.attribution p,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      font-style: normal;
      margin-top: 5px
    }

    #sbo-rt-content blockquote div.attribution p:before,
    #sbo-rt-content blockquote p[data-type="attribution"]:before {
      font-style: normal;
      content: "—";
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none
    }

    #sbo-rt-content p.right {
      text-align: right;
      margin: 0
    }

    #sbo-rt-content div[data-type="footnotes"] {
      border-top: 1px solid black;
      margin-top: 2em
    }

    #sbo-rt-content sub,
    #sbo-rt-content sup {
      font-size: 75%;
      line-height: 0;
      position: relative
    }

    #sbo-rt-content sup {
      top: -.5em
    }

    #sbo-rt-content sub {
      bottom: -.25em
    }

    #sbo-rt-content p[data-type="footnote"] {
      font-size: 90% !important;
      line-height: 1.2em !important;
      margin-left: 2.5em !important;
      text-indent: -2.3em !important
    }

    #sbo-rt-content p[data-type="footnote"] sup {
      display: inline-block !important;
      position: static !important;
      width: 2em !important;
      text-align: right !important;
      font-size: 100% !important;
      padding-right: .5em !important
    }

    #sbo-rt-content p[data-type="footnote"] a[href$="-marker"] {
      font-family: sans-serif !important;
      font-size: 90% !important;
      color: #8e0012 !important
    }

    #sbo-rt-content a[data-type="noteref"] {
      font-family: sans-serif !important;
      color: #8e0012;
      margin-left: 0;
      padding-left: 0
    }

    #sbo-rt-content div.refentry p.refname {
      font-size: 1em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold;
      margin-bottom: 5px;
      overflow: auto;
      width: 100%
    }

    #sbo-rt-content div.refentry {
      width: 100%;
      display: block;
      margin-top: 2em
    }

    #sbo-rt-content div.refsynopsisdiv {
      display: block;
      clear: both
    }

    #sbo-rt-content div.refentry header {
      page-break-inside: avoid !important;
      display: block;
      break-inside: avoid !important;
      padding-top: 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content div.refsect1 h6 {
      font-size: .9em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold
    }

    #sbo-rt-content div.refsect1 {
      margin-top: 3em
    }

    #sbo-rt-content dt {
      padding-top: 10px !important;
      padding-bottom: 0 !important
    }

    #sbo-rt-content dd {
      margin-left: 1.5em !important;
      margin-bottom: .25em
    }

    #sbo-rt-content dd ol,
    #sbo-rt-content dd ul {
      padding-left: 1em
    }

    #sbo-rt-content dd li {
      margin-top: 0;
      margin-bottom: 0
    }

    #sbo-rt-content dd,
    #sbo-rt-content li {
      text-align: left
    }

    #sbo-rt-content ul,
    #sbo-rt-content ul&gt;
    li,
    #sbo-rt-content ol ul,
    #sbo-rt-content ol ul&gt;
    li,
    #sbo-rt-content ul ol ul,
    #sbo-rt-content ul ol ul&gt;

    li {
      list-style-type: disc
    }

    #sbo-rt-content ul ul,
    #sbo-rt-content ul ul&gt;

    li {
      list-style-type: square
    }

    #sbo-rt-content ul ul ul,
    #sbo-rt-content ul ul ul&gt;

    li {
      list-style-type: circle
    }

    #sbo-rt-content ol,
    #sbo-rt-content ol&gt;
    li,
    #sbo-rt-content ol ul ol,
    #sbo-rt-content ol ul ol&gt;
    li,
    #sbo-rt-content ul ol,
    #sbo-rt-content ul ol&gt;

    li {
      list-style-type: decimal
    }

    #sbo-rt-content ol ol,
    #sbo-rt-content ol ol&gt;

    li {
      list-style-type: lower-alpha
    }

    #sbo-rt-content ol ol ol,
    #sbo-rt-content ol ol ol&gt;

    li {
      list-style-type: lower-roman
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style-position: outside;
      margin: 15px 0 15px 1.25em;
      padding-left: 2.25em
    }

    #sbo-rt-content ol li,
    #sbo-rt-content ul li {
      margin: .5em 0 .65em;
      line-height: 125%
    }

    #sbo-rt-content div.orderedlistalpha {
      list-style-type: upper-alpha
    }

    #sbo-rt-content table.simplelist,
    #sbo-rt-content ul.simplelist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.simplelist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content table.simplelist td {
      border: none
    }

    #sbo-rt-content table.simplelist tr {
      border-bottom: none
    }

    #sbo-rt-content table.simplelist tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content dl.calloutlist p:first-child {
      margin-top: -25px !important
    }

    #sbo-rt-content dl.calloutlist dd {
      padding-left: 0;
      margin-top: -25px
    }

    #sbo-rt-content dl.calloutlist img,
    #sbo-rt-content a.co img {
      padding: 0
    }

    #sbo-rt-content div.toc ol {
      margin-top: 8px !important;
      margin-bottom: 8px !important;
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol ol {
      margin-left: 30px !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol li {
      list-style-type: none
    }

    #sbo-rt-content div.toc a {
      color: #8e0012
    }

    #sbo-rt-content div.toc ol a {
      font-size: 1em;
      font-weight: bold
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;

    ol a {
      font-weight: bold;
      font-size: 1em
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;
    ol&gt;
    li&gt;

    ol a {
      text-decoration: none;
      font-weight: normal;
      font-size: 1em
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"],
    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      margin: 30px !important;
      font-size: 90%;
      padding: 10px 8px 20px 8px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.tip ol,
    #sbo-rt-content div.tip ul,
    #sbo-rt-content div[data-type="tip"] ol,
    #sbo-rt-content div[data-type="tip"] ul,
    #sbo-rt-content div.note ol,
    #sbo-rt-content div.note ul,
    #sbo-rt-content div[data-type="note"] ol,
    #sbo-rt-content div[data-type="note"] ul,
    #sbo-rt-content div.warning ol,
    #sbo-rt-content div.warning ul,
    #sbo-rt-content div[data-type="warning"] ol,
    #sbo-rt-content div[data-type="warning"] ul,
    #sbo-rt-content div[data-type="caution"] ol,
    #sbo-rt-content div[data-type="caution"] ul,
    #sbo-rt-content div[data-type="important"] ol,
    #sbo-rt-content div[data-type="important"] ul {
      margin-left: 1.5em !important
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"] {
      border: 1px solid #BEBEBE;
      background-color: transparent
    }

    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      border: 1px solid #BC8F8F
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="note"] h1,
    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1,
    #sbo-rt-content div[data-type="important"] h6 {
      font-weight: bold;
      font-size: 110%;
      font-family: sans-serif !important;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px !important
    }

    #sbo-rt-content div[data-type="tip"] figure h6,
    #sbo-rt-content div[data-type="note"] figure h6,
    #sbo-rt-content div[data-type="warning"] figure h6,
    #sbo-rt-content div[data-type="caution"] figure h6,
    #sbo-rt-content div[data-type="important"] figure h6 {
      font-family: serif !important
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div[data-type="note"] h1 {
      color: #737373
    }

    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="important"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1 {
      color: #C67171
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,
    #sbo-rt-content div.safarienabled {
      background-color: transparent;
      margin: 8px 0 0 !important;
      border: 0 solid #BEBEBE;
      font-size: 100%;
      padding: 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,
    #sbo-rt-content div.safarienabled h6 {
      display: none
    }

    #sbo-rt-content div.table,
    #sbo-rt-content table {
      margin: 15px 0 30px 0 !important;
      max-width: 95%;
      border: none !important;
      background: none;
      display: table !important
    }

    #sbo-rt-content div.table,
    #sbo-rt-content div.informaltable,
    #sbo-rt-content table {
      page-break-inside: avoid
    }

    #sbo-rt-content tr,
    #sbo-rt-content tr td {
      border-bottom: 1px solid #c3c3c3
    }

    #sbo-rt-content thead td,
    #sbo-rt-content thead th {
      border-bottom: #9d9d9d 1px solid !important;
      border-top: #9d9d9d 1px solid !important
    }

    #sbo-rt-content tr:nth-of-type(even) {
      background-color: #f1f6fc
    }

    #sbo-rt-content thead {
      font-family: sans-serif;
      font-weight: bold
    }

    #sbo-rt-content td,
    #sbo-rt-content th {
      display: table-cell;
      padding: .3em;
      text-align: left;
      vertical-align: middle;
      font-size: 80%
    }

    #sbo-rt-content div.informaltable table {
      margin: 10px auto !important
    }

    #sbo-rt-content div.informaltable table tr {
      border-bottom: none
    }

    #sbo-rt-content div.informaltable table tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content div.informaltable td,
    #sbo-rt-content div.informaltable th {
      border: #9d9d9d 1px solid
    }

    #sbo-rt-content div.table-title,
    #sbo-rt-content table caption {
      font-weight: normal;
      font-style: italic;
      font-family: serif;
      font-size: 1em;
      margin: 10px 0 10px 0 !important;
      padding: 0;
      page-break-after: avoid;
      text-align: left !important
    }

    #sbo-rt-content table code {
      font-size: smaller
    }

    #sbo-rt-content table.border tbody&gt;
    tr:last-child&gt;

    td {
      border-bottom: transparent
    }

    #sbo-rt-content div.equation,
    #sbo-rt-content div[data-type="equation"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div.equation-title,
    #sbo-rt-content div[data-type="equation"] h5 {
      font-style: italic;
      font-weight: normal;
      font-family: serif !important;
      font-size: 90%;
      margin: 20px 0 10px 0 !important;
      page-break-after: avoid
    }

    #sbo-rt-content div.equation-contents {
      margin-left: 20px
    }

    #sbo-rt-content div[data-type="equation"] math {
      font-size: calc(.35em + 1vw)
    }

    #sbo-rt-content span.inlinemediaobject {
      height: .85em;
      display: inline-block;
      margin-bottom: .2em
    }

    #sbo-rt-content span.inlinemediaobject img {
      margin: 0;
      height: .85em
    }

    #sbo-rt-content div.informalequation {
      margin: 20px 0 20px 20px;
      width: 75%
    }

    #sbo-rt-content div.informalequation img {
      width: 75%
    }

    #sbo-rt-content div.index {
      text-indent: 0
    }

    #sbo-rt-content div.index h3 {
      padding: .25em;
      margin-top: 1em !important;
      background-color: #F0F0F0
    }

    #sbo-rt-content div.index li {
      line-height: 130%;
      list-style-type: none
    }

    #sbo-rt-content div.index a.indexterm {
      color: #8e0012 !important
    }

    #sbo-rt-content div.index ul {
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.index ul ul {
      margin-left: 1em !important;
      margin-top: 0 !important
    }

    #sbo-rt-content code.boolean,
    #sbo-rt-content .navy {
      color: rgb(0, 0, 128);
    }

    #sbo-rt-content code.character,
    #sbo-rt-content .olive {
      color: rgb(128, 128, 0);
    }

    #sbo-rt-content code.comment,
    #sbo-rt-content .blue {
      color: rgb(0, 0, 255);
    }

    #sbo-rt-content code.conditional,
    #sbo-rt-content .limegreen {
      color: rgb(50, 205, 50);
    }

    #sbo-rt-content code.constant,
    #sbo-rt-content .darkorange {
      color: rgb(255, 140, 0);
    }

    #sbo-rt-content code.debug,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.define,
    #sbo-rt-content .darkgoldenrod,
    #sbo-rt-content .gold {
      color: rgb(184, 134, 11);
    }

    #sbo-rt-content code.delimiter,
    #sbo-rt-content .dimgray {
      color: rgb(105, 105, 105);
    }

    #sbo-rt-content code.error,
    #sbo-rt-content .red {
      color: rgb(255, 0, 0);
    }

    #sbo-rt-content code.exception,
    #sbo-rt-content .salmon {
      color: rgb(250, 128, 11);
    }

    #sbo-rt-content code.float,
    #sbo-rt-content .steelblue {
      color: rgb(70, 130, 180);
    }

    #sbo-rt-content pre code.function,
    #sbo-rt-content .green {
      color: rgb(0, 128, 0);
    }

    #sbo-rt-content code.identifier,
    #sbo-rt-content .royalblue {
      color: rgb(65, 105, 225);
    }

    #sbo-rt-content code.ignore,
    #sbo-rt-content .gray {
      color: rgb(128, 128, 128);
    }

    #sbo-rt-content code.include,
    #sbo-rt-content .purple {
      color: rgb(128, 0, 128);
    }

    #sbo-rt-content code.keyword,
    #sbo-rt-content .sienna {
      color: rgb(160, 82, 45);
    }

    #sbo-rt-content code.label,
    #sbo-rt-content .deeppink {
      color: rgb(255, 20, 147);
    }

    #sbo-rt-content code.macro,
    #sbo-rt-content .orangered {
      color: rgb(255, 69, 0);
    }

    #sbo-rt-content code.number,
    #sbo-rt-content .brown {
      color: rgb(165, 42, 42);
    }

    #sbo-rt-content code.operator,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.preCondit,
    #sbo-rt-content .teal {
      color: rgb(0, 128, 128);
    }

    #sbo-rt-content code.preProc,
    #sbo-rt-content .fuschia {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.repeat,
    #sbo-rt-content .indigo {
      color: rgb(75, 0, 130);
    }

    #sbo-rt-content code.special,
    #sbo-rt-content .saddlebrown {
      color: rgb(139, 69, 19);
    }

    #sbo-rt-content code.specialchar,
    #sbo-rt-content .magenta {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.specialcomment,
    #sbo-rt-content .seagreen {
      color: rgb(46, 139, 87);
    }

    #sbo-rt-content code.statement,
    #sbo-rt-content .forestgreen {
      color: rgb(34, 139, 34);
    }

    #sbo-rt-content code.storageclass,
    #sbo-rt-content .plum {
      color: rgb(221, 160, 221);
    }

    #sbo-rt-content code.string,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.structure,
    #sbo-rt-content .chocolate {
      color: rgb(210, 106, 30);
    }

    #sbo-rt-content code.tag,
    #sbo-rt-content .darkcyan {
      color: rgb(0, 139, 139);
    }

    #sbo-rt-content code.todo,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.type,
    #sbo-rt-content .mediumslateblue {
      color: rgb(123, 104, 238);
    }

    #sbo-rt-content code.typedef,
    #sbo-rt-content .darkgreen {
      color: rgb(0, 100, 0);
    }

    #sbo-rt-content code.underlined {
      text-decoration: underline;
    }

    #sbo-rt-content pre code.hll {
      background-color: #ffc
    }

    #sbo-rt-content pre code.c {
      color: #09F;
      font-style: italic
    }

    #sbo-rt-content pre code.err {
      color: #A00
    }

    #sbo-rt-content pre code.k {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.o {
      color: #555
    }

    #sbo-rt-content pre code.cm {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cp {
      color: #099
    }

    #sbo-rt-content pre code.c1 {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cs {
      color: #35586C;
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content pre code.gd {
      background-color: #FCC
    }

    #sbo-rt-content pre code.ge {
      font-style: italic
    }

    #sbo-rt-content pre code.gr {
      color: #F00
    }

    #sbo-rt-content pre code.gh {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gi {
      background-color: #CFC
    }

    #sbo-rt-content pre code.go {
      color: #000
    }

    #sbo-rt-content pre code.gp {
      color: #009;
      font-weight: bold
    }

    #sbo-rt-content pre code.gs {
      font-weight: bold
    }

    #sbo-rt-content pre code.gu {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gt {
      color: #9C6
    }

    #sbo-rt-content pre code.kc {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kd {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kn {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kp {
      color: #069
    }

    #sbo-rt-content pre code.kr {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kt {
      color: #078;
      font-weight: bold
    }

    #sbo-rt-content pre code.m {
      color: #F60
    }

    #sbo-rt-content pre code.s {
      color: #C30
    }

    #sbo-rt-content pre code.na {
      color: #309
    }

    #sbo-rt-content pre code.nb {
      color: #366
    }

    #sbo-rt-content pre code.nc {
      color: #0A8;
      font-weight: bold
    }

    #sbo-rt-content pre code.no {
      color: #360
    }

    #sbo-rt-content pre code.nd {
      color: #99F
    }

    #sbo-rt-content pre code.ni {
      color: #999;
      font-weight: bold
    }

    #sbo-rt-content pre code.ne {
      color: #C00;
      font-weight: bold
    }

    #sbo-rt-content pre code.nf {
      color: #C0F
    }

    #sbo-rt-content pre code.nl {
      color: #99F
    }

    #sbo-rt-content pre code.nn {
      color: #0CF;
      font-weight: bold
    }

    #sbo-rt-content pre code.nt {
      color: #309;
      font-weight: bold
    }

    #sbo-rt-content pre code.nv {
      color: #033
    }

    #sbo-rt-content pre code.ow {
      color: #000;
      font-weight: bold
    }

    #sbo-rt-content pre code.w {
      color: #bbb
    }

    #sbo-rt-content pre code.mf {
      color: #F60
    }

    #sbo-rt-content pre code.mh {
      color: #F60
    }

    #sbo-rt-content pre code.mi {
      color: #F60
    }

    #sbo-rt-content pre code.mo {
      color: #F60
    }

    #sbo-rt-content pre code.sb {
      color: #C30
    }

    #sbo-rt-content pre code.sc {
      color: #C30
    }

    #sbo-rt-content pre code.sd {
      color: #C30;
      font-style: italic
    }

    #sbo-rt-content pre code.s2 {
      color: #C30
    }

    #sbo-rt-content pre code.se {
      color: #C30;
      font-weight: bold
    }

    #sbo-rt-content pre code.sh {
      color: #C30
    }

    #sbo-rt-content pre code.si {
      color: #A00
    }

    #sbo-rt-content pre code.sx {
      color: #C30
    }

    #sbo-rt-content pre code.sr {
      color: #3AA
    }

    #sbo-rt-content pre code.s1 {
      color: #C30
    }

    #sbo-rt-content pre code.ss {
      color: #A60
    }

    #sbo-rt-content pre code.bp {
      color: #366
    }

    #sbo-rt-content pre code.vc {
      color: #033
    }

    #sbo-rt-content pre code.vg {
      color: #033
    }

    #sbo-rt-content pre code.vi {
      color: #033
    }

    #sbo-rt-content pre code.il {
      color: #F60
    }

    #sbo-rt-content pre code.g {
      color: #050
    }

    #sbo-rt-content pre code.l {
      color: #C60
    }

    #sbo-rt-content pre code.l {
      color: #F90
    }

    #sbo-rt-content pre code.n {
      color: #008
    }

    #sbo-rt-content pre code.nx {
      color: #008
    }

    #sbo-rt-content pre code.py {
      color: #96F
    }

    #sbo-rt-content pre code.p {
      color: #000
    }

    #sbo-rt-content pre code.x {
      color: #F06
    }

    #sbo-rt-content div.blockquote_sampler_toc {
      width: 95%;
      margin: 5px 5px 5px 10px !important
    }

    #sbo-rt-content div {
      font-family: serif;
      text-align: left
    }

    #sbo-rt-content .gray-background,
    #sbo-rt-content .reverse-video {
      background: #2E2E2E;
      color: #FFF
    }

    #sbo-rt-content .light-gray-background {
      background: #A0A0A0
    }

    #sbo-rt-content .preserve-whitespace {
      white-space: pre-wrap
    }

    #sbo-rt-content span.gray {
      color: #4C4C4C
    }

    #sbo-rt-content .width-10 {
      width: 10vw !important
    }

    #sbo-rt-content .width-20 {
      width: 20vw !important
    }

    #sbo-rt-content .width-30 {
      width: 30vw !important
    }

    #sbo-rt-content .width-40 {
      width: 40vw !important
    }

    #sbo-rt-content .width-50 {
      width: 50vw !important
    }

    #sbo-rt-content .width-60 {
      width: 60vw !important
    }

    #sbo-rt-content .width-70 {
      width: 70vw !important
    }

    #sbo-rt-content .width-80 {
      width: 80vw !important
    }

    #sbo-rt-content .width-90 {
      width: 90vw !important
    }

    #sbo-rt-content .width-full,
    #sbo-rt-content .width-100 {
      width: 100vw !important
    }

    #sbo-rt-content div[data-type="equation"].fifty-percent img {
      width: 50%
    }
  </style>
  <style type="text/css" id="font-styles">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-size: &lt;
      %=font_size %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="font-family">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-family: &lt;
      %=font_family %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="column-width">
    #sbo-rt-content {
      max-width: &lt;
      %=column_width %&gt;
      % !important;
      margin: 0 auto !important;
    }
  </style>

  <style type="text/css">
    body {
      background-color: #fbfbfb !important;
      margin: 1em;
    }

    #sbo-rt-content * {
      text-indent: 0pt !important;
    }

    #sbo-rt-content .bq {
      margin-right: 1em !important;
    }

    #sbo-rt-content * {
      word-wrap: break-word !important;
      word-break: break-word !important;
    }

    #sbo-rt-content table,
    #sbo-rt-content pre {
      overflow-x: unset !important;
      overflow: unset !important;
      overflow-y: unset !important;
      white-space: pre-wrap !important;
    }
  </style>
</head>

<body>
  <div id="sbo-rt-content">
    <section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 5. Support Vector Machines">
      <div class="chapter" id="svm_chapter">
        <h1><span class="label">Chapter 5. </span>Support Vector Machines</h1>


        <p>A <em>Support Vector Machine</em> (SVM) is <a data-type="indexterm"
            data-primary="Support Vector Machines (SVMs)" id="svm5" />a very powerful and versatile Machine Learning
          model, capable of performing linear or nonlinear classification, regression, and even outlier detection. It is
          one of the most popular models in Machine Learning, and anyone interested in Machine Learning should have it
          in their toolbox. SVMs are particularly well suited for classification of complex but small- or medium-sized
          datasets.</p>

        <p>This chapter will explain the core concepts of SVMs, how to use them, and how they work.</p>






        <section data-type="sect1" data-pdf-bookmark="Linear SVM Classification">
          <div class="sect1" id="idm139656377321712">
            <h1>Linear SVM Classification</h1>

            <p>The <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                data-secondary="linear classification" id="svm5lc" /><a data-type="indexterm"
                data-primary="linear SVM classification" id="lsvmc5" /><a data-type="indexterm"
                data-primary="linear models" data-secondary="SVM" id="lm5svm" />fundamental idea behind SVMs is best
              explained with some pictures. <a data-type="xref" href="#large_margin_classification_plot">Figure 5-1</a>
              shows part of the iris dataset that was introduced at the end of <a data-type="xref"
                href="ch04.xhtml#linear_models_chapter">Chapter 4</a>. The two classes can clearly be separated easily
              with a straight line (they are <em>linearly separable</em>). The left plot shows the decision boundaries
              of three possible linear classifiers. The model whose decision boundary is represented by the dashed line
              is so bad that it does not even separate the classes properly. The other two models work perfectly on this
              training set, but their decision boundaries come so close to the instances that these models will probably
              not perform as well on new instances. In contrast, the solid line in the plot on the right represents the
              decision boundary of an SVM classifier; this line not only separates the two classes but also stays as far
              away from the closest training instances as possible. You can think of an SVM classifier as fitting the
              widest possible street (represented by the parallel dashed lines) between the classes. This is <a
                data-type="indexterm" data-primary="large margin classification" id="lmc5" />called <em>large margin
                classification</em>.</p>

            <figure>
              <div id="large_margin_classification_plot" class="figure">
                <img src="mlst_0501.png" alt="mlst 0501" width="3498" height="710" />
                <h6><span class="label">Figure 5-1. </span>Large margin classification</h6>
              </div>
            </figure>

            <p>Notice that adding more training instances “off the street” will not affect the decision boundary at all:
              it is fully determined (or “supported”) by the instances located on the edge of the street. These
              instances are called <a data-type="indexterm" data-primary="support vectors" id="idm139656377309488" />the
              <em>support vectors</em> (they are circled in <a data-type="xref"
                href="#large_margin_classification_plot">Figure 5-1</a>).</p>
            <div data-type="warning" epub:type="warning">
              <h6>Warning</h6>
              <p>SVMs are <a data-type="indexterm" data-primary="large margin classification" data-startref="lmc5"
                  id="idm139656377306304" />sensitive to the feature scales, as you can see in <a data-type="xref"
                  href="#sensitivity_to_feature_scales_plot">Figure 5-2</a>: on the left plot, the vertical scale is
                much larger than the horizontal scale, so the widest possible street is close to horizontal. After
                feature scaling (e.g., using Scikit-Learn’s <code>StandardScaler</code>), <a data-type="indexterm"
                  data-primary="Scikit-Learn" data-secondary="sklearn.preprocessing.StandardScaler"
                  id="idm139656377303744" />the decision boundary looks much better (on the right plot).</p>
            </div>

            <figure>
              <div id="sensitivity_to_feature_scales_plot" class="figure">
                <img src="mlst_0502.png" alt="mlst 0502" width="3504" height="863" />
                <h6><span class="label">Figure 5-2. </span>Sensitivity to feature scales</h6>
              </div>
            </figure>








            <section data-type="sect2" data-pdf-bookmark="Soft Margin Classification">
              <div class="sect2" id="idm139656377300384">
                <h2>Soft Margin Classification</h2>

                <p>If we <a data-type="indexterm" data-primary="soft margin classification" id="smc5" />strictly impose
                  that all instances be off the street and on the right side, this is called <em>hard margin
                    classification</em>. <a data-type="indexterm" data-primary="hard margin classification"
                    id="hmc5" />There are two main issues with hard margin classification. First, it only works if the
                  data is linearly separable, and second it is quite sensitive to outliers. <a data-type="xref"
                    href="#sensitivity_to_outliers_plot">Figure 5-3</a> shows the iris dataset with just one additional
                  outlier: on the left, it is impossible to find a hard margin, and on the right the decision boundary
                  ends up very different from the one we saw in <a data-type="xref"
                    href="#large_margin_classification_plot">Figure 5-1</a> without the outlier, and it will probably
                  not generalize as well.</p>

                <figure>
                  <div id="sensitivity_to_outliers_plot" class="figure">
                    <img src="mlst_0503.png" alt="mlst 0503" width="3498" height="710" />
                    <h6><span class="label">Figure 5-3. </span>Hard margin sensitivity to outliers</h6>
                  </div>
                </figure>

                <p>To avoid these issues <a data-type="indexterm" data-primary="hard margin classification"
                    data-startref="hmc5" id="idm139656377291392" />it is preferable to use a more flexible model. The
                  objective is to find a good balance between keeping the street as large as possible and limiting the
                  <em>margin violations</em> <a data-type="indexterm" data-primary="margin violations"
                    id="idm139656377289632" />(i.e., instances that end up in the middle of the street or even on the
                  wrong side). This is called <em>soft margin classification</em>.</p>

                <p>In Scikit-Learn’s SVM classes, you can control this balance using the <code>C</code> hyperparameter:
                  a smaller <code>C</code> value leads to a wider street but more margin violations. <a data-type="xref"
                    href="#regularization_plot">Figure 5-4</a> shows the decision boundaries and margins of two soft
                  margin SVM classifiers on a nonlinearly separable dataset. On the right, using a low <code>C</code>
                  value the margin is quite large, but many instances end up on the street. On the left, using a high
                  <code>C</code> value the classifier makes fewer margin violations but ends up with a smaller margin.
                  However, it seems likely that the first classifier will generalize better: in fact even on this
                  training set it makes fewer prediction errors, since most of the margin violations are actually on the
                  correct side of the decision boundary.</p>

                <figure>
                  <div id="regularization_plot" class="figure">
                    <img src="mlst_0504.png" alt="mlst 0504" width="3507" height="860" />
                    <h6><span class="label">Figure 5-4. </span>Large margin (left) versus fewer margin violations
                      (right)</h6>
                  </div>
                </figure>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>If your SVM model is <a data-type="indexterm" data-primary="overfitting"
                      id="idm139656377281712" />overfitting, you can try regularizing it by reducing <code>C</code>.</p>
                </div>

                <p>The following Scikit-Learn code loads the iris dataset, scales the features, and then trains a linear
                  SVM model <a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.svm.LinearSVC"
                    id="sklsvmlsvcch5" />(using the <code>LinearSVC</code> class with <em>C</em> = 1 and the <em>hinge
                    loss</em> function, described shortly) to detect Iris-Virginica flowers. The resulting model is
                  represented on <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.preprocessing.StandardScaler" id="sklpssch5" /><a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="sklearn.pipeline.Pipeline" id="sklpipepipech5" /><a
                    data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.datasets.load_iris()"
                    id="idm139656377274288" />the left of <a data-type="xref"
                    href="#regularization_plot">Figure 5-4</a>.</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>
<code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">StandardScaler</code>
<code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">LinearSVC</code>

<code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="p">[</code><code class="s2">"data"</code><code class="p">][:,</code> <code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">)]</code>  <code class="c1"># petal length, petal width</code>
<code class="n">y</code> <code class="o">=</code> <code class="p">(</code><code class="n">iris</code><code class="p">[</code><code class="s2">"target"</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">float64</code><code class="p">)</code>  <code class="c1"># Iris-Virginica</code>

<code class="n">svm_clf</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
        <code class="p">(</code><code class="s2">"scaler"</code><code class="p">,</code> <code class="n">StandardScaler</code><code class="p">()),</code>
        <code class="p">(</code><code class="s2">"linear_svc"</code><code class="p">,</code> <code class="n">LinearSVC</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">loss</code><code class="o">=</code><code class="s2">"hinge"</code><code class="p">)),</code>
    <code class="p">])</code>

<code class="n">svm_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

                <p>Then, as usual, you can use the model to make predictions:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">svm_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mf">5.5</code><code class="p">,</code> <code class="mf">1.7</code><code class="p">]])</code>
<code class="go">array([1.])</code></pre>
                <div data-type="note" epub:type="note">
                  <h6>Note</h6>
                  <p>Unlike Logistic Regression classifiers, SVM classifiers do not output probabilities for each class.
                  </p>
                </div>

                <p>Alternatively, <a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.svm.SVC"
                    id="idm139656377111568" />you could use the <code>SVC</code> class, using
                  <code>SVC(kernel="linear", C=1)</code>, but it is much slower, especially with large training sets, so
                  it is not recommended. Another option is to use the <code>SGDClassifier</code> class, with
                  <code>SGDClassifier(loss="hinge", alpha=1/(m*C))</code>. This applies regular <a data-type="indexterm"
                    data-primary="Stochastic Gradient Descent (SGD)" id="idm139656377108640" /><a data-type="indexterm"
                    data-primary="Gradient Descent (GD)" data-secondary="Stochastic GD"
                    id="idm139656377107968" />Stochastic Gradient Descent (see <a data-type="xref"
                    href="ch04.xhtml#linear_models_chapter">Chapter 4</a>) to train a linear SVM classifier. It does not
                  converge as fast as the <code>LinearSVC</code> class, but it can be useful to handle huge datasets
                  that do not fit in memory (out-of-core training), or to handle online classification tasks.</p>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>The <code>LinearSVC</code> class regularizes the bias term, so you should center the training set
                    first by subtracting its mean. This is automatic if you scale the data using the
                    <code>StandardScaler</code>. Moreover, make sure you set the <code>loss</code> hyperparameter to
                    <code>"hinge"</code>, as it is not the default value. Finally, for better performance you should set
                    the <code>dual</code> hyperparameter to <code>False</code>, unless there are more features than
                    training instances (we will discuss duality later in the <a data-type="indexterm"
                      data-primary="soft margin classification" data-startref="smc5" id="idm139656376994880" /><a
                      data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                      data-secondary="linear classification" data-startref="svm5lc" id="idm139656376993936" /><a
                      data-type="indexterm" data-primary="linear models" data-secondary="SVM" data-startref="lm5svm"
                      id="idm139656376992752" /><a data-type="indexterm" data-primary="linear SVM classification"
                      data-startref="lsvmc5" id="idm139656376991536" />chapter).</p>
                </div>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Nonlinear SVM Classification">
          <div class="sect1" id="idm139656377299760">
            <h1>Nonlinear SVM Classification</h1>

            <p>Although <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                data-secondary="nonlinear classification" id="svm5nc" /><a data-type="indexterm"
                data-primary="nonlinear SVM classification" id="nsvmc5" />linear SVM classifiers are efficient and work
              surprisingly well in many cases, many datasets are not even close to being linearly separable. One
              approach to handling nonlinear datasets is to add more features, such as <a data-type="indexterm"
                data-primary="nonlinear SVM classification" data-secondary="with polynomial features"
                data-secondary-sortas="polynomial features" id="nsvmc5wpf" /><a data-type="indexterm"
                data-primary="polynomial features, adding" id="pf5" />polynomial features (as you did in <a
                data-type="xref" href="ch04.xhtml#linear_models_chapter">Chapter 4</a>); in some cases this can result
              in a linearly separable dataset. Consider the left plot in <a data-type="xref"
                href="#higher_dimensions_plot">Figure 5-5</a>: it represents a simple dataset with just one feature
              <em>x</em><sub>1</sub>. This dataset is not linearly separable, as you can see. But if you add a second
              feature <em>x</em><sub>2</sub> = (<em>x</em><sub>1</sub>)<sup>2</sup>, the resulting 2D dataset is
              perfectly linearly separable.</p>

            <figure>
              <div id="higher_dimensions_plot" class="figure">
                <img src="mlst_0505.png" alt="mlst 0505" width="2889" height="1058" />
                <h6><span class="label">Figure 5-5. </span>Adding features to make a dataset linearly separable</h6>
              </div>
            </figure>

            <p>To implement this idea using <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.svm.LinearSVC" data-startref="sklsvmlsvcch5" id="idm139656376976752" /> <a
                data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.preprocessing.PolynomialFeatures" id="idm139656377101712" /><a
                data-type="indexterm" data-primary="Scikit-Learn" data-secondary="Pipeline constructor"
                id="idm139656377100640" /><a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.datasets.make_moons()" id="idm139656377099696" /><a data-type="indexterm"
                data-primary="Scikit-Learn" data-secondary="sklearn.pipeline.Pipeline" data-startref="sklpipepipech5"
                id="idm139656377098736" />Scikit-Learn, you can create a <code>Pipeline</code> containing a
              <code>PolynomialFeatures</code> transformer (discussed in <a data-type="xref"
                href="ch04.xhtml#polynomial_regression">“Polynomial Regression”</a>), followed by a
              <code>StandardScaler</code> and a <code>LinearSVC</code>. Let’s test this on the moons dataset: this is a
              toy dataset for binary classification in which the data points are shaped as two interleaving half circles
              (see <a data-type="xref" href="#moons_polynomial_svc_plot">Figure 5-6</a>). You can generate this dataset
              using the <code>make_moons()</code> function:</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code>
<code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">PolynomialFeatures</code>

<code class="n">polynomial_svm_clf</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
        <code class="p">(</code><code class="s2">"poly_features"</code><code class="p">,</code> <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">3</code><code class="p">)),</code>
        <code class="p">(</code><code class="s2">"scaler"</code><code class="p">,</code> <code class="n">StandardScaler</code><code class="p">()),</code>
        <code class="p">(</code><code class="s2">"svm_clf"</code><code class="p">,</code> <code class="n">LinearSVC</code><code class="p">(</code><code class="n">C</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">loss</code><code class="o">=</code><code class="s2">"hinge"</code><code class="p">))</code>
    <code class="p">])</code>

<code class="n">polynomial_svm_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

            <figure>
              <div id="moons_polynomial_svc_plot" class="figure">
                <img src="mlst_0506.png" alt="mlst 0506" width="1704" height="1097" />
                <h6><span class="label">Figure 5-6. </span>Linear SVM classifier using polynomial features</h6>
              </div>
            </figure>








            <section data-type="sect2" data-pdf-bookmark="Polynomial Kernel">
              <div class="sect2" id="idm139656377051472">
                <h2>Polynomial Kernel</h2>

                <p>Adding <a data-type="indexterm" data-primary="nonlinear SVM classification"
                    data-secondary="with polynomial features" data-secondary-sortas="polynomial features"
                    data-startref="nsvmc5wpf" id="idm139656377049936" /><a data-type="indexterm"
                    data-primary="polynomial features, adding" data-startref="pf5" id="idm139656377048448" /><a
                    data-type="indexterm" data-primary="nonlinear SVM classification" data-secondary="polynomial kernel"
                    id="nsvmc5pk" /><a data-type="indexterm" data-primary="polynomial kernel" id="pk5" /><a
                    data-type="indexterm" data-primary="kernel trick" id="idm139656377045312" /><a data-type="indexterm"
                    data-primary="kernels" id="k5" />polynomial features is simple to implement and can work great with
                  all sorts of Machine Learning algorithms (not just SVMs), but at a low polynomial degree it cannot
                  deal with very complex datasets, and with a high polynomial degree it creates a huge number of
                  features, making the model too slow.</p>

                <p>Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the
                  <em>kernel trick</em> (it is explained in a moment). It makes it possible to get the same result as if
                  you added many polynomial features, even with very high-degree polynomials, without actually having to
                  add them. So there is no combinatorial explosion of the number of features since you don’t actually
                  add any features. This trick is implemented by the <code>SVC</code> class. <a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="sklearn.svm.SVC" id="idm139656377041568" />Let’s test it
                  on the moons dataset:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVC</code>
<code class="n">poly_kernel_svm_clf</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
        <code class="p">(</code><code class="s2">"scaler"</code><code class="p">,</code> <code class="n">StandardScaler</code><code class="p">()),</code>
        <code class="p">(</code><code class="s2">"svm_clf"</code><code class="p">,</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"poly"</code><code class="p">,</code> <code class="n">degree</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">coef0</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mi">5</code><code class="p">))</code>
    <code class="p">])</code>
<code class="n">poly_kernel_svm_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

                <p>This code <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.preprocessing.StandardScaler" data-startref="sklpssch5"
                    id="idm139656376914640" />trains an SVM classifier using a 3<sup>rd</sup>-degree polynomial kernel.
                  It is represented on the left of <a data-type="xref"
                    href="#moons_kernelized_polynomial_svc_plot">Figure 5-7</a>. On the right is another SVM classifier
                  using a 10<sup>th</sup>-degree polynomial kernel. Obviously, if your model is overfitting, you might
                  want to reduce the polynomial degree. Conversely, if it is underfitting, you can try increasing it.
                  The hyperparameter <code>coef0</code> controls how much the model is influenced by high-degree
                  polynomials versus low-degree polynomials.</p>

                <figure>
                  <div id="moons_kernelized_polynomial_svc_plot" class="figure">
                    <img src="mlst_0507.png" alt="mlst 0507" width="3228" height="1091" />
                    <h6><span class="label">Figure 5-7. </span>SVM classifiers with a polynomial kernel</h6>
                  </div>
                </figure>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>A common approach to find the right <a data-type="indexterm" data-primary="hyperparameters"
                      id="idm139656376789424" />hyperparameter values is to use grid <a data-type="indexterm"
                      data-primary="grid search" id="idm139656376788544" />search (see <a data-type="xref"
                      href="ch02.xhtml#project_chapter">Chapter 2</a>). It is often faster to first do a very coarse
                    grid search, then a finer grid search around the best values found. Having a good sense of what each
                    hyperparameter actually does can also help you search in the right part of the <a
                      data-type="indexterm" data-primary="nonlinear SVM classification"
                      data-secondary="polynomial kernel" data-startref="nsvmc5pk" id="idm139656376786672" /><a
                      data-type="indexterm" data-primary="polynomial kernel" data-startref="pk5"
                      id="idm139656376785392" />hyperparameter space.</p>
                </div>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Adding Similarity Features">
              <div class="sect2" id="idm139656376784064">
                <h2>Adding Similarity Features</h2>

                <p>Another <a data-type="indexterm" data-primary="nonlinear SVM classification"
                    data-secondary="similarity features, adding" id="nsvmc5sfa" /><a data-type="indexterm"
                    data-primary="similarity function" id="sf5" />technique to tackle nonlinear problems is to add
                  features computed using a <em>similarity function</em> that measures how much each instance resembles
                  a particular <em>landmark</em>. <a data-type="indexterm" data-primary="landmarks" id="l5" />For
                  example, let’s take the one-dimensional dataset discussed earlier and add two landmarks to it at
                  <em>x</em><sub>1</sub> = –2 and <em>x</em><sub>1</sub> = 1 (see the left plot in <a data-type="xref"
                    href="#kernel_method_plot">Figure 5-8</a>). Next, let’s define the similarity function to be the <a
                    data-type="indexterm" data-primary="Radial Basis Function (RBF)" id="idm139656376775520" /><a
                    data-type="indexterm" data-primary="Gaussian RBF" id="idm139656376774784" />Gaussian <em>Radial
                    Basis Function</em> (<em>RBF</em>) with <em>γ</em> = 0.3 (see <a data-type="xref"
                    href="#grbf_function">Equation 5-1</a>).</p>
                <div id="grbf_function" data-type="equation">
                  <h5><span class="label">Equation 5-1. </span>Gaussian RBF</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mrow>
                          <msub>
                            <mi>ϕ</mi>
                            <mi>γ</mi>
                          </msub>
                          <mrow>
                            <mo>(</mo>
                            <mi mathvariant="bold">x</mi>
                            <mo>,</mo>
                            <mi>ℓ</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </mstyle>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mrow>
                          <mo form="prefix">exp</mo>
                          <mo>(</mo>
                          <mstyle scriptlevel="0" displaystyle="true">
                            <mrow>
                              <mo>-</mo>
                              <mi>γ</mi>
                              <msup>
                                <mfenced separators="" open="∥" close="∥">
                                  <mi mathvariant="bold">x</mi>
                                  <mo>-</mo>
                                  <mi>ℓ</mi>
                                </mfenced>
                                <mn>2</mn>
                              </msup>
                            </mrow>
                          </mstyle>
                          <mo>)</mo>
                        </mrow>
                      </mstyle>
                    </mrow>
                  </math>
                </div>

                <p>It is a bell-shaped function varying from 0 (very far away from the landmark) to 1 (at the landmark).
                  Now we are ready to compute the new features. For example, let’s look at the instance
                  <em>x</em><sub>1</sub> = –1: it is located at a distance of 1 from the first landmark, and 2 from the
                  second landmark. Therefore its new features are <em>x</em><sub>2</sub> = exp (–0.3 × 1<sup>2</sup>) ≈
                  0.74 and <em>x</em><sub>3</sub> = exp (–0.3 × 2<sup>2</sup>) ≈ 0.30. The plot on the right of <a
                    data-type="xref" href="#kernel_method_plot">Figure 5-8</a> shows the transformed dataset (dropping
                  the original features). As you can see, it is now linearly <span
                    class="keep-together">separable</span>.</p>

                <figure>
                  <div id="kernel_method_plot" class="figure">
                    <img src="mlst_0508.png" alt="mlst 0508" width="3195" height="1088" />
                    <h6><span class="label">Figure 5-8. </span>Similarity features using the Gaussian RBF</h6>
                  </div>
                </figure>

                <p>You may wonder how to select the landmarks. The simplest approach is to create a landmark at the
                  location of each and every instance in the dataset. This creates many dimensions and thus increases
                  the chances that the transformed training set will be linearly separable. The downside is that a
                  training set with <em>m</em> instances and <em>n</em> features gets transformed into a training set
                  with <em>m</em> instances and <em>m</em> features (assuming you drop the original features). If your
                  training set is very large, you end up with an equally large number of <a data-type="indexterm"
                    data-primary="landmarks" data-startref="l5" id="idm139656376900528" /><a data-type="indexterm"
                    data-primary="nonlinear SVM classification" data-secondary="similarity features, adding"
                    data-startref="nsvmc5sfa" id="idm139656376899552" /><a data-type="indexterm"
                    data-primary="similarity function" data-startref="sf5" id="idm139656376898304" />features.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Gaussian RBF Kernel">
              <div class="sect2" id="idm139656376746864">
                <h2>Gaussian RBF Kernel</h2>

                <p>Just <a data-type="indexterm" data-primary="nonlinear SVM classification"
                    data-secondary="Gaussian RBF kernel" id="nsvm5grbf" /><a data-type="indexterm"
                    data-primary="Gaussian RBF kernel" id="grbfk5" />like the polynomial features method, the similarity
                  features method can be useful with any Machine Learning algorithm, but it may be computationally
                  expensive to compute all the additional features, especially on large training sets. However, once
                  again the <a data-type="indexterm" data-primary="kernel trick" id="idm139656376893184" />kernel trick
                  does its SVM magic: it makes it possible to obtain a similar result as if you had added many
                  similarity features, without actually having to add them. Let’s try the Gaussian RBF kernel using <a
                    data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.svm.SVC"
                    id="sklsvmsvcch5" /><a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.preprocessing.StandardScaler" id="idm139656376890944" />the <code>SVC</code>
                  class:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">rbf_kernel_svm_clf</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
        <code class="p">(</code><code class="s2">"scaler"</code><code class="p">,</code> <code class="n">StandardScaler</code><code class="p">()),</code>
        <code class="p">(</code><code class="s2">"svm_clf"</code><code class="p">,</code> <code class="n">SVC</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"rbf"</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mf">0.001</code><code class="p">))</code>
    <code class="p">])</code>
<code class="n">rbf_kernel_svm_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

                <p>This model is represented on the bottom left of <a data-type="xref"
                    href="#moons_rbf_svc_plot">Figure 5-9</a>. The other plots show models trained with different values
                  of hyperparameters <code>gamma</code> (<em>γ</em>) and <em>C</em>. Increasing <code>gamma</code> makes
                  the bell-shape curve narrower (see the left plot of <a data-type="xref"
                    href="#kernel_method_plot">Figure 5-8</a>), and as a result each instance’s range of influence is
                  smaller: the decision boundary ends up being more irregular, wiggling around individual instances.
                  Conversely, a small <code>gamma</code> <a data-type="indexterm" data-primary="gamma value"
                    id="idm139656376871392" />value makes the bell-shaped curve wider, so instances have a larger range
                  of influence, and the decision boundary ends up smoother. So <em>γ</em> acts like a regularization
                  hyperparameter: if your model is <a data-type="indexterm" data-primary="overfitting"
                    id="idm139656376870000" />overfitting, you should reduce it, and if it is <a data-type="indexterm"
                    data-primary="underfitting" id="idm139656376869104" />underfitting, you should increase it (similar
                  to the <code>C</code> hyperparameter).</p>

                <figure>
                  <div id="moons_rbf_svc_plot" class="figure">
                    <img src="mlst_0509.png" alt="mlst 0509" width="3228" height="1994" />
                    <h6><span class="label">Figure 5-9. </span>SVM classifiers using an RBF kernel</h6>
                  </div>
                </figure>

                <p>Other kernels exist but are used much more rarely. For example, some kernels are specialized for
                  specific data structures. <em>String kernels</em> <a data-type="indexterm"
                    data-primary="string kernels" id="idm139656376716544" />are sometimes used when classifying text
                  documents or DNA sequences (e.g., using the <em>string subsequence kernel</em> or kernels based on <a
                    data-type="indexterm" data-primary="Levenshtein distance" id="idm139656376715296" />the
                  <em>Levenshtein distance</em>).</p>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>With so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you
                    should always try the linear kernel first (remember that <code>LinearSVC</code> <a
                      data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.svm.LinearSVC"
                      id="sklsvmlsvcch5part2" />is much faster than <code>SVC(kernel="linear")</code>), especially if
                    the training set is very large or if it has plenty of features. If the training set is not too
                    large, you should try the Gaussian RBF kernel as well; it works well in most cases. Then if you have
                    spare time and computing power, you can also experiment with a few other kernels using
                    cross-validation and grid search, especially if there are kernels specialized for your training
                    set’s data <a data-type="indexterm" data-primary="nonlinear SVM classification"
                      data-secondary="Gaussian RBF kernel" data-startref="nsvm5grbf" id="idm139656376710064" /><a
                      data-type="indexterm" data-primary="Gaussian RBF kernel" data-startref="grbfk5"
                      id="idm139656376708848" /><a data-type="indexterm" data-primary="kernels" data-startref="k5"
                      id="idm139656376707904" />structure.</p>
                </div>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Computational Complexity">
              <div class="sect2" id="idm139656376706704">
                <h2>Computational Complexity</h2>

                <p>The<a data-type="indexterm" data-primary="nonlinear SVM classification"
                    data-secondary="computational complexity" id="idm139656376704944" /><a data-type="indexterm"
                    data-primary="computational complexity" id="idm139656376704000" /> <code>LinearSVC</code> class is
                  based on the <em>liblinear</em> library, <a data-type="indexterm" data-primary="liblinear library"
                    id="idm139656376702368" />which implements an <a href="https://homl.info/13">optimized algorithm</a>
                  for linear SVMs.<sup><a data-type="noteref" id="idm139656376700816-marker"
                      href="ch05.xhtml#idm139656376700816">1</a></sup> It does not support the kernel trick, but it
                  scales almost linearly with the number of training instances and the number of features: its training
                  time complexity is roughly <em>O</em>(<em>m</em> × <em>n</em>).</p>

                <p>The algorithm takes longer if you require a very high precision. This is controlled by the <a
                    data-type="indexterm" data-primary="tolerance hyperparameter" id="idm139656376698192" /><a
                    data-type="indexterm" data-primary="hyperparameters" id="idm139656376697520" />tolerance
                  hyperparameter <em>ϵ</em> (called <code>tol</code> in Scikit-Learn). In most classification tasks, the
                  default tolerance is fine.</p>

                <p>The <code>SVC</code> class is based on <a data-type="indexterm" data-primary="libsvm library"
                    id="idm139656376694640" />the <em>libsvm</em> library, which implements <a
                    href="https://homl.info/14">an algorithm</a> that supports the kernel trick.<sup><a
                      data-type="noteref" id="idm139656376692624-marker"
                      href="ch05.xhtml#idm139656376692624">2</a></sup> The training time complexity is usually between
                  <em>O</em>(<em>m</em><sup>2</sup> × <em>n</em>) and <em>O</em>(<em>m</em><sup>3</sup> × <em>n</em>).
                  Unfortunately, this means that it gets dreadfully slow when the number of training instances gets
                  large (e.g., hundreds of thousands of instances). This algorithm is perfect for complex but small or
                  medium training sets. However, it scales well with the number of features, especially with <em>sparse
                    features</em> (i.e., when each instance has few nonzero features). In this case, the algorithm
                  scales roughly with the average number of nonzero features per instance. <a data-type="xref"
                    href="#svm_classification_algorithm_comparison">Table 5-1</a> compares <a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="SVM classification classes"
                    id="idm139656376686912" />Scikit-Learn’s SVM classification <a data-type="indexterm"
                    data-primary="Support Vector Machines (SVMs)" data-secondary="nonlinear classification"
                    data-startref="svm5nc" id="idm139656376685744" /><a data-type="indexterm"
                    data-primary="nonlinear SVM classification" data-startref="nsvmc5"
                    id="idm139656376684448" />classes.</p>
                <table id="svm_classification_algorithm_comparison">
                  <caption><span class="label">Table 5-1. </span>Comparison of Scikit-Learn classes for SVM <a
                      data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.svm.LinearSVC"
                      data-startref="sklsvmlsvcch5part2" id="idm139656376682672" /><a data-type="indexterm"
                      data-primary="Scikit-Learn" data-secondary="sklearn.svm.SVC" data-startref="sklsvmsvcch5"
                      id="idm139656376681424" />classification</caption>
                  <thead>
                    <tr>
                      <th>Class</th>
                      <th>Time complexity</th>
                      <th>Out-of-core support</th>
                      <th>Scaling required</th>
                      <th>Kernel trick</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>
                        <p><code>LinearSVC</code></p>
                      </td>
                      <td>
                        <p>O(<em>m</em> × <em>n</em>)</p>
                      </td>
                      <td>
                        <p>No</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                      <td>
                        <p>No</p>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <p><code>SGDClassifier</code></p>
                      </td>
                      <td>
                        <p>O(<em>m</em> × <em>n</em>)</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                      <td>
                        <p>No</p>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <p><code>SVC</code></p>
                      </td>
                      <td>
                        <p>O(<em>m</em>² × <em>n</em>) to O(<em>m</em>³ × <em>n</em>)</p>
                      </td>
                      <td>
                        <p>No</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                    </tr>
                  </tbody>
                </table>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="SVM Regression">
          <div class="sect1" id="idm139656376989856">
            <h1>SVM Regression</h1>

            <p>As <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="SVM regression"
                id="svm5svmr" />we mentioned earlier, the SVM algorithm is quite versatile: not only does it support
              linear and nonlinear classification, but it also supports linear and nonlinear regression. The trick is to
              reverse the objective: instead of trying to fit the largest possible street between two classes while
              limiting margin violations, SVM Regression tries to fit as many instances as possible <em>on</em> the
              street while limiting margin violations (i.e., instances <em>off</em> the street). The width of the street
              is controlled by a hyperparameter <em>ϵ</em>. <a data-type="xref"
                href="#svm_regression_plot">Figure 5-10</a> shows two linear SVM Regression models trained on some
              random linear data, one with a large margin (<em>ϵ</em> = 1.5) and the other with a small margin
              (<em>ϵ</em> = 0.5).</p>

            <figure>
              <div id="svm_regression_plot" class="figure">
                <img src="mlst_0510.png" alt="mlst 0510" width="2604" height="1095" />
                <h6><span class="label">Figure 5-10. </span>SVM Regression</h6>
              </div>
            </figure>

            <p>Adding more training instances within the margin does not affect the model’s predictions; thus, the model
              is said to <a data-type="indexterm" data-primary="ε-insensitive" id="idm139656376652896" /><a
                data-type="indexterm" data-primary="ε-insensitive" data-primary-sortas="epsilon-insensitive"
                id="idm139656376652192" />be <em>ϵ-insensitive</em>.</p>

            <p>You can use Scikit-Learn’s <code>LinearSVR</code> class <a data-type="indexterm"
                data-primary="Scikit-Learn" data-secondary="sklearn.svm.LinearSVR" id="sklsvmlsvrch5" />to perform
              linear SVM Regression. The following code produces the model represented on the left of <a
                data-type="xref" href="#svm_regression_plot">Figure 5-10</a> (the training data should be scaled and
              centered first):</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">LinearSVR</code>

<code class="n">svm_reg</code> <code class="o">=</code> <code class="n">LinearSVR</code><code class="p">(</code><code class="n">epsilon</code><code class="o">=</code><code class="mf">1.5</code><code class="p">)</code>
<code class="n">svm_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

            <p>To tackle nonlinear regression tasks, you can use a kernelized SVM model. For example, <a
                data-type="xref" href="#svm_with_polynomial_kernel_plot">Figure 5-11</a> shows SVM Regression on a
              random quadratic training set, using a 2<sup>nd</sup>-degree polynomial kernel. There is little
              regularization on the left plot (i.e., a large <code>C</code> value), and much more regularization on the
              right plot (i.e., a small <code>C</code> value).</p>

            <figure>
              <div id="svm_with_polynomial_kernel_plot" class="figure">
                <img src="mlst_0511.png" alt="mlst 0511" width="2595" height="1087" />
                <h6><span class="label">Figure 5-11. </span>SVM regression using a 2<sup>nd</sup>-degree polynomial
                  kernel</h6>
              </div>
            </figure>

            <p>The following code produces the model represented on the left of <a data-type="xref"
                href="#svm_with_polynomial_kernel_plot">Figure 5-11</a> using Scikit-Learn’s <code>SVR</code> class
              (which supports the kernel trick). The <code>SVR</code> class is the regression equivalent of the
              <code>SVC</code> class, and <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="LinearSVR class" id="idm139656376592816" />the <code>LinearSVR</code> class is the
              regression equivalent of the <code>LinearSVC</code> class. The <code>LinearSVR</code> class scales
              linearly with the size of the training set (just like the <code>LinearSVC</code> class), while the
              <code>SVR</code> class gets much too slow when the training set grows <a data-type="indexterm"
                data-primary="Support Vector Machines (SVMs)" data-secondary="SVM regression" data-startref="svm5svmr"
                id="idm139656376589408" />large <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.svm.SVC" id="idm139656376587968" /> <a data-type="indexterm"
                data-primary="Scikit-Learn" data-secondary="sklearn.svm.SVR" id="idm139656376586864" /> <a
                data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.svm.LinearSVC"
                id="idm139656376585760" /> <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.svm.LinearSVR" data-startref="sklsvmlsvrch5" id="idm139656376584656" />(just
              like the <code>SVC</code> class).</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.svm</code> <code class="kn">import</code> <code class="n">SVR</code>

<code class="n">svm_poly_reg</code> <code class="o">=</code> <code class="n">SVR</code><code class="p">(</code><code class="n">kernel</code><code class="o">=</code><code class="s2">"poly"</code><code class="p">,</code> <code class="n">degree</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mi">100</code><code class="p">,</code> <code class="n">epsilon</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
<code class="n">svm_poly_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>
            <div data-type="note" epub:type="note">
              <h6>Note</h6>
              <p>SVMs can also be used for outlier detection; see Scikit-Learn’s documentation for more details.</p>
            </div>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Under the Hood">
          <div class="sect1" id="idm139656376661440">
            <h1>Under the Hood</h1>

            <p>This <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="mechanics of"
                id="svm5mo" />section explains how SVMs make predictions and how their training algorithms work,
              starting with linear SVM classifiers. You can safely skip it and go straight to the exercises at the end
              of this chapter if you are just getting started with Machine Learning, and come back later when you want
              to get a deeper understanding of SVMs.</p>

            <p>First, a word about notations: in <a data-type="xref"
                href="ch04.xhtml#linear_models_chapter">Chapter 4</a> we used the convention of putting all the <a
                data-type="indexterm" data-primary="model parameters" id="idm139656376544928" />model parameters in one
              vector <strong>θ</strong>, including the bias term <em>θ</em><sub>0</sub> and the input feature weights
              <em>θ</em><sub>1</sub> to <em>θ</em><sub><em>n</em></sub>, and adding a bias input <em>x</em><sub>0</sub>
              = 1 to all instances. In this chapter, we will use a different convention, which is more convenient (and
              more common) when you are dealing with SVMs: the bias term will be called <em>b</em> and the feature
              weights vector will be called <strong>w</strong>. No bias feature will be added to the input <a
                data-type="indexterm" data-primary="feature vector" id="idm139656376539744" />feature vectors.</p>








            <section data-type="sect2" data-pdf-bookmark="Decision Function and Predictions">
              <div class="sect2" id="idm139656376545856">
                <h2>Decision Function and Predictions</h2>

                <p>The <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                    data-secondary="decision function and predictions" id="svm5modfap" /><a data-type="indexterm"
                    data-primary="decision function" id="df5" /><a data-type="indexterm" data-primary="predictions"
                    id="p5" />linear SVM classifier model predicts the class of a new instance <strong>x</strong> by
                  simply computing the decision function <strong>w</strong><sup><em>T</em></sup> <strong>x</strong> +
                  <em>b</em> = <em>w</em><sub>1</sub> <em>x</em><sub>1</sub> + ⋯ + <em>w</em><sub><em>n</em></sub>
                  <em>x</em><sub><em>n</em></sub> + <em>b</em>: if the result is positive, the predicted class
                  <em>ŷ</em> is the positive class (1), or else it is the negative class (0); see <a data-type="xref"
                    href="#linear_svm_classifier_prediction">Equation 5-2</a>.</p>
                <div class="fifty-percent" id="linear_svm_classifier_prediction" data-type="equation">
                  <h5><span class="label">Equation 5-2. </span>Linear SVM classifier prediction</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mover accent="true">
                        <mi>y</mi>
                        <mo>^</mo>
                      </mover>
                      <mo>=</mo>
                      <mfenced separators="" open="{" close="">
                        <mtable>
                          <mtr>
                            <mtd columnalign="left">
                              <mn>0</mn>
                            </mtd>
                            <mtd columnalign="left">
                              <mrow>
                                <mtext>if</mtext>
                                <mspace width="4.pt" />
                                <msup>
                                  <mi mathvariant="bold">w</mi>
                                  <mi>T</mi>
                                </msup>
                                <mi mathvariant="bold">x</mi>
                                <mo>+</mo>
                                <mi>b</mi>
                                <mo>&lt;</mo>
                                <mn>0</mn>
                                <mo>,</mo>
                              </mrow>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd columnalign="left">
                              <mn>1</mn>
                            </mtd>
                            <mtd columnalign="left">
                              <mrow>
                                <mtext>if</mtext>
                                <mspace width="4.pt" />
                                <msup>
                                  <mi mathvariant="bold">w</mi>
                                  <mi>T</mi>
                                </msup>
                                <mi mathvariant="bold">x</mi>
                                <mo>+</mo>
                                <mi>b</mi>
                                <mo>≥</mo>
                                <mn>0</mn>
                              </mrow>
                            </mtd>
                          </mtr>
                        </mtable>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <p><a data-type="xref" href="#iris_3D_plot">Figure 5-12</a> shows the decision function that corresponds
                  to the model on the left of <a data-type="xref" href="#regularization_plot">Figure 5-4</a>: it is a
                  two-dimensional plane since this dataset has two features (petal width and petal length). The decision
                  boundary is the set of points where the decision function is equal to 0: it is the intersection of two
                  planes, which is a straight line (represented by the thick solid line).<sup><a data-type="noteref"
                      id="idm139656376480112-marker" href="ch05.xhtml#idm139656376480112">3</a></sup></p>

                <figure>
                  <div id="iris_3D_plot" class="figure">
                    <img src="mlst_0512.png" alt="mlst 0512" width="3071" height="1657" />
                    <h6><span class="label">Figure 5-12. </span>Decision function for the iris dataset</h6>
                  </div>
                </figure>

                <p>The dashed lines represent the points where the decision function is equal to 1 or –1: they are
                  parallel and at equal distance to the decision boundary, forming a margin around it. Training a linear
                  SVM classifier means finding the value of <strong>w</strong> and <em>b</em> that make this margin as
                  wide as possible while avoiding margin violations (hard margin) or <a data-type="indexterm"
                    data-primary="Support Vector Machines (SVMs)" data-secondary="decision function and predictions"
                    data-startref="svm5modfap" id="idm139656376473232" /><a data-type="indexterm"
                    data-primary="decision function" data-startref="df5" id="idm139656376471904" /><a
                    data-type="indexterm" data-primary="predictions" data-startref="p5"
                    id="idm139656376470960" />limiting them (soft margin).</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Training Objective">
              <div class="sect2" id="idm139656376469760">
                <h2>Training Objective</h2>

                <p>Consider <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                    data-secondary="training objective" id="svm5moto" /><a data-type="indexterm"
                    data-primary="training objectives" id="to5" />the slope of the decision function: it is equal to the
                  norm of the weight vector, ∥ <strong>w</strong> ∥. If we divide this slope by 2, the points where the
                  decision function is equal to ±1 are going to be twice as far away from the decision boundary. In
                  other words, dividing the slope by 2 will multiply the margin by 2. Perhaps this is easier to
                  visualize in 2D in <a data-type="xref" href="#small_w_large_margin_plot">Figure 5-13</a>. The smaller
                  the weight vector <strong>w</strong>, the larger the margin.</p>

                <figure>
                  <div id="small_w_large_margin_plot" class="figure">
                    <img src="mlst_0513.png" alt="mlst 0513" width="3498" height="855" />
                    <h6><span class="label">Figure 5-13. </span>A smaller weight vector results in a larger margin</h6>
                  </div>
                </figure>

                <p>So we want to minimize ∥ <strong>w</strong> ∥ to get a large margin. However, if we also want to
                  avoid any margin violation (hard margin), then we need the decision function to be greater than 1 for
                  all positive training instances, and lower than –1 for negative training instances. If we define
                  <em>t</em><sup><em>(i)</em></sup> = –1 for negative instances (if <em>y</em><sup><em>(i)</em></sup> =
                  0) and <em>t</em><sup><em>(i)</em></sup> = 1 for positive instances (if
                  <em>y</em><sup><em>(i)</em></sup> = 1), then we can express this constraint as
                  <em>t</em><sup><em>(i)</em></sup>(<strong>w</strong><sup><em>T</em></sup>
                  <strong>x</strong><sup><em>(i)</em></sup> + <em>b</em>) ≥ 1 for all instances.</p>

                <p>We can therefore express the hard margin linear SVM classifier objective as the <em>constrained
                    optimization</em> <a data-type="indexterm" data-primary="constrained optimization"
                    id="idm139656376453312" />problem in <a data-type="xref" href="#hard_margin_objective">Equation
                    5-3</a>.</p>
                <div id="hard_margin_objective" data-type="equation">
                  <h5><span class="label">Equation 5-3. </span>Hard margin linear SVM classifier objective</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <munder>
                              <mo form="prefix">minimize</mo>
                              <mrow>
                                <mi mathvariant="bold">w</mi>
                                <mo>,</mo>
                                <mi>b</mi>
                              </mrow>
                            </munder>
                            <mspace width="1.em" />
                            <mrow>
                              <mfrac>
                                <mn>1</mn>
                                <mn>2</mn>
                              </mfrac>
                              <msup>
                                <mi mathvariant="bold">w</mi>
                                <mi>T</mi>
                              </msup>
                              <mi mathvariant="bold">w</mi>
                            </mrow>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <mtext>subject</mtext>
                            <mspace width="4.pt" />
                            <mtext>to</mtext>
                            <mspace width="1.em" />
                            <msup>
                              <mi>t</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mrow>
                              <mo>(</mo>
                              <msup>
                                <mi mathvariant="bold">w</mi>
                                <mi>T</mi>
                              </msup>
                              <msup>
                                <mi mathvariant="bold">x</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>+</mo>
                              <mi>b</mi>
                              <mo>)</mo>
                            </mrow>
                            <mo>≥</mo>
                            <mn>1</mn>
                            <mspace width="1.em" />
                            <mtext>for</mtext>
                            <mspace width="4.pt" />
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                            <mo>,</mo>
                            <mn>2</mn>
                            <mo>,</mo>
                            <mo>⋯</mo>
                            <mo>,</mo>
                            <mi>m</mi>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>
                <div data-type="note" epub:type="note">
                  <h6>Note</h6>
                  <p>We are minimizing <math xmlns="http://www.w3.org/1998/Math/MathML" alttext="one-half">
                      <mfrac>
                        <mn>1</mn>
                        <mn>2</mn>
                      </mfrac>
                    </math><strong>w</strong><sup><em>T</em></sup> <strong>w</strong>, which is equal to <math
                      xmlns="http://www.w3.org/1998/Math/MathML" alttext="one-half">
                      <mfrac>
                        <mn>1</mn>
                        <mn>2</mn>
                      </mfrac>
                    </math>∥ <strong>w</strong> ∥<sup>2</sup>, rather than minimizing ∥ <strong>w</strong> ∥. Indeed,
                    <math xmlns="http://www.w3.org/1998/Math/MathML" alttext="one-half">
                      <mfrac>
                        <mn>1</mn>
                        <mn>2</mn>
                      </mfrac>
                    </math>∥ <strong>w</strong> ∥<sup>2</sup> has a nice and simple derivative (it is just
                    <strong>w</strong>) while ∥ <strong>w</strong> ∥ is not differentiable at <strong>w</strong> =
                    <strong>0</strong>. Optimization algorithms work much better on differentiable functions.</p>
                </div>

                <p>To get the soft margin objective, we need to introduce <a data-type="indexterm"
                    data-primary="slack variable" id="idm139656376385104" />a <em>slack variable</em>
                  <em>ζ</em><sup><em>(i)</em></sup> ≥ 0 for each instance:<sup><a data-type="noteref"
                      id="idm139656376382608-marker" href="ch05.xhtml#idm139656376382608">4</a></sup>
                  <em>ζ</em><sup><em>(i)</em></sup> measures how much the i<sup>th</sup> instance is allowed to violate
                  the margin. We now have two conflicting objectives: making the slack variables as small as possible to
                  reduce the margin violations, and making <math xmlns="http://www.w3.org/1998/Math/MathML"
                    alttext="one-half">
                    <mfrac>
                      <mn>1</mn>
                      <mn>2</mn>
                    </mfrac>
                  </math><strong>w</strong><sup><em>T</em></sup> <strong>w</strong> as small as possible to increase the
                  margin. This is where the <code>C</code> hyperparameter comes in: it allows us to define the tradeoff
                  between these two objectives. This gives us the constrained <a data-type="indexterm"
                    data-primary="Support Vector Machines (SVMs)" data-secondary="training objective"
                    data-startref="svm5moto" id="idm139656376375904" /><a data-type="indexterm"
                    data-primary="training objectives" data-startref="to5" id="idm139656376374592" />optimization
                  problem in <a data-type="xref" href="#soft_margin_objective">Equation 5-4</a>.</p>
                <div id="soft_margin_objective" data-type="equation">
                  <h5><span class="label">Equation 5-4. </span>Soft margin linear SVM classifier objective</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <munder>
                              <mo form="prefix">minimize</mo>
                              <mrow>
                                <mi mathvariant="bold">w</mi>
                                <mo>,</mo>
                                <mi>b</mi>
                                <mo>,</mo>
                                <mi>ζ</mi>
                              </mrow>
                            </munder>
                            <mspace width="1.em" />
                            <mrow>
                              <mstyle scriptlevel="0" displaystyle="true">
                                <mfrac>
                                  <mn>1</mn>
                                  <mn>2</mn>
                                </mfrac>
                              </mstyle>
                              <msup>
                                <mi mathvariant="bold">w</mi>
                                <mi>T</mi>
                              </msup>
                              <mi mathvariant="bold">w</mi>
                              <mo>+</mo>
                              <mi>C</mi>
                              <munderover>
                                <mo>∑</mo>
                                <mrow>
                                  <mi>i</mi>
                                  <mo>=</mo>
                                  <mn>1</mn>
                                </mrow>
                                <mi>m</mi>
                              </munderover>
                              <msup>
                                <mi>ζ</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                            </mrow>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <mtext>subject</mtext>
                            <mspace width="4.pt" />
                            <mtext>to</mtext>
                            <mspace width="1.em" />
                            <msup>
                              <mi>t</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mrow>
                              <mo>(</mo>
                              <msup>
                                <mi mathvariant="bold">w</mi>
                                <mi>T</mi>
                              </msup>
                              <msup>
                                <mi mathvariant="bold">x</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>+</mo>
                              <mi>b</mi>
                              <mo>)</mo>
                            </mrow>
                            <mo>≥</mo>
                            <mn>1</mn>
                            <mo>-</mo>
                            <msup>
                              <mi>ζ</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mspace width="1.em" />
                            <mtext>and</mtext>
                            <mspace width="1.em" />
                            <msup>
                              <mi>ζ</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mo>≥</mo>
                            <mn>0</mn>
                            <mspace width="1.em" />
                            <mtext>for</mtext>
                            <mspace width="4.pt" />
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                            <mo>,</mo>
                            <mn>2</mn>
                            <mo>,</mo>
                            <mo>⋯</mo>
                            <mo>,</mo>
                            <mi>m</mi>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Quadratic Programming">
              <div class="sect2" id="quadratic_programming_paragraph">
                <h2>Quadratic Programming</h2>

                <p>The <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                    data-secondary="Quadratic Programming (QP) problems" id="svm5moqpp" /><a data-type="indexterm"
                    data-primary="Quadratic Programming (QP) Problems" id="qpp5" />hard margin and soft margin problems
                  are both convex quadratic optimization problems with linear constraints. Such problems are known as
                  <em>Quadratic Programming</em> (QP) problems. Many off-the-shelf solvers are available to solve QP
                  problems using a variety of techniques that are outside the scope of this book.<sup><a
                      data-type="noteref" id="idm139656376325360-marker"
                      href="ch05.xhtml#idm139656376325360">5</a></sup> The general problem formulation is given by <a
                    data-type="xref" href="#quadratic_programming_problem_formulation">Equation 5-5</a>.</p>
                <div id="quadratic_programming_problem_formulation" data-type="equation">
                  <h5><span class="label">Equation 5-5. </span>Quadratic Programming problem</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <munder>
                              <mtext>Minimize</mtext>
                              <mi mathvariant="bold">p</mi>
                            </munder>
                            <mspace width="1.em" />
                          </mrow>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mstyle scriptlevel="0" displaystyle="true">
                              <mfrac>
                                <mn>1</mn>
                                <mn>2</mn>
                              </mfrac>
                            </mstyle>
                            <msup>
                              <mi mathvariant="bold">p</mi>
                              <mi>T</mi>
                            </msup>
                            <mi mathvariant="bold">H</mi>
                            <mi mathvariant="bold">p</mi>
                            <mspace width="1.em" />
                            <mo>+</mo>
                            <mspace width="1.em" />
                            <msup>
                              <mi mathvariant="bold">f</mi>
                              <mi>T</mi>
                            </msup>
                            <mi mathvariant="bold">p</mi>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <mtext>subject</mtext>
                            <mspace width="4.pt" />
                            <mtext>to</mtext>
                            <mspace width="1.em" />
                          </mrow>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mi mathvariant="bold">A</mi>
                            <mi mathvariant="bold">p</mi>
                            <mo>≤</mo>
                            <mi mathvariant="bold">b</mi>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <mtext>where</mtext>
                            <mspace width="4.pt" />
                          </mrow>
                        </mtd>
                        <mtd columnalign="left">
                          <mfenced separators="" open="{" close="">
                            <mtable>
                              <mtr>
                                <mtd columnalign="left">
                                  <mi mathvariant="bold">p</mi>
                                </mtd>
                                <mtd columnalign="left">
                                  <mrow>
                                    <mspace width="4.pt" />
                                    <mtext>is</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>an</mtext>
                                    <mspace width="4.pt" />
                                    <msub>
                                      <mi>n</mi>
                                      <mi>p</mi>
                                    </msub>
                                    <mtext>-dimensional</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>vector</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>(</mtext>
                                    <msub>
                                      <mi>n</mi>
                                      <mi>p</mi>
                                    </msub>
                                    <mo>=</mo>
                                    <mtext>number</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>of</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>parameters),</mtext>
                                  </mrow>
                                </mtd>
                              </mtr>
                              <mtr>
                                <mtd columnalign="left">
                                  <mi mathvariant="bold">H</mi>
                                </mtd>
                                <mtd columnalign="left">
                                  <mrow>
                                    <mspace width="4.pt" />
                                    <mtext>is</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>an</mtext>
                                    <mspace width="4.pt" />
                                    <msub>
                                      <mi>n</mi>
                                      <mi>p</mi>
                                    </msub>
                                    <mo>×</mo>
                                    <msub>
                                      <mi>n</mi>
                                      <mi>p</mi>
                                    </msub>
                                    <mspace width="4.pt" />
                                    <mtext>matrix,</mtext>
                                  </mrow>
                                </mtd>
                              </mtr>
                              <mtr>
                                <mtd columnalign="left">
                                  <mi mathvariant="bold">f</mi>
                                </mtd>
                                <mtd columnalign="left">
                                  <mrow>
                                    <mspace width="4.pt" />
                                    <mtext>is</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>an</mtext>
                                    <mspace width="4.pt" />
                                    <msub>
                                      <mi>n</mi>
                                      <mi>p</mi>
                                    </msub>
                                    <mtext>-dimensional</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>vector,</mtext>
                                  </mrow>
                                </mtd>
                              </mtr>
                              <mtr>
                                <mtd columnalign="left">
                                  <mi mathvariant="bold">A</mi>
                                </mtd>
                                <mtd columnalign="left">
                                  <mrow>
                                    <mspace width="4.pt" />
                                    <mtext>is</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>an</mtext>
                                    <mspace width="4.pt" />
                                    <msub>
                                      <mi>n</mi>
                                      <mi>c</mi>
                                    </msub>
                                    <mo>×</mo>
                                    <msub>
                                      <mi>n</mi>
                                      <mi>p</mi>
                                    </msub>
                                    <mspace width="4.pt" />
                                    <mtext>matrix</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>(</mtext>
                                    <msub>
                                      <mi>n</mi>
                                      <mi>c</mi>
                                    </msub>
                                    <mo>=</mo>
                                    <mtext>number</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>of</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>constraints),</mtext>
                                  </mrow>
                                </mtd>
                              </mtr>
                              <mtr>
                                <mtd columnalign="left">
                                  <mi mathvariant="bold">b</mi>
                                </mtd>
                                <mtd columnalign="left">
                                  <mrow>
                                    <mspace width="4.pt" />
                                    <mtext>is</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>an</mtext>
                                    <mspace width="4.pt" />
                                    <msub>
                                      <mi>n</mi>
                                      <mi>c</mi>
                                    </msub>
                                    <mtext>-dimensional</mtext>
                                    <mspace width="4.pt" />
                                    <mtext>vector.</mtext>
                                  </mrow>
                                </mtd>
                              </mtr>
                            </mtable>
                          </mfenced>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>

                <p>Note that the expression <strong>A</strong> <strong>p</strong> ≤ <strong>b</strong> actually defines
                  <em>n</em><sub><em>c</em></sub> constraints: <strong>p</strong><sup><em>T</em></sup>
                  <strong>a</strong><sup><em>(i)</em></sup> ≤ <em>b</em><sup><em>(i)</em></sup> for <em>i</em> = 1, 2,
                  ⋯, <em>n</em><sub><em>c</em></sub>, where <strong>a</strong><sup><em>(i)</em></sup> is the vector
                  containing the elements of the i<sup>th</sup> row of <strong>A</strong> and
                  <em>b</em><sup><em>(i)</em></sup> is the i<sup>th</sup> element of <strong>b</strong>.</p>

                <p>You can easily verify that if you set the QP <a data-type="indexterm" data-primary="model parameters"
                    id="idm139656376228272" />parameters in the following way, you get the hard margin linear SVM
                  classifier objective:</p>

                <ul>
                  <li>
                    <p><em>n</em><sub><em>p</em></sub> = <em>n</em> + 1, where <em>n</em> is the number of features (the
                      +1 is for the bias term).</p>
                  </li>
                  <li>
                    <p><em>n</em><sub><em>c</em></sub> = <em>m</em>, where <em>m</em> is the number of training
                      instances.</p>
                  </li>
                  <li>
                    <p><strong>H</strong> is the <em>n</em><sub><em>p</em></sub> × <em>n</em><sub><em>p</em></sub> <a
                        data-type="indexterm" data-primary="identity matrix" id="idm139656376218992" />identity matrix,
                      except with a zero in the top-left cell (to ignore the bias term).</p>
                  </li>
                  <li>
                    <p><strong>f</strong> = <strong>0</strong>, an <em>n</em><sub><em>p</em></sub>-dimensional vector
                      full of 0s.</p>
                  </li>
                  <li>
                    <p><strong>b</strong> = <strong>–1</strong>, an <em>n</em><sub><em>c</em></sub>-dimensional vector
                      full of –1s.</p>
                  </li>
                  <li>
                    <p><strong>a</strong><sup><em>(i)</em></sup> = –<em>t</em><sup><em>(i)</em></sup> <math
                        xmlns="http://www.w3.org/1998/Math/MathML">
                        <mover accent="true">
                          <mi mathvariant="bold">x</mi>
                          <mo>˙</mo>
                        </mover>
                      </math> <sup><em>(i)</em></sup>, where <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mover accent="true">
                          <mi mathvariant="bold">x</mi>
                          <mo>˙</mo>
                        </mover>
                      </math> <sup><em>(i)</em></sup> is equal to <strong>x</strong><sup><em>(i)</em></sup> with an
                      extra bias feature <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mover accent="true">
                          <mi mathvariant="bold">x</mi>
                          <mo>˙</mo>
                        </mover>
                      </math> <sub>0</sub> = 1.</p>
                  </li>
                </ul>

                <p>So one way to train a hard margin linear SVM classifier is just to use an off-the-shelf QP solver by
                  passing it the preceding parameters. The resulting vector <strong>p</strong> will contain the bias
                  term <em>b</em> = <em>p</em><sub>0</sub> and the feature weights <em>w</em><sub><em>i</em></sub> =
                  <em>p</em><sub><em>i</em></sub> for <em>i</em> = 1, 2, ⋯, <em>n</em>. Similarly, you can use a QP
                  solver to solve the soft margin problem (see the exercises at the end of the chapter).</p>

                <p>However, to use the kernel trick we are going to look at a different constrained optimization <a
                    data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                    data-secondary="Quadratic Programming (QP) problems" data-startref="svm5moqpp"
                    id="idm139656376196464" /><a data-type="indexterm"
                    data-primary="Quadratic Programming (QP) Problems" data-startref="qpp5"
                    id="idm139656376195280" />problem.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="The Dual Problem">
              <div class="sect2" id="idm139656376194112">
                <h2>The Dual Problem</h2>

                <p>Given <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                    data-secondary="the dual problem" id="idm139656376192352" />a constrained optimization problem,
                  known as <a data-type="indexterm" data-primary="primal problem" id="idm139656376191216" />the
                  <em>primal problem</em>, it is possible to express a different but closely related problem, called <a
                    data-type="indexterm" data-primary="dual problem" id="idm139656376189968" />its <em>dual
                    problem</em>. The solution to the dual problem typically gives a lower bound to the solution of the
                  primal problem, but under some conditions it can even have the same solutions as the primal problem.
                  Luckily, the SVM problem happens to meet these conditions,<sup><a data-type="noteref"
                      id="idm139656376188464-marker" href="ch05.xhtml#idm139656376188464">6</a></sup> so you can choose
                  to solve the primal problem or the dual problem; both will have the same solution. <a data-type="xref"
                    href="#svm_dual_form">Equation 5-6</a> shows the dual form of the linear SVM objective (if you are
                  interested in knowing how to derive the dual problem from the primal problem, see Appendix C).</p>
                <div id="svm_dual_form" data-type="equation">
                  <h5><span class="label">Equation 5-6. </span>Dual form of the linear SVM objective</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <munder>
                              <mo form="prefix">minimize</mo>
                              <mi>α</mi>
                            </munder>
                            <mstyle scriptlevel="0" displaystyle="true">
                              <mfrac>
                                <mn>1</mn>
                                <mn>2</mn>
                              </mfrac>
                            </mstyle>
                            <munderover>
                              <mo>∑</mo>
                              <mrow>
                                <mi>i</mi>
                                <mo>=</mo>
                                <mn>1</mn>
                              </mrow>
                              <mi>m</mi>
                            </munderover>
                            <mrow>
                              <munderover>
                                <mo>∑</mo>
                                <mrow>
                                  <mi>j</mi>
                                  <mo>=</mo>
                                  <mn>1</mn>
                                </mrow>
                                <mi>m</mi>
                              </munderover>
                              <mrow>
                                <msup>
                                  <mi>α</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>i</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <msup>
                                  <mi>α</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>j</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <msup>
                                  <mi>t</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>i</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <msup>
                                  <mi>t</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>j</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <msup>
                                  <mrow>
                                    <msup>
                                      <mi mathvariant="bold">x</mi>
                                      <mrow>
                                        <mo>(</mo>
                                        <mi>i</mi>
                                        <mo>)</mo>
                                      </mrow>
                                    </msup>
                                  </mrow>
                                  <mi>T</mi>
                                </msup>
                                <msup>
                                  <mi mathvariant="bold">x</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>j</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                              </mrow>
                            </mrow>
                            <mspace width="1.em" />
                            <mo>-</mo>
                            <mspace width="1.em" />
                            <munderover>
                              <mo>∑</mo>
                              <mrow>
                                <mi>i</mi>
                                <mo>=</mo>
                                <mn>1</mn>
                              </mrow>
                              <mi>m</mi>
                            </munderover>
                            <msup>
                              <mi>α</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <mtext>subject</mtext>
                            <mspace width="4.pt" />
                            <mtext>to</mtext>
                            <mspace width="1.em" />
                            <msup>
                              <mi>α</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mo>≥</mo>
                            <mn>0</mn>
                            <mspace width="1.em" />
                            <mtext>for</mtext>
                            <mspace width="4.pt" />
                            <mi>i</mi>
                            <mo>=</mo>
                            <mn>1</mn>
                            <mo>,</mo>
                            <mn>2</mn>
                            <mo>,</mo>
                            <mo>⋯</mo>
                            <mo>,</mo>
                            <mi>m</mi>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>

                <p>Once you find the vector <math xmlns="http://www.w3.org/1998/Math/MathML"
                    alttext="ModifyingAbove alpha With caret">
                    <mover accent="true">
                      <mi>α</mi>
                      <mo>^</mo>
                    </mover>
                  </math> that minimizes this equation (using a QP solver), you can compute <math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mover accent="true">
                      <mi mathvariant="bold">w</mi>
                      <mo>^</mo>
                    </mover>
                  </math> and <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mover accent="true">
                      <mi>b</mi>
                      <mo>^</mo>
                    </mover>
                  </math> that minimize the primal problem by using <a data-type="xref"
                    href="#from_alpha_to_w_and_b">Equation 5-7</a>.</p>
                <div class="fifty-percent" id="from_alpha_to_w_and_b" data-type="equation">
                  <h5><span class="label">Equation 5-7. </span>From the dual solution to the primal solution</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <mover accent="true">
                              <mi mathvariant="bold">w</mi>
                              <mo>^</mo>
                            </mover>
                            <mo>=</mo>
                            <munderover>
                              <mo>∑</mo>
                              <mrow>
                                <mi>i</mi>
                                <mo>=</mo>
                                <mn>1</mn>
                              </mrow>
                              <mi>m</mi>
                            </munderover>
                            <msup>
                              <mrow>
                                <mover accent="true">
                                  <mi>α</mi>
                                  <mo>^</mo>
                                </mover>
                              </mrow>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <msup>
                              <mi>t</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <msup>
                              <mi mathvariant="bold">x</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <mover accent="true">
                              <mi>b</mi>
                              <mo>^</mo>
                            </mover>
                            <mo>=</mo>
                            <mstyle scriptlevel="0" displaystyle="true">
                              <mfrac>
                                <mn>1</mn>
                                <msub>
                                  <mi>n</mi>
                                  <mi>s</mi>
                                </msub>
                              </mfrac>
                            </mstyle>
                            <munderover>
                              <mo>∑</mo>
                              <mfrac linethickness="0pt">
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <mi>i</mi>
                                    <mo>=</mo>
                                    <mn>1</mn>
                                  </mrow>
                                </mstyle>
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <msup>
                                      <mrow>
                                        <mover accent="true">
                                          <mi>α</mi>
                                          <mo>^</mo>
                                        </mover>
                                      </mrow>
                                      <mrow>
                                        <mo>(</mo>
                                        <mi>i</mi>
                                        <mo>)</mo>
                                      </mrow>
                                    </msup>
                                    <mo>&gt;</mo>
                                    <mn>0</mn>
                                  </mrow>
                                </mstyle>
                              </mfrac>
                              <mi>m</mi>
                            </munderover>
                            <mfenced separators="" open="(" close=")">
                              <msup>
                                <mi>t</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>-</mo>
                              <mrow>
                                <msup>
                                  <mrow>
                                    <mover accent="true">
                                      <mi mathvariant="bold">w</mi>
                                      <mo>^</mo>
                                    </mover>
                                  </mrow>
                                  <mi>T</mi>
                                </msup>
                                <msup>
                                  <mi mathvariant="bold">x</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>i</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                              </mrow>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>

                <p>The dual problem is faster to solve than the primal when the number of training instances is smaller
                  than the number of features. More importantly, it makes the kernel trick possible, while the primal
                  does not. So what is this <a data-type="indexterm" data-primary="kernel trick" id="kt5" />kernel trick
                  anyway?</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Kernelized SVM">
              <div class="sect2" id="idm139656376094800">
                <h2>Kernelized SVM</h2>

                <p>Suppose <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                    data-secondary="kernelized SVM" id="svm5moksvm" /><a data-type="indexterm"
                    data-primary="kernelized SVM" id="ksvm5" />you want to apply a 2<sup>nd</sup>-degree polynomial
                  transformation to a two-dimensional training set (such as the moons training set), then train a linear
                  SVM classifier on the transformed training set. <a data-type="xref"
                    href="#example_second_degree_polynomial_mapping">Equation 5-8</a> shows the 2<sup>nd</sup>-degree
                  polynomial mapping function <em>ϕ</em> that you want to apply.</p>
                <div class="fifty-percent" id="example_second_degree_polynomial_mapping" data-type="equation">
                  <h5><span class="label">Equation 5-8. </span>Second-degree polynomial mapping</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mi>ϕ</mi>
                      <mfenced open="(" close=")">
                        <mi mathvariant="bold">x</mi>
                      </mfenced>
                      <mo>=</mo>
                      <mi>ϕ</mi>
                      <mfenced open="(" close=")">
                        <mfenced open="(" close=")">
                          <mtable>
                            <mtr>
                              <mtd>
                                <msub>
                                  <mi>x</mi>
                                  <mn>1</mn>
                                </msub>
                              </mtd>
                            </mtr>
                            <mtr>
                              <mtd>
                                <msub>
                                  <mi>x</mi>
                                  <mn>2</mn>
                                </msub>
                              </mtd>
                            </mtr>
                          </mtable>
                        </mfenced>
                      </mfenced>
                      <mo>=</mo>
                      <mfenced open="(" close=")">
                        <mtable>
                          <mtr>
                            <mtd>
                              <msup>
                                <mrow>
                                  <msub>
                                    <mi>x</mi>
                                    <mn>1</mn>
                                  </msub>
                                </mrow>
                                <mn>2</mn>
                              </msup>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mrow>
                                <msqrt>
                                  <mn>2</mn>
                                </msqrt>
                                <mspace width="0.166667em" />
                                <msub>
                                  <mi>x</mi>
                                  <mn>1</mn>
                                </msub>
                                <msub>
                                  <mi>x</mi>
                                  <mn>2</mn>
                                </msub>
                              </mrow>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <msup>
                                <mrow>
                                  <msub>
                                    <mi>x</mi>
                                    <mn>2</mn>
                                  </msub>
                                </mrow>
                                <mn>2</mn>
                              </msup>
                            </mtd>
                          </mtr>
                        </mtable>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <p>Notice that the transformed vector is three-dimensional instead of two-dimensional. Now let’s look at
                  what happens to a couple of two-dimensional vectors, <strong>a</strong> and <strong>b</strong>, if we
                  apply this 2<sup>nd</sup>-degree polynomial mapping and then compute the dot product<sup><a
                      data-type="noteref" id="idm139656376061008-marker"
                      href="ch05.xhtml#idm139656376061008">7</a></sup> of the transformed vectors (See <a
                    data-type="xref" href="#kernel_trick_for_second_degree_polynomial_mapping">Equation 5-9</a>).</p>
                <div class="pagebreak-before less_space" id="kernel_trick_for_second_degree_polynomial_mapping"
                  data-type="equation">
                  <h5><span class="label">Equation 5-9. </span>Kernel trick for a 2<sup>nd</sup>-degree polynomial
                    mapping</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <mi>ϕ</mi>
                            <msup>
                              <mrow>
                                <mo>(</mo>
                                <mi mathvariant="bold">a</mi>
                                <mo>)</mo>
                              </mrow>
                              <mi>T</mi>
                            </msup>
                            <mi>ϕ</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi mathvariant="bold">b</mi>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="1.em" />
                            <mo>=</mo>
                            <msup>
                              <mfenced open="(" close=")">
                                <mtable>
                                  <mtr>
                                    <mtd>
                                      <msup>
                                        <mrow>
                                          <msub>
                                            <mi>a</mi>
                                            <mn>1</mn>
                                          </msub>
                                        </mrow>
                                        <mn>2</mn>
                                      </msup>
                                    </mtd>
                                  </mtr>
                                  <mtr>
                                    <mtd>
                                      <mrow>
                                        <msqrt>
                                          <mn>2</mn>
                                        </msqrt>
                                        <mspace width="0.166667em" />
                                        <msub>
                                          <mi>a</mi>
                                          <mn>1</mn>
                                        </msub>
                                        <msub>
                                          <mi>a</mi>
                                          <mn>2</mn>
                                        </msub>
                                      </mrow>
                                    </mtd>
                                  </mtr>
                                  <mtr>
                                    <mtd>
                                      <msup>
                                        <mrow>
                                          <msub>
                                            <mi>a</mi>
                                            <mn>2</mn>
                                          </msub>
                                        </mrow>
                                        <mn>2</mn>
                                      </msup>
                                    </mtd>
                                  </mtr>
                                </mtable>
                              </mfenced>
                              <mi>T</mi>
                            </msup>
                            <mfenced open="(" close=")">
                              <mtable>
                                <mtr>
                                  <mtd>
                                    <msup>
                                      <mrow>
                                        <msub>
                                          <mi>b</mi>
                                          <mn>1</mn>
                                        </msub>
                                      </mrow>
                                      <mn>2</mn>
                                    </msup>
                                  </mtd>
                                </mtr>
                                <mtr>
                                  <mtd>
                                    <mrow>
                                      <msqrt>
                                        <mn>2</mn>
                                      </msqrt>
                                      <mspace width="0.166667em" />
                                      <msub>
                                        <mi>b</mi>
                                        <mn>1</mn>
                                      </msub>
                                      <msub>
                                        <mi>b</mi>
                                        <mn>2</mn>
                                      </msub>
                                    </mrow>
                                  </mtd>
                                </mtr>
                                <mtr>
                                  <mtd>
                                    <msup>
                                      <mrow>
                                        <msub>
                                          <mi>b</mi>
                                          <mn>2</mn>
                                        </msub>
                                      </mrow>
                                      <mn>2</mn>
                                    </msup>
                                  </mtd>
                                </mtr>
                              </mtable>
                            </mfenced>
                            <mo>=</mo>
                            <msup>
                              <mrow>
                                <msub>
                                  <mi>a</mi>
                                  <mn>1</mn>
                                </msub>
                              </mrow>
                              <mn>2</mn>
                            </msup>
                            <msup>
                              <mrow>
                                <msub>
                                  <mi>b</mi>
                                  <mn>1</mn>
                                </msub>
                              </mrow>
                              <mn>2</mn>
                            </msup>
                            <mo>+</mo>
                            <mn>2</mn>
                            <msub>
                              <mi>a</mi>
                              <mn>1</mn>
                            </msub>
                            <msub>
                              <mi>b</mi>
                              <mn>1</mn>
                            </msub>
                            <msub>
                              <mi>a</mi>
                              <mn>2</mn>
                            </msub>
                            <msub>
                              <mi>b</mi>
                              <mn>2</mn>
                            </msub>
                            <mo>+</mo>
                            <msup>
                              <mrow>
                                <msub>
                                  <mi>a</mi>
                                  <mn>2</mn>
                                </msub>
                              </mrow>
                              <mn>2</mn>
                            </msup>
                            <msup>
                              <mrow>
                                <msub>
                                  <mi>b</mi>
                                  <mn>2</mn>
                                </msub>
                              </mrow>
                              <mn>2</mn>
                            </msup>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="1.em" />
                            <mo>=</mo>
                            <msup>
                              <mfenced separators="" open="(" close=")">
                                <msub>
                                  <mi>a</mi>
                                  <mn>1</mn>
                                </msub>
                                <msub>
                                  <mi>b</mi>
                                  <mn>1</mn>
                                </msub>
                                <mo>+</mo>
                                <msub>
                                  <mi>a</mi>
                                  <mn>2</mn>
                                </msub>
                                <msub>
                                  <mi>b</mi>
                                  <mn>2</mn>
                                </msub>
                              </mfenced>
                              <mn>2</mn>
                            </msup>
                            <mo>=</mo>
                            <msup>
                              <mfenced separators="" open="(" close=")">
                                <msup>
                                  <mfenced open="(" close=")">
                                    <mtable>
                                      <mtr>
                                        <mtd>
                                          <msub>
                                            <mi>a</mi>
                                            <mn>1</mn>
                                          </msub>
                                        </mtd>
                                      </mtr>
                                      <mtr>
                                        <mtd>
                                          <msub>
                                            <mi>a</mi>
                                            <mn>2</mn>
                                          </msub>
                                        </mtd>
                                      </mtr>
                                    </mtable>
                                  </mfenced>
                                  <mi>T</mi>
                                </msup>
                                <mfenced open="(" close=")">
                                  <mtable>
                                    <mtr>
                                      <mtd>
                                        <msub>
                                          <mi>b</mi>
                                          <mn>1</mn>
                                        </msub>
                                      </mtd>
                                    </mtr>
                                    <mtr>
                                      <mtd>
                                        <msub>
                                          <mi>b</mi>
                                          <mn>2</mn>
                                        </msub>
                                      </mtd>
                                    </mtr>
                                  </mtable>
                                </mfenced>
                              </mfenced>
                              <mn>2</mn>
                            </msup>
                            <mo>=</mo>
                            <msup>
                              <mrow>
                                <mo>(</mo>
                                <msup>
                                  <mi mathvariant="bold">a</mi>
                                  <mi>T</mi>
                                </msup>
                                <mi mathvariant="bold">b</mi>
                                <mo>)</mo>
                              </mrow>
                              <mn>2</mn>
                            </msup>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>

                <p>How about that? The dot product of the transformed vectors is equal to the square of the dot product
                  of the original vectors: <em>ϕ</em>(<strong>a</strong>)<sup><em>T</em></sup>
                  <em>ϕ</em>(<strong>b</strong>) = (<strong>a</strong><sup><em>T</em></sup>
                  <strong>b</strong>)<sup>2</sup>.</p>

                <p>Now here is the key insight: if you apply the transformation <em>ϕ</em> to all training instances,
                  then the dual problem (see <a data-type="xref" href="#svm_dual_form">Equation 5-6</a>) will contain
                  the dot product <em>ϕ</em>(<strong>x</strong><sup><em>(i)</em></sup>)<sup><em>T</em></sup>
                  <em>ϕ</em>(<strong>x</strong><sup><em>(j)</em></sup>). But if <em>ϕ</em> is the 2<sup>nd</sup>-degree
                  polynomial transformation defined in <a data-type="xref"
                    href="#example_second_degree_polynomial_mapping">Equation 5-8</a>, then you can replace this dot
                  product of transformed vectors simply by <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msup>
                      <mrow>
                        <mo>(</mo>
                        <msup>
                          <mrow>
                            <msup>
                              <mi mathvariant="bold">x</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                          </mrow>
                          <mi>T</mi>
                        </msup>
                        <msup>
                          <mi mathvariant="bold">x</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>j</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                        <mo>)</mo>
                      </mrow>
                      <mn>2</mn>
                    </msup>
                  </math>. So you don’t actually need to transform the training instances at all: just replace the dot
                  product by its square in <a data-type="xref" href="#svm_dual_form">Equation 5-6</a>. The result will
                  be strictly the same as if you went through the trouble of actually transforming the training set then
                  fitting a linear SVM algorithm, but this trick makes the whole process much more computationally
                  efficient. This is the essence of the kernel trick.</p>

                <p>The function <em>K</em>(<strong>a</strong>, <strong>b</strong>) =
                  (<strong>a</strong><sup><em>T</em></sup> <strong>b</strong>)<sup>2</sup> is called a
                  2<sup>nd</sup>-degree <em>polynomial kernel</em>. <a data-type="indexterm"
                    data-primary="polynomial kernel" id="idm139656375959136" />In Machine Learning, a <em>kernel</em> is
                  a function capable of computing the dot product <em>ϕ</em>(<strong>a</strong>)<sup><em>T</em></sup>
                  <em>ϕ</em>(<strong>b</strong>) based only on the original vectors <strong>a</strong> and
                  <strong>b</strong>, without having to compute (or even to know about) the transformation <em>ϕ</em>.
                  <a data-type="xref" href="#common_kernels">Equation 5-10</a> lists some of the most commonly used
                  kernels.</p>
                <div id="common_kernels" data-type="equation">
                  <h5><span class="label">Equation 5-10. </span>Common kernels</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd columnalign="right">
                          <mtext>Linear:</mtext>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="1.em" />
                            <mi>K</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi mathvariant="bold">a</mi>
                              <mo>,</mo>
                              <mi mathvariant="bold">b</mi>
                              <mo>)</mo>
                            </mrow>
                            <mo>=</mo>
                            <msup>
                              <mi mathvariant="bold">a</mi>
                              <mi>T</mi>
                            </msup>
                            <mi mathvariant="bold">b</mi>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mtext>Polynomial:</mtext>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="1.em" />
                            <mi>K</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi mathvariant="bold">a</mi>
                              <mo>,</mo>
                              <mi mathvariant="bold">b</mi>
                              <mo>)</mo>
                            </mrow>
                            <mo>=</mo>
                            <msup>
                              <mfenced separators="" open="(" close=")">
                                <mi>γ</mi>
                                <msup>
                                  <mi mathvariant="bold">a</mi>
                                  <mi>T</mi>
                                </msup>
                                <mi mathvariant="bold">b</mi>
                                <mo>+</mo>
                                <mi>r</mi>
                              </mfenced>
                              <mi>d</mi>
                            </msup>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <mtext>Gaussian</mtext>
                            <mspace width="4.pt" />
                            <mtext>RBF:</mtext>
                          </mrow>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="1.em" />
                            <mi>K</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi mathvariant="bold">a</mi>
                              <mo>,</mo>
                              <mi mathvariant="bold">b</mi>
                              <mo>)</mo>
                            </mrow>
                            <mo>=</mo>
                            <mo form="prefix">exp</mo>
                            <mrow>
                              <mo>(</mo>
                              <mstyle scriptlevel="0" displaystyle="true">
                                <mrow>
                                  <mo>-</mo>
                                  <mi>γ</mi>
                                  <msup>
                                    <mfenced separators="" open="∥" close="∥">
                                      <mi mathvariant="bold">a</mi>
                                      <mo>-</mo>
                                      <mi mathvariant="bold">b</mi>
                                    </mfenced>
                                    <mn>2</mn>
                                  </msup>
                                </mrow>
                              </mstyle>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mtext>Sigmoid:</mtext>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="1.em" />
                            <mi>K</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi mathvariant="bold">a</mi>
                              <mo>,</mo>
                              <mi mathvariant="bold">b</mi>
                              <mo>)</mo>
                            </mrow>
                            <mo>=</mo>
                            <mo form="prefix">tanh</mo>
                            <mfenced separators="" open="(" close=")">
                              <mi>γ</mi>
                              <msup>
                                <mi mathvariant="bold">a</mi>
                                <mi>T</mi>
                              </msup>
                              <mi mathvariant="bold">b</mi>
                              <mo>+</mo>
                              <mi>r</mi>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>
                <aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space">
                  <div class="sidebar" id="idm139656375899104">
                    <h5>Mercer’s Theorem</h5>
                    <p>According <a data-type="indexterm" data-primary="Mercer's theorem" id="idm139656375897536" />to
                      <em>Mercer’s theorem</em>, if a function <em>K</em>(<strong>a</strong>, <strong>b</strong>)
                      respects a few mathematical conditions called <em>Mercer’s conditions</em> (<em>K</em> must be
                      continuous, symmetric in its arguments so <em>K</em>(<strong>a</strong>, <strong>b</strong>) =
                      <em>K</em>(<strong>b</strong>, <strong>a</strong>), etc.), then there exists a function <em>ϕ</em>
                      that maps <strong>a</strong> and <strong>b</strong> into another space (possibly with much higher
                      dimensions) such that <em>K</em>(<strong>a</strong>, <strong>b</strong>) =
                      <em>ϕ</em>(<strong>a</strong>)<sup><em>T</em></sup> <em>ϕ</em>(<strong>b</strong>). So you can use
                      <em>K</em> as a kernel since you know <em>ϕ</em> exists, even if you don’t know what <em>ϕ</em>
                      is. In the case of the <a data-type="indexterm" data-primary="Gaussian RBF kernel"
                        id="idm139656375884256" />Gaussian RBF kernel, it can be shown that <em>ϕ</em> actually maps
                      each training instance to an infinite-dimensional space, so it’s a good thing you don’t need to
                      actually perform the mapping!</p>

                    <p>Note that some frequently used kernels (such as the Sigmoid kernel) don’t respect all of Mercer’s
                      conditions, yet they generally work well in practice.</p>
                  </div>
                </aside>

                <p>There is still one loose end we must tie. <a data-type="xref" href="#from_alpha_to_w_and_b">Equation
                    5-7</a> shows how to go from the dual solution to the primal solution in the case of a linear SVM
                  classifier, but if you apply the kernel trick you end up with equations that include
                  <em>ϕ</em>(<em>x</em><sup><em>(i)</em></sup>). In fact, <math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mover accent="true">
                      <mi mathvariant="bold">w</mi>
                      <mo>^</mo>
                    </mover>
                  </math> must have the same number of dimensions as <em>ϕ</em>(<em>x</em><sup><em>(i)</em></sup>),
                  which may be huge or even infinite, so you can’t compute it. But how can you make predictions without
                  knowing <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mover accent="true">
                      <mi mathvariant="bold">w</mi>
                      <mo>^</mo>
                    </mover>
                  </math>? Well, the good news is that you can plug in the formula for <math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mover accent="true">
                      <mi mathvariant="bold">w</mi>
                      <mo>^</mo>
                    </mover>
                  </math> from <a data-type="xref" href="#from_alpha_to_w_and_b">Equation 5-7</a> into the decision
                  function for a new instance <strong>x</strong><sup><em>(n)</em></sup>, and you get an equation with
                  only dot products between input vectors. This makes it possible to use the kernel trick, once again
                  (<a data-type="xref" href="#making_predictions_with_a_kernelized_svm">Equation 5-11</a>).</p>
                <div id="making_predictions_with_a_kernelized_svm" data-type="equation">
                  <h5><span class="label">Equation 5-11. </span>Making predictions with a kernelized SVM</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <msub>
                              <mi>h</mi>
                              <mrow>
                                <mover accent="true">
                                  <mi mathvariant="bold">w</mi>
                                  <mo>^</mo>
                                </mover>
                                <mo>,</mo>
                                <mover accent="true">
                                  <mi>b</mi>
                                  <mo>^</mo>
                                </mover>
                              </mrow>
                            </msub>
                            <mfenced separators="" open="(" close=")">
                              <mi>ϕ</mi>
                              <mo>(</mo>
                              <msup>
                                <mi mathvariant="bold">x</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>n</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>)</mo>
                            </mfenced>
                          </mrow>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mo>=</mo>
                            <mspace width="0.166667em" />
                            <msup>
                              <mover accent="true">
                                <mi mathvariant="bold">w</mi>
                                <mo>^</mo>
                              </mover>
                              <mi>T</mi>
                            </msup>
                            <mi>ϕ</mi>
                            <mrow>
                              <mo>(</mo>
                              <msup>
                                <mi mathvariant="bold">x</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>n</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>)</mo>
                            </mrow>
                            <mo>+</mo>
                            <mover accent="true">
                              <mi>b</mi>
                              <mo>^</mo>
                            </mover>
                            <mo>=</mo>
                            <msup>
                              <mfenced separators="" open="(" close=")">
                                <munderover>
                                  <mo>∑</mo>
                                  <mrow>
                                    <mi>i</mi>
                                    <mo>=</mo>
                                    <mn>1</mn>
                                  </mrow>
                                  <mi>m</mi>
                                </munderover>
                                <msup>
                                  <mrow>
                                    <mover accent="true">
                                      <mi>α</mi>
                                      <mo>^</mo>
                                    </mover>
                                  </mrow>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>i</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <msup>
                                  <mi>t</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>i</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <mi>ϕ</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <msup>
                                    <mi mathvariant="bold">x</mi>
                                    <mrow>
                                      <mo>(</mo>
                                      <mi>i</mi>
                                      <mo>)</mo>
                                    </mrow>
                                  </msup>
                                  <mo>)</mo>
                                </mrow>
                              </mfenced>
                              <mi>T</mi>
                            </msup>
                            <mi>ϕ</mi>
                            <mrow>
                              <mo>(</mo>
                              <msup>
                                <mi mathvariant="bold">x</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>n</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>)</mo>
                            </mrow>
                            <mo>+</mo>
                            <mover accent="true">
                              <mi>b</mi>
                              <mo>^</mo>
                            </mover>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <mo>=</mo>
                            <mspace width="0.166667em" />
                            <munderover>
                              <mo>∑</mo>
                              <mrow>
                                <mi>i</mi>
                                <mo>=</mo>
                                <mn>1</mn>
                              </mrow>
                              <mi>m</mi>
                            </munderover>
                            <msup>
                              <mrow>
                                <mover accent="true">
                                  <mi>α</mi>
                                  <mo>^</mo>
                                </mover>
                              </mrow>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <msup>
                              <mi>t</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mfenced separators="" open="(" close=")">
                              <mi>ϕ</mi>
                              <msup>
                                <mrow>
                                  <mo>(</mo>
                                  <msup>
                                    <mi mathvariant="bold">x</mi>
                                    <mrow>
                                      <mo>(</mo>
                                      <mi>i</mi>
                                      <mo>)</mo>
                                    </mrow>
                                  </msup>
                                  <mo>)</mo>
                                </mrow>
                                <mi>T</mi>
                              </msup>
                              <mi>ϕ</mi>
                              <mrow>
                                <mo>(</mo>
                                <msup>
                                  <mi mathvariant="bold">x</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>n</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <mo>)</mo>
                              </mrow>
                            </mfenced>
                            <mo>+</mo>
                            <mover accent="true">
                              <mi>b</mi>
                              <mo>^</mo>
                            </mover>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <mo>=</mo>
                            <munderover>
                              <mo>∑</mo>
                              <mfrac linethickness="0pt">
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <mi>i</mi>
                                    <mo>=</mo>
                                    <mn>1</mn>
                                  </mrow>
                                </mstyle>
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <msup>
                                      <mrow>
                                        <mover accent="true">
                                          <mi>α</mi>
                                          <mo>^</mo>
                                        </mover>
                                      </mrow>
                                      <mrow>
                                        <mo>(</mo>
                                        <mi>i</mi>
                                        <mo>)</mo>
                                      </mrow>
                                    </msup>
                                    <mo>&gt;</mo>
                                    <mn>0</mn>
                                  </mrow>
                                </mstyle>
                              </mfrac>
                              <mi>m</mi>
                            </munderover>
                            <msup>
                              <mrow>
                                <mover accent="true">
                                  <mi>α</mi>
                                  <mo>^</mo>
                                </mover>
                              </mrow>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <msup>
                              <mi>t</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mi>K</mi>
                            <mrow>
                              <mo>(</mo>
                              <msup>
                                <mi mathvariant="bold">x</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>,</mo>
                              <msup>
                                <mi mathvariant="bold">x</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>n</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>)</mo>
                            </mrow>
                            <mo>+</mo>
                            <mover accent="true">
                              <mi>b</mi>
                              <mo>^</mo>
                            </mover>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>

                <p>Note that since <em>α</em><sup><em>(i)</em></sup> ≠ 0 only for support vectors, making predictions
                  involves computing the dot product of the new input vector <strong>x</strong><sup><em>(n)</em></sup>
                  with only the support vectors, not all the training instances. Of course, you also need to compute the
                  bias term <math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove b With caret">
                    <mover accent="true">
                      <mi>b</mi>
                      <mo>^</mo>
                    </mover>
                  </math>, using the same trick (<a data-type="xref" href="#bias_term_using_the_kernel_trick">Equation
                    5-12</a>).</p>
                <div class="pagebreak-before less_space" id="bias_term_using_the_kernel_trick" data-type="equation">
                  <h5><span class="label">Equation 5-12. </span>Computing the bias term using the kernel trick</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd columnalign="right">
                          <mover accent="true">
                            <mi>b</mi>
                            <mo>^</mo>
                          </mover>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mo>=</mo>
                            <mstyle scriptlevel="0" displaystyle="true">
                              <mfrac>
                                <mn>1</mn>
                                <msub>
                                  <mi>n</mi>
                                  <mi>s</mi>
                                </msub>
                              </mfrac>
                            </mstyle>
                            <munderover>
                              <mo>∑</mo>
                              <mfrac linethickness="0pt">
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <mi>i</mi>
                                    <mo>=</mo>
                                    <mn>1</mn>
                                  </mrow>
                                </mstyle>
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <msup>
                                      <mrow>
                                        <mover accent="true">
                                          <mi>α</mi>
                                          <mo>^</mo>
                                        </mover>
                                      </mrow>
                                      <mrow>
                                        <mo>(</mo>
                                        <mi>i</mi>
                                        <mo>)</mo>
                                      </mrow>
                                    </msup>
                                    <mo>&gt;</mo>
                                    <mn>0</mn>
                                  </mrow>
                                </mstyle>
                              </mfrac>
                              <mi>m</mi>
                            </munderover>
                            <mfenced separators="" open="(" close=")">
                              <msup>
                                <mi>t</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>-</mo>
                              <msup>
                                <mrow>
                                  <mover accent="true">
                                    <mi mathvariant="bold">w</mi>
                                    <mo>^</mo>
                                  </mover>
                                </mrow>
                                <mi>T</mi>
                              </msup>
                              <mi>ϕ</mi>
                              <mrow>
                                <mo>(</mo>
                                <msup>
                                  <mi mathvariant="bold">x</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>i</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <mo>)</mo>
                              </mrow>
                            </mfenced>
                            <mo>=</mo>
                            <mstyle scriptlevel="0" displaystyle="true">
                              <mfrac>
                                <mn>1</mn>
                                <msub>
                                  <mi>n</mi>
                                  <mi>s</mi>
                                </msub>
                              </mfrac>
                            </mstyle>
                            <munderover>
                              <mo>∑</mo>
                              <mfrac linethickness="0pt">
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <mi>i</mi>
                                    <mo>=</mo>
                                    <mn>1</mn>
                                  </mrow>
                                </mstyle>
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <msup>
                                      <mrow>
                                        <mover accent="true">
                                          <mi>α</mi>
                                          <mo>^</mo>
                                        </mover>
                                      </mrow>
                                      <mrow>
                                        <mo>(</mo>
                                        <mi>i</mi>
                                        <mo>)</mo>
                                      </mrow>
                                    </msup>
                                    <mo>&gt;</mo>
                                    <mn>0</mn>
                                  </mrow>
                                </mstyle>
                              </mfrac>
                              <mi>m</mi>
                            </munderover>
                            <mfenced separators="" open="(" close=")">
                              <msup>
                                <mi>t</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>-</mo>
                              <msup>
                                <mrow>
                                  <mfenced separators="" open="(" close=")">
                                    <munderover>
                                      <mo>∑</mo>
                                      <mrow>
                                        <mi>j</mi>
                                        <mo>=</mo>
                                        <mn>1</mn>
                                      </mrow>
                                      <mi>m</mi>
                                    </munderover>
                                    <msup>
                                      <mrow>
                                        <mover accent="true">
                                          <mi>α</mi>
                                          <mo>^</mo>
                                        </mover>
                                      </mrow>
                                      <mrow>
                                        <mo>(</mo>
                                        <mi>j</mi>
                                        <mo>)</mo>
                                      </mrow>
                                    </msup>
                                    <msup>
                                      <mi>t</mi>
                                      <mrow>
                                        <mo>(</mo>
                                        <mi>j</mi>
                                        <mo>)</mo>
                                      </mrow>
                                    </msup>
                                    <mi>ϕ</mi>
                                    <mrow>
                                      <mo>(</mo>
                                      <msup>
                                        <mi mathvariant="bold">x</mi>
                                        <mrow>
                                          <mo>(</mo>
                                          <mi>j</mi>
                                          <mo>)</mo>
                                        </mrow>
                                      </msup>
                                      <mo>)</mo>
                                    </mrow>
                                  </mfenced>
                                </mrow>
                                <mi>T</mi>
                              </msup>
                              <mi>ϕ</mi>
                              <mrow>
                                <mo>(</mo>
                                <msup>
                                  <mi mathvariant="bold">x</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>i</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <mo>)</mo>
                              </mrow>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <mo>=</mo>
                            <mstyle scriptlevel="0" displaystyle="true">
                              <mfrac>
                                <mn>1</mn>
                                <msub>
                                  <mi>n</mi>
                                  <mi>s</mi>
                                </msub>
                              </mfrac>
                            </mstyle>
                            <munderover>
                              <mo>∑</mo>
                              <mfrac linethickness="0pt">
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <mi>i</mi>
                                    <mo>=</mo>
                                    <mn>1</mn>
                                  </mrow>
                                </mstyle>
                                <mstyle scriptlevel="1" displaystyle="false">
                                  <mrow>
                                    <msup>
                                      <mrow>
                                        <mover accent="true">
                                          <mi>α</mi>
                                          <mo>^</mo>
                                        </mover>
                                      </mrow>
                                      <mrow>
                                        <mo>(</mo>
                                        <mi>i</mi>
                                        <mo>)</mo>
                                      </mrow>
                                    </msup>
                                    <mo>&gt;</mo>
                                    <mn>0</mn>
                                  </mrow>
                                </mstyle>
                              </mfrac>
                              <mi>m</mi>
                            </munderover>
                            <mfenced separators="" open="(" close=")">
                              <msup>
                                <mi>t</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>-</mo>
                              <munderover>
                                <mo>∑</mo>
                                <mfrac linethickness="0pt">
                                  <mstyle scriptlevel="1" displaystyle="false">
                                    <mrow>
                                      <mi>j</mi>
                                      <mo>=</mo>
                                      <mn>1</mn>
                                    </mrow>
                                  </mstyle>
                                  <mstyle scriptlevel="1" displaystyle="false">
                                    <mrow>
                                      <msup>
                                        <mrow>
                                          <mover accent="true">
                                            <mi>α</mi>
                                            <mo>^</mo>
                                          </mover>
                                        </mrow>
                                        <mrow>
                                          <mo>(</mo>
                                          <mi>j</mi>
                                          <mo>)</mo>
                                        </mrow>
                                      </msup>
                                      <mo>&gt;</mo>
                                      <mn>0</mn>
                                    </mrow>
                                  </mstyle>
                                </mfrac>
                                <mi>m</mi>
                              </munderover>
                              <mrow>
                                <msup>
                                  <mrow>
                                    <mover accent="true">
                                      <mi>α</mi>
                                      <mo>^</mo>
                                    </mover>
                                  </mrow>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>j</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <msup>
                                  <mi>t</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>j</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <mi>K</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <msup>
                                    <mi mathvariant="bold">x</mi>
                                    <mrow>
                                      <mo>(</mo>
                                      <mi>i</mi>
                                      <mo>)</mo>
                                    </mrow>
                                  </msup>
                                  <mo>,</mo>
                                  <msup>
                                    <mi mathvariant="bold">x</mi>
                                    <mrow>
                                      <mo>(</mo>
                                      <mi>j</mi>
                                      <mo>)</mo>
                                    </mrow>
                                  </msup>
                                  <mo>)</mo>
                                </mrow>
                              </mrow>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>

                <p>If you are starting to get a headache, it’s perfectly normal: it’s an unfortunate side effect of the
                  <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="kernelized SVM"
                    data-startref="svm5moksvm" id="idm139656375684864" /><a data-type="indexterm"
                    data-primary="kernelized SVM" data-startref="ksvm5" id="idm139656375683648" /><a
                    data-type="indexterm" data-primary="kernel trick" data-startref="kt5"
                    id="idm139656375682704" />kernel trick.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Online SVMs">
              <div class="sect2" id="idm139656376094208">
                <h2>Online SVMs</h2>

                <p>Before <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                    data-secondary="online SVMs" id="svm5osvm" /><a data-type="indexterm" data-primary="online SVMs"
                    id="osvm5" />concluding this chapter, let’s take a quick look at online SVM classifiers (recall that
                  online learning means learning incrementally, typically as new instances arrive).</p>

                <p>For linear SVM classifiers, one method is to use <a data-type="indexterm"
                    data-primary="Gradient Descent (GD)" id="idm139656375677168" />Gradient Descent (e.g., using
                  <code>SGDClassifier</code>) to minimize the cost function in <a data-type="xref"
                    href="#linear_svm_classifier_cost_function">Equation 5-13</a>, which is derived from the primal
                  problem. Unfortunately it converges much more slowly than the methods based on QP.</p>
                <div id="linear_svm_classifier_cost_function" data-type="equation">
                  <h5><span class="label">Equation 5-13. </span>Linear SVM classifier cost function</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mi>J</mi>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">w</mi>
                        <mo>,</mo>
                        <mi>b</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mn>1</mn>
                          <mn>2</mn>
                        </mfrac>
                      </mstyle>
                      <msup>
                        <mi mathvariant="bold">w</mi>
                        <mi>T</mi>
                      </msup>
                      <mi mathvariant="bold">w</mi>
                      <mspace width="1.em" />
                      <mo>+</mo>
                      <mspace width="1.em" />
                      <mi>C</mi>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mrow>
                          <munderover>
                            <mo>∑</mo>
                            <mrow>
                              <mi>i</mi>
                              <mo>=</mo>
                              <mn>1</mn>
                            </mrow>
                            <mi>m</mi>
                          </munderover>
                          <mi>m</mi>
                          <mi>a</mi>
                          <mi>x</mi>
                          <mfenced separators="" open="(" close=")">
                            <mn>0</mn>
                            <mo>,</mo>
                            <mn>1</mn>
                            <mo>-</mo>
                            <msup>
                              <mi>t</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mrow>
                              <mo>(</mo>
                              <msup>
                                <mi mathvariant="bold">w</mi>
                                <mi>T</mi>
                              </msup>
                              <msup>
                                <mi mathvariant="bold">x</mi>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                              <mo>+</mo>
                              <mi>b</mi>
                              <mo>)</mo>
                            </mrow>
                          </mfenced>
                        </mrow>
                      </mstyle>
                    </mrow>
                  </math>
                </div>

                <p>The first sum in the cost function will push the model to have a small weight vector
                  <strong>w</strong>, leading to a larger margin. The second sum computes the total of all margin
                  violations. An instance’s margin violation is equal to 0 if it is located off the street and on the
                  correct side, or else it is proportional to the distance to the correct side of the street. Minimizing
                  this term ensures that the model makes the margin violations as small and as few as possible</p>
                <aside data-type="sidebar" epub:type="sidebar">
                  <div class="sidebar" id="hinge_loss_function">
                    <h5>Hinge Loss</h5>
                    <p>The function <em>max</em>(0, 1 – <em>t</em>) is called <a data-type="indexterm"
                        data-primary="hinge loss function" id="idm139656375642640" />the <em>hinge loss</em> function
                      (represented below). It is equal to 0 when <em>t</em> ≥ 1. Its derivative (slope) is equal to –1
                      if <em>t</em> &lt; 1 and 0 if <em>t</em> &gt; 1. It is not differentiable at <em>t</em> = 1, but
                      just like for Lasso Regression (see <a data-type="xref" href="ch04.xhtml#lasso_regression">“Lasso
                        Regression”</a>) you can still use Gradient Descent using <a data-type="indexterm"
                        data-primary="subderivatives" id="idm139656375639248" />any <em>subderivative</em> at <em>t</em>
                      = 1 (i.e., any value between –1 and 0).</p>

                    <figure class="smallersixty">
                      <div class="figure">
                        <img src="mlst_05in01.png" alt="mlst 05in01" width="1403" height="716" />
                        <h6 />
                      </div>
                    </figure>
                  </div>
                </aside>

                <p>It is also possible to implement online kernelized SVMs—for example, using <a
                    href="https://homl.info/17">“Incremental and Decremental SVM Learning”</a><sup><a
                      data-type="noteref" id="idm139656375634736-marker"
                      href="ch05.xhtml#idm139656375634736">8</a></sup> or <a href="https://homl.info/18">“Fast Kernel
                    Classifiers with Online and Active Learning.”</a><sup><a data-type="noteref"
                      id="idm139656375633408-marker" href="ch05.xhtml#idm139656375633408">9</a></sup> However, these are
                  implemented in Matlab and C++. For large-scale nonlinear problems, you may want to consider using
                  neural networks instead <a data-type="indexterm" data-primary="Support Vector Machines (SVMs)"
                    data-secondary="mechanics of" data-startref="svm5mo" id="idm139656375632384" /><a
                    data-type="indexterm" data-primary="Support Vector Machines (SVMs)" data-secondary="SVM regression"
                    data-startref="svm5svmr" id="idm139656375631200" /><a data-type="indexterm"
                    data-primary="Support Vector Machines (SVMs)" data-secondary="online SVMs" data-startref="svm5osvm"
                    id="idm139656375630016" /><a data-type="indexterm" data-primary="online SVMs" data-startref="osvm5"
                    id="idm139656375628832" />(see Part II).</p>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Exercises">
          <div class="sect1" id="idm139656375627760">
            <h1>Exercises</h1>
            <ol>
              <li>
                <p>What is the fundamental idea behind Support Vector Machines?</p>
              </li>
              <li>
                <p>What is a support vector?</p>
              </li>
              <li>
                <p>Why is it important to scale the inputs when using SVMs?</p>
              </li>
              <li>
                <p>Can an SVM classifier output a confidence score when it classifies an instance? What about a
                  probability?</p>
              </li>
              <li>
                <p>Should you use the primal or the dual form of the SVM problem to train a model on a training set with
                  millions of instances and hundreds of features?</p>
              </li>
              <li>
                <p>Say you trained an SVM classifier with an RBF kernel. It seems to underfit the training set: should
                  you increase or decrease <em>γ</em> (<code>gamma</code>)? What about <code>C</code>?</p>
              </li>
              <li>
                <p>How should you set the QP parameters (<strong>H</strong>, <strong>f</strong>, <strong>A</strong>, and
                  <strong>b</strong>) to solve the soft margin linear SVM classifier problem using an off-the-shelf QP
                  solver?</p>
              </li>
              <li>
                <p>Train a <code>LinearSVC</code> on a linearly separable dataset. Then train an <code>SVC</code> and a
                  <code>SGDClassifier</code> on the same dataset. <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.svm.SVC" id="idm139656375613696" /> <a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="sklearn.svm.LinearSVC" id="idm139656375612560" />See if
                  you can get them to produce roughly the same model.</p>
              </li>
              <li>
                <p>Train an SVM classifier on the MNIST dataset. Since SVM classifiers are binary classifiers, you will
                  need to use <a data-type="indexterm" data-primary="one-versus-all  (OvA)  strategy"
                    id="idm139656375610576" />one-versus-all to classify all 10 digits. You may want to tune the
                  hyperparameters using small validation sets to speed up the process. What accuracy can you reach?</p>
              </li>
              <li>
                <p>Train an SVM regressor on the California housing <a data-type="indexterm"
                    data-primary="Support Vector Machines (SVMs)" data-startref="svm5"
                    id="idm139656375608672" />dataset.</p>
              </li>

            </ol>

            <p>Solutions to these exercises are available in Appendix A.</p>
          </div>
        </section>







        <div data-type="footnotes">
          <p data-type="footnote" id="idm139656376700816"><sup><a
                href="ch05.xhtml#idm139656376700816-marker">1</a></sup> “A Dual Coordinate Descent Method for
            Large-scale Linear SVM,” Lin et al. (2008).</p>
          <p data-type="footnote" id="idm139656376692624"><sup><a
                href="ch05.xhtml#idm139656376692624-marker">2</a></sup> “Sequential Minimal Optimization (SMO),” J.
            Platt (1998).</p>
          <p data-type="footnote" id="idm139656376480112"><sup><a
                href="ch05.xhtml#idm139656376480112-marker">3</a></sup> More generally, when there are <em>n</em>
            features, the decision function is an <em>n</em>-dimensional <em>hyperplane</em>, <a data-type="indexterm"
              data-primary="hyperplane" id="idm139656376477968" />and the decision boundary is an (<em>n</em> –
            1)-dimensional hyperplane.</p>
          <p data-type="footnote" id="idm139656376382608"><sup><a
                href="ch05.xhtml#idm139656376382608-marker">4</a></sup> Zeta (<em>ζ</em>) is the 6<sup>th</sup> letter
            of the Greek alphabet.</p>
          <p data-type="footnote" id="idm139656376325360"><sup><a
                href="ch05.xhtml#idm139656376325360-marker">5</a></sup> To learn more about Quadratic Programming, you
            can start by reading Stephen Boyd and Lieven Vandenberghe, <a href="https://homl.info/15"><em>Convex
                Optimization</em></a> (Cambridge, UK: Cambridge University Press, 2004) or watch Richard Brown’s <a
              href="https://homl.info/16">series of video lectures</a>.</p>
          <p data-type="footnote" id="idm139656376188464"><sup><a
                href="ch05.xhtml#idm139656376188464-marker">6</a></sup> The objective function is convex, and the
            inequality constraints are continuously differentiable and convex functions.</p>
          <p data-type="footnote" id="idm139656376061008"><sup><a
                href="ch05.xhtml#idm139656376061008-marker">7</a></sup> As explained in <a data-type="xref"
              href="ch04.xhtml#linear_models_chapter">Chapter 4</a>, the dot product of two vectors <strong>a</strong>
            and <strong>b</strong> is normally noted <strong>a</strong> · <strong>b</strong>. However, in Machine
            Learning, vectors are frequently represented as column vectors (i.e., single-column matrices), so the dot
            product is achieved by computing <strong>a</strong><sup>T</sup><strong>b</strong>. To remain consistent with
            the rest of the book, we will use this notation here, ignoring the fact that this technically results in a
            single-cell matrix rather than a scalar value.</p>
          <p data-type="footnote" id="idm139656375634736"><sup><a
                href="ch05.xhtml#idm139656375634736-marker">8</a></sup> “Incremental and Decremental Support Vector
            Machine Learning,” G. Cauwenberghs, T. Poggio (2001).</p>
          <p data-type="footnote" id="idm139656375633408"><sup><a
                href="ch05.xhtml#idm139656375633408-marker">9</a></sup> “Fast Kernel Classifiers with Online and Active
            Learning,“ A. Bordes, S. Ertekin, J. Weston, L. Bottou (2005).</p>
        </div>
      </div>
    </section>
  </div>



</body>

</html>