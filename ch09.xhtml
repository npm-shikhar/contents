<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd"
  xmlns:epub="http://www.idpf.org/2007/ops">

<head>
  <link href="Style00.css" rel="stylesheet" type="text/css" />
  <link href="Style01.css" rel="stylesheet" type="text/css" />
  <link href="Style02.css" rel="stylesheet" type="text/css" />
  <link href="Style03.css" rel="stylesheet" type="text/css" />
  <style type="text/css" title="ibis-book">
    @charset "utf-8";

    #sbo-rt-content html,
    #sbo-rt-content div,
    #sbo-rt-content div,
    #sbo-rt-content span,
    #sbo-rt-content applet,
    #sbo-rt-content object,
    #sbo-rt-content iframe,
    #sbo-rt-content h1,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5,
    #sbo-rt-content h6,
    #sbo-rt-content p,
    #sbo-rt-content blockquote,
    #sbo-rt-content pre,
    #sbo-rt-content a,
    #sbo-rt-content abbr,
    #sbo-rt-content acronym,
    #sbo-rt-content address,
    #sbo-rt-content big,
    #sbo-rt-content cite,
    #sbo-rt-content code,
    #sbo-rt-content del,
    #sbo-rt-content dfn,
    #sbo-rt-content em,
    #sbo-rt-content img,
    #sbo-rt-content ins,
    #sbo-rt-content kbd,
    #sbo-rt-content q,
    #sbo-rt-content s,
    #sbo-rt-content samp,
    #sbo-rt-content small,
    #sbo-rt-content strike,
    #sbo-rt-content strong,
    #sbo-rt-content sub,
    #sbo-rt-content sup,
    #sbo-rt-content tt,
    #sbo-rt-content var,
    #sbo-rt-content b,
    #sbo-rt-content u,
    #sbo-rt-content i,
    #sbo-rt-content center,
    #sbo-rt-content dl,
    #sbo-rt-content dt,
    #sbo-rt-content dd,
    #sbo-rt-content ol,
    #sbo-rt-content ul,
    #sbo-rt-content li,
    #sbo-rt-content fieldset,
    #sbo-rt-content form,
    #sbo-rt-content label,
    #sbo-rt-content legend,
    #sbo-rt-content table,
    #sbo-rt-content caption,
    #sbo-rt-content tdiv,
    #sbo-rt-content tfoot,
    #sbo-rt-content thead,
    #sbo-rt-content tr,
    #sbo-rt-content th,
    #sbo-rt-content td,
    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content canvas,
    #sbo-rt-content details,
    #sbo-rt-content embed,
    #sbo-rt-content figure,
    #sbo-rt-content figcaption,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content output,
    #sbo-rt-content ruby,
    #sbo-rt-content section,
    #sbo-rt-content summary,
    #sbo-rt-content time,
    #sbo-rt-content mark,
    #sbo-rt-content audio,
    #sbo-rt-content video {
      margin: 0;
      padding: 0;
      border: 0;
      font-size: 100%;
      font: inherit;
      vertical-align: baseline
    }

    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content details,
    #sbo-rt-content figcaption,
    #sbo-rt-content figure,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content section {
      display: block
    }

    #sbo-rt-content div {
      line-height: 1
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style: none
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content q {
      quotes: none
    }

    #sbo-rt-content blockquote:before,
    #sbo-rt-content blockquote:after,
    #sbo-rt-content q:before,
    #sbo-rt-content q:after {
      content: none
    }

    #sbo-rt-content table {
      border-collapse: collapse;
      border-spacing: 0
    }

    @page {
      margin: 5px !important
    }

    #sbo-rt-content p {
      margin: 10px 0 0;
      line-height: 125%;
      text-align: left
    }

    #sbo-rt-content p.byline {
      text-align: left;
      margin: -33px auto 35px;
      font-style: italic;
      font-weight: bold
    }

    #sbo-rt-content div.preface p+p.byline {
      margin: 1em 0 0 !important
    }

    #sbo-rt-content div.preface p.byline+p.byline {
      margin: 0 !important
    }

    #sbo-rt-content div.sect1&gt;

    p.byline {
      margin: -.25em 0 1em
    }

    #sbo-rt-content div.sect1&gt;

    p.byline+p.byline {
      margin-top: -1em
    }

    #sbo-rt-content em {
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content em strong,
    #sbo-rt-content strong em {
      font-weight: bold;
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content strong,
    #sbo-rt-content span.bold {
      font-weight: bold
    }

    #sbo-rt-content em.replaceable {
      font-style: italic
    }

    #sbo-rt-content strong.userinput {
      font-weight: bold;
      font-style: normal
    }

    #sbo-rt-content span.bolditalic {
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content a.ulink,
    #sbo-rt-content a.xref,
    #sbo-rt-content a.email,
    #sbo-rt-content a.link,
    #sbo-rt-content a {
      text-decoration: none;
      color: #8e0012
    }

    #sbo-rt-content span.lineannotation {
      font-style: italic;
      color: #a62a2a;
      font-family: serif
    }

    #sbo-rt-content span.underline {
      text-decoration: underline
    }

    #sbo-rt-content span.strikethrough {
      text-decoration: line-through
    }

    #sbo-rt-content span.smallcaps {
      font-variant: small-caps
    }

    #sbo-rt-content span.cursor {
      background: #000;
      color: #fff
    }

    #sbo-rt-content span.smaller {
      font-size: 75%
    }

    #sbo-rt-content .boxedtext,
    #sbo-rt-content .keycap {
      border-style: solid;
      border-width: 1px;
      border-color: #000;
      padding: 1px
    }

    #sbo-rt-content span.gray50 {
      color: #7F7F7F;
    }

    #sbo-rt-content h1,
    #sbo-rt-content div.toc-title,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5 {
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      font-weight: bold;
      text-align: left;
      page-break-after: avoid !important;
      font-family: sans-serif, "DejaVuSans"
    }

    #sbo-rt-content div.toc-title {
      font-size: 1.5em;
      margin-top: 20px !important;
      margin-bottom: 30px !important
    }

    #sbo-rt-content section[data-type="sect1"] h1 {
      font-size: 1.3em;
      color: #8e0012;
      margin: 40px 0 8px 0
    }

    #sbo-rt-content section[data-type="sect2"] h2 {
      font-size: 1.1em;
      margin: 30px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect3"] h3 {
      font-size: 1em;
      color: #555;
      margin: 20px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect4"] h4 {
      font-size: 1em;
      font-weight: normal;
      font-style: italic;
      margin: 15px 0 6px 0 !important
    }

    #sbo-rt-content section[data-type="chapter"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="preface"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="appendix"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="glossary"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="bibliography"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="index"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      line-height: 1;
      margin-bottom: 50px;
      color: #000;
      padding-bottom: 10px;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content span.label,
    #sbo-rt-content span.keep-together {
      font-size: inherit;
      font-weight: inherit
    }

    #sbo-rt-content div[data-type="part"] h1 {
      font-size: 2em;
      text-align: center;
      margin-top: 0 !important;
      margin-bottom: 50px;
      padding: 50px 0 10px 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content img.width-ninety {
      width: 90%
    }

    #sbo-rt-content img {
      max-width: 95%;
      margin: 0 auto;
      padding: 0
    }

    #sbo-rt-content div.figure {
      background-color: transparent;
      text-align: center !important;
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content figure {
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.figure h6,
    #sbo-rt-content figure h6,
    #sbo-rt-content figure figcaption {
      font-size: .9rem !important;
      text-align: center;
      font-weight: normal !important;
      font-style: italic;
      font-family: serif !important;
      text-transform: none !important;
      letter-spacing: normal !important;
      color: #000 !important;
      padding-top: 10px !important;
      page-break-before: avoid
    }

    #sbo-rt-content div.informalfigure {
      text-align: center !important;
      padding: 5px 0 !important
    }

    #sbo-rt-content div.sidebar {
      margin: 15px 0 10px 0 !important;
      border: 1px solid #DCDCDC;
      background-color: #F7F7F7;
      padding: 15px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content aside[data-type="sidebar"] {
      margin: 15px 0 10px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar-title,
    #sbo-rt-content aside[data-type="sidebar"] h5 {
      font-weight: bold;
      font-size: 1em;
      font-family: sans-serif;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar ol,
    #sbo-rt-content div.sidebar ul,
    #sbo-rt-content aside[data-type="sidebar"] ol,
    #sbo-rt-content aside[data-type="sidebar"] ul {
      margin-left: 1.25em !important
    }

    #sbo-rt-content div.sidebar div.figure p.title,
    #sbo-rt-content aside[data-type="sidebar"] figcaption,
    #sbo-rt-content div.sidebar div.informalfigure div.caption {
      font-size: 90%;
      text-align: center;
      font-weight: normal;
      font-style: italic;
      font-family: serif !important;
      color: #000;
      padding: 5px !important;
      page-break-before: avoid;
      page-break-after: avoid
    }

    #sbo-rt-content div.sidebar div.tip,
    #sbo-rt-content div.sidebar div[data-type="tip"],
    #sbo-rt-content div.sidebar div.note,
    #sbo-rt-content div.sidebar div[data-type="note"],
    #sbo-rt-content div.sidebar div.warning,
    #sbo-rt-content div.sidebar div[data-type="warning"],
    #sbo-rt-content div.sidebar div[data-type="caution"],
    #sbo-rt-content div.sidebar div[data-type="important"] {
      margin: 20px auto 20px auto !important;
      font-size: 90%;
      width: 85%
    }

    #sbo-rt-content aside[data-type="sidebar"] p.byline {
      font-size: 90%;
      font-weight: bold;
      font-style: italic;
      text-align: center;
      text-indent: 0;
      margin: 5px auto 6px;
      page-break-after: avoid
    }

    #sbo-rt-content pre {
      white-space: pre-wrap;
      font-family: "Ubuntu Mono", monospace;
      margin: 25px 0 25px 20px;
      font-size: 85%;
      display: block;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content div.note pre.programlisting,
    #sbo-rt-content div.tip pre.programlisting,
    #sbo-rt-content div.warning pre.programlisting,
    #sbo-rt-content div.caution pre.programlisting,
    #sbo-rt-content div.important pre.programlisting {
      margin-bottom: 0
    }

    #sbo-rt-content code {
      font-family: "Ubuntu Mono", monospace;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content code strong em,
    #sbo-rt-content code em strong,
    #sbo-rt-content pre em strong,
    #sbo-rt-content pre strong em,
    #sbo-rt-content strong code em code,
    #sbo-rt-content em code strong code,
    #sbo-rt-content span.bolditalic code {
      font-weight: bold;
      font-style: italic;
      font-family: "Ubuntu Mono BoldItal", monospace
    }

    #sbo-rt-content code em,
    #sbo-rt-content em code,
    #sbo-rt-content pre em,
    #sbo-rt-content em.replaceable {
      font-family: "Ubuntu Mono Ital", monospace;
      font-style: italic
    }

    #sbo-rt-content code strong,
    #sbo-rt-content strong code,
    #sbo-rt-content pre strong,
    #sbo-rt-content strong.userinput {
      font-family: "Ubuntu Mono Bold", monospace;
      font-weight: bold
    }

    #sbo-rt-content div[data-type="example"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div[data-type="example"] h1,
    #sbo-rt-content div[data-type="example"] h2,
    #sbo-rt-content div[data-type="example"] h3,
    #sbo-rt-content div[data-type="example"] h4,
    #sbo-rt-content div[data-type="example"] h5,
    #sbo-rt-content div[data-type="example"] h6 {
      font-style: italic;
      font-weight: normal;
      text-align: left !important;
      text-transform: none !important;
      font-family: serif !important;
      margin: 10px 0 5px 0 !important;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content li pre.example {
      padding: 10px 0 !important
    }

    #sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],
    #sbo-rt-content div[data-type="example"] pre[data-type="screen"] {
      margin: 0
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      margin: 50px 0 10px 0 !important;
      line-height: 1;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] h2,
    #sbo-rt-content section[data-type="titlepage"] p.subtitle,
    #sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"] {
      font-size: 1.3em;
      font-weight: normal;
      text-align: center;
      margin-top: .5em;
      color: #555
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h2[data-type="author"],
    #sbo-rt-content section[data-type="titlepage"] p.author {
      font-size: 1.3em;
      font-family: serif !important;
      font-weight: bold;
      margin: 50px 0 !important;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] p.edition {
      text-align: center;
      text-transform: uppercase;
      margin-top: 2em
    }

    #sbo-rt-content section[data-type="titlepage"] {
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"]:after {
      content: url(css_assets/titlepage_footer_ebook.png);
      margin: 0 auto;
      max-width: 80%
    }

    #sbo-rt-content div.book div.titlepage div.publishername {
      margin-top: 60%;
      margin-bottom: 20px;
      text-align: center;
      font-size: 1.25em
    }

    #sbo-rt-content div.book div.titlepage div.locations p {
      margin: 0;
      text-align: center
    }

    #sbo-rt-content div.book div.titlepage div.locations p.cities {
      font-size: 80%;
      text-align: center;
      margin-top: 5px
    }

    #sbo-rt-content section.preface[title="Dedication"]&gt;

    div.titlepage h2.title {
      text-align: center;
      text-transform: uppercase;
      font-size: 1.5em;
      margin-top: 50px;
      margin-bottom: 50px
    }

    #sbo-rt-content ul.stafflist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.stafflist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content ul.printings li {
      list-style-type: none
    }

    #sbo-rt-content section.preface[title="Dedication"] p {
      font-style: italic;
      text-align: center
    }

    #sbo-rt-content div.colophon h1.title {
      font-size: 1.3em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon h2.subtitle {
      margin: 0 !important;
      color: #000;
      font-family: serif !important;
      font-size: 1em;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.author h3.author {
      font-size: 1.1em;
      font-family: serif !important;
      margin: 10px 0 0 !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h4,
    #sbo-rt-content div.colophon div.editor h3.editor {
      color: #000;
      font-size: .8em;
      margin: 15px 0 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h3.editor {
      font-size: .8em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.publisher {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon div.publisher p,
    #sbo-rt-content div.colophon div.publisher span.publishername {
      margin: 0;
      font-size: .8em
    }

    #sbo-rt-content div.legalnotice p,
    #sbo-rt-content div.timestamp p {
      font-size: .8em
    }

    #sbo-rt-content div.timestamp p {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon[title="About the Author"] h1.title,
    #sbo-rt-content div.colophon[title="Colophon"] h1.title {
      font-size: 1.5em;
      margin: 0 !important;
      font-family: sans-serif !important
    }

    #sbo-rt-content section.chapter div.titlepage div.author {
      margin: 10px 0 10px 0
    }

    #sbo-rt-content section.chapter div.titlepage div.author div.affiliation {
      font-style: italic
    }

    #sbo-rt-content div.attribution {
      margin: 5px 0 0 50px !important
    }

    #sbo-rt-content h3.author span.orgname {
      display: none
    }

    #sbo-rt-content div.epigraph {
      margin: 10px 0 10px 20px !important;
      page-break-inside: avoid;
      font-size: 90%
    }

    #sbo-rt-content div.epigraph p {
      font-style: italic
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content div.blockquote {
      margin: 10px !important;
      page-break-inside: avoid;
      font-size: 95%
    }

    #sbo-rt-content blockquote p,
    #sbo-rt-content div.blockquote p {
      font-style: italic;
      margin: .75em 0 0 !important
    }

    #sbo-rt-content blockquote div.attribution,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      margin: 5px 0 10px 30px !important;
      text-align: right;
      width: 80%
    }

    #sbo-rt-content blockquote div.attribution p,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      font-style: normal;
      margin-top: 5px
    }

    #sbo-rt-content blockquote div.attribution p:before,
    #sbo-rt-content blockquote p[data-type="attribution"]:before {
      font-style: normal;
      content: "—";
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none
    }

    #sbo-rt-content p.right {
      text-align: right;
      margin: 0
    }

    #sbo-rt-content div[data-type="footnotes"] {
      border-top: 1px solid black;
      margin-top: 2em
    }

    #sbo-rt-content sub,
    #sbo-rt-content sup {
      font-size: 75%;
      line-height: 0;
      position: relative
    }

    #sbo-rt-content sup {
      top: -.5em
    }

    #sbo-rt-content sub {
      bottom: -.25em
    }

    #sbo-rt-content p[data-type="footnote"] {
      font-size: 90% !important;
      line-height: 1.2em !important;
      margin-left: 2.5em !important;
      text-indent: -2.3em !important
    }

    #sbo-rt-content p[data-type="footnote"] sup {
      display: inline-block !important;
      position: static !important;
      width: 2em !important;
      text-align: right !important;
      font-size: 100% !important;
      padding-right: .5em !important
    }

    #sbo-rt-content p[data-type="footnote"] a[href$="-marker"] {
      font-family: sans-serif !important;
      font-size: 90% !important;
      color: #8e0012 !important
    }

    #sbo-rt-content a[data-type="noteref"] {
      font-family: sans-serif !important;
      color: #8e0012;
      margin-left: 0;
      padding-left: 0
    }

    #sbo-rt-content div.refentry p.refname {
      font-size: 1em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold;
      margin-bottom: 5px;
      overflow: auto;
      width: 100%
    }

    #sbo-rt-content div.refentry {
      width: 100%;
      display: block;
      margin-top: 2em
    }

    #sbo-rt-content div.refsynopsisdiv {
      display: block;
      clear: both
    }

    #sbo-rt-content div.refentry header {
      page-break-inside: avoid !important;
      display: block;
      break-inside: avoid !important;
      padding-top: 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content div.refsect1 h6 {
      font-size: .9em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold
    }

    #sbo-rt-content div.refsect1 {
      margin-top: 3em
    }

    #sbo-rt-content dt {
      padding-top: 10px !important;
      padding-bottom: 0 !important
    }

    #sbo-rt-content dd {
      margin-left: 1.5em !important;
      margin-bottom: .25em
    }

    #sbo-rt-content dd ol,
    #sbo-rt-content dd ul {
      padding-left: 1em
    }

    #sbo-rt-content dd li {
      margin-top: 0;
      margin-bottom: 0
    }

    #sbo-rt-content dd,
    #sbo-rt-content li {
      text-align: left
    }

    #sbo-rt-content ul,
    #sbo-rt-content ul&gt;
    li,
    #sbo-rt-content ol ul,
    #sbo-rt-content ol ul&gt;
    li,
    #sbo-rt-content ul ol ul,
    #sbo-rt-content ul ol ul&gt;

    li {
      list-style-type: disc
    }

    #sbo-rt-content ul ul,
    #sbo-rt-content ul ul&gt;

    li {
      list-style-type: square
    }

    #sbo-rt-content ul ul ul,
    #sbo-rt-content ul ul ul&gt;

    li {
      list-style-type: circle
    }

    #sbo-rt-content ol,
    #sbo-rt-content ol&gt;
    li,
    #sbo-rt-content ol ul ol,
    #sbo-rt-content ol ul ol&gt;
    li,
    #sbo-rt-content ul ol,
    #sbo-rt-content ul ol&gt;

    li {
      list-style-type: decimal
    }

    #sbo-rt-content ol ol,
    #sbo-rt-content ol ol&gt;

    li {
      list-style-type: lower-alpha
    }

    #sbo-rt-content ol ol ol,
    #sbo-rt-content ol ol ol&gt;

    li {
      list-style-type: lower-roman
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style-position: outside;
      margin: 15px 0 15px 1.25em;
      padding-left: 2.25em
    }

    #sbo-rt-content ol li,
    #sbo-rt-content ul li {
      margin: .5em 0 .65em;
      line-height: 125%
    }

    #sbo-rt-content div.orderedlistalpha {
      list-style-type: upper-alpha
    }

    #sbo-rt-content table.simplelist,
    #sbo-rt-content ul.simplelist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.simplelist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content table.simplelist td {
      border: none
    }

    #sbo-rt-content table.simplelist tr {
      border-bottom: none
    }

    #sbo-rt-content table.simplelist tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content dl.calloutlist p:first-child {
      margin-top: -25px !important
    }

    #sbo-rt-content dl.calloutlist dd {
      padding-left: 0;
      margin-top: -25px
    }

    #sbo-rt-content dl.calloutlist img,
    #sbo-rt-content a.co img {
      padding: 0
    }

    #sbo-rt-content div.toc ol {
      margin-top: 8px !important;
      margin-bottom: 8px !important;
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol ol {
      margin-left: 30px !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol li {
      list-style-type: none
    }

    #sbo-rt-content div.toc a {
      color: #8e0012
    }

    #sbo-rt-content div.toc ol a {
      font-size: 1em;
      font-weight: bold
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;

    ol a {
      font-weight: bold;
      font-size: 1em
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;
    ol&gt;
    li&gt;

    ol a {
      text-decoration: none;
      font-weight: normal;
      font-size: 1em
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"],
    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      margin: 30px !important;
      font-size: 90%;
      padding: 10px 8px 20px 8px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.tip ol,
    #sbo-rt-content div.tip ul,
    #sbo-rt-content div[data-type="tip"] ol,
    #sbo-rt-content div[data-type="tip"] ul,
    #sbo-rt-content div.note ol,
    #sbo-rt-content div.note ul,
    #sbo-rt-content div[data-type="note"] ol,
    #sbo-rt-content div[data-type="note"] ul,
    #sbo-rt-content div.warning ol,
    #sbo-rt-content div.warning ul,
    #sbo-rt-content div[data-type="warning"] ol,
    #sbo-rt-content div[data-type="warning"] ul,
    #sbo-rt-content div[data-type="caution"] ol,
    #sbo-rt-content div[data-type="caution"] ul,
    #sbo-rt-content div[data-type="important"] ol,
    #sbo-rt-content div[data-type="important"] ul {
      margin-left: 1.5em !important
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"] {
      border: 1px solid #BEBEBE;
      background-color: transparent
    }

    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      border: 1px solid #BC8F8F
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="note"] h1,
    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1,
    #sbo-rt-content div[data-type="important"] h6 {
      font-weight: bold;
      font-size: 110%;
      font-family: sans-serif !important;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px !important
    }

    #sbo-rt-content div[data-type="tip"] figure h6,
    #sbo-rt-content div[data-type="note"] figure h6,
    #sbo-rt-content div[data-type="warning"] figure h6,
    #sbo-rt-content div[data-type="caution"] figure h6,
    #sbo-rt-content div[data-type="important"] figure h6 {
      font-family: serif !important
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div[data-type="note"] h1 {
      color: #737373
    }

    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="important"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1 {
      color: #C67171
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,
    #sbo-rt-content div.safarienabled {
      background-color: transparent;
      margin: 8px 0 0 !important;
      border: 0 solid #BEBEBE;
      font-size: 100%;
      padding: 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,
    #sbo-rt-content div.safarienabled h6 {
      display: none
    }

    #sbo-rt-content div.table,
    #sbo-rt-content table {
      margin: 15px 0 30px 0 !important;
      max-width: 95%;
      border: none !important;
      background: none;
      display: table !important
    }

    #sbo-rt-content div.table,
    #sbo-rt-content div.informaltable,
    #sbo-rt-content table {
      page-break-inside: avoid
    }

    #sbo-rt-content tr,
    #sbo-rt-content tr td {
      border-bottom: 1px solid #c3c3c3
    }

    #sbo-rt-content thead td,
    #sbo-rt-content thead th {
      border-bottom: #9d9d9d 1px solid !important;
      border-top: #9d9d9d 1px solid !important
    }

    #sbo-rt-content tr:nth-of-type(even) {
      background-color: #f1f6fc
    }

    #sbo-rt-content thead {
      font-family: sans-serif;
      font-weight: bold
    }

    #sbo-rt-content td,
    #sbo-rt-content th {
      display: table-cell;
      padding: .3em;
      text-align: left;
      vertical-align: middle;
      font-size: 80%
    }

    #sbo-rt-content div.informaltable table {
      margin: 10px auto !important
    }

    #sbo-rt-content div.informaltable table tr {
      border-bottom: none
    }

    #sbo-rt-content div.informaltable table tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content div.informaltable td,
    #sbo-rt-content div.informaltable th {
      border: #9d9d9d 1px solid
    }

    #sbo-rt-content div.table-title,
    #sbo-rt-content table caption {
      font-weight: normal;
      font-style: italic;
      font-family: serif;
      font-size: 1em;
      margin: 10px 0 10px 0 !important;
      padding: 0;
      page-break-after: avoid;
      text-align: left !important
    }

    #sbo-rt-content table code {
      font-size: smaller
    }

    #sbo-rt-content table.border tbody&gt;
    tr:last-child&gt;

    td {
      border-bottom: transparent
    }

    #sbo-rt-content div.equation,
    #sbo-rt-content div[data-type="equation"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div.equation-title,
    #sbo-rt-content div[data-type="equation"] h5 {
      font-style: italic;
      font-weight: normal;
      font-family: serif !important;
      font-size: 90%;
      margin: 20px 0 10px 0 !important;
      page-break-after: avoid
    }

    #sbo-rt-content div.equation-contents {
      margin-left: 20px
    }

    #sbo-rt-content div[data-type="equation"] math {
      font-size: calc(.35em + 1vw)
    }

    #sbo-rt-content span.inlinemediaobject {
      height: .85em;
      display: inline-block;
      margin-bottom: .2em
    }

    #sbo-rt-content span.inlinemediaobject img {
      margin: 0;
      height: .85em
    }

    #sbo-rt-content div.informalequation {
      margin: 20px 0 20px 20px;
      width: 75%
    }

    #sbo-rt-content div.informalequation img {
      width: 75%
    }

    #sbo-rt-content div.index {
      text-indent: 0
    }

    #sbo-rt-content div.index h3 {
      padding: .25em;
      margin-top: 1em !important;
      background-color: #F0F0F0
    }

    #sbo-rt-content div.index li {
      line-height: 130%;
      list-style-type: none
    }

    #sbo-rt-content div.index a.indexterm {
      color: #8e0012 !important
    }

    #sbo-rt-content div.index ul {
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.index ul ul {
      margin-left: 1em !important;
      margin-top: 0 !important
    }

    #sbo-rt-content code.boolean,
    #sbo-rt-content .navy {
      color: rgb(0, 0, 128);
    }

    #sbo-rt-content code.character,
    #sbo-rt-content .olive {
      color: rgb(128, 128, 0);
    }

    #sbo-rt-content code.comment,
    #sbo-rt-content .blue {
      color: rgb(0, 0, 255);
    }

    #sbo-rt-content code.conditional,
    #sbo-rt-content .limegreen {
      color: rgb(50, 205, 50);
    }

    #sbo-rt-content code.constant,
    #sbo-rt-content .darkorange {
      color: rgb(255, 140, 0);
    }

    #sbo-rt-content code.debug,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.define,
    #sbo-rt-content .darkgoldenrod,
    #sbo-rt-content .gold {
      color: rgb(184, 134, 11);
    }

    #sbo-rt-content code.delimiter,
    #sbo-rt-content .dimgray {
      color: rgb(105, 105, 105);
    }

    #sbo-rt-content code.error,
    #sbo-rt-content .red {
      color: rgb(255, 0, 0);
    }

    #sbo-rt-content code.exception,
    #sbo-rt-content .salmon {
      color: rgb(250, 128, 11);
    }

    #sbo-rt-content code.float,
    #sbo-rt-content .steelblue {
      color: rgb(70, 130, 180);
    }

    #sbo-rt-content pre code.function,
    #sbo-rt-content .green {
      color: rgb(0, 128, 0);
    }

    #sbo-rt-content code.identifier,
    #sbo-rt-content .royalblue {
      color: rgb(65, 105, 225);
    }

    #sbo-rt-content code.ignore,
    #sbo-rt-content .gray {
      color: rgb(128, 128, 128);
    }

    #sbo-rt-content code.include,
    #sbo-rt-content .purple {
      color: rgb(128, 0, 128);
    }

    #sbo-rt-content code.keyword,
    #sbo-rt-content .sienna {
      color: rgb(160, 82, 45);
    }

    #sbo-rt-content code.label,
    #sbo-rt-content .deeppink {
      color: rgb(255, 20, 147);
    }

    #sbo-rt-content code.macro,
    #sbo-rt-content .orangered {
      color: rgb(255, 69, 0);
    }

    #sbo-rt-content code.number,
    #sbo-rt-content .brown {
      color: rgb(165, 42, 42);
    }

    #sbo-rt-content code.operator,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.preCondit,
    #sbo-rt-content .teal {
      color: rgb(0, 128, 128);
    }

    #sbo-rt-content code.preProc,
    #sbo-rt-content .fuschia {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.repeat,
    #sbo-rt-content .indigo {
      color: rgb(75, 0, 130);
    }

    #sbo-rt-content code.special,
    #sbo-rt-content .saddlebrown {
      color: rgb(139, 69, 19);
    }

    #sbo-rt-content code.specialchar,
    #sbo-rt-content .magenta {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.specialcomment,
    #sbo-rt-content .seagreen {
      color: rgb(46, 139, 87);
    }

    #sbo-rt-content code.statement,
    #sbo-rt-content .forestgreen {
      color: rgb(34, 139, 34);
    }

    #sbo-rt-content code.storageclass,
    #sbo-rt-content .plum {
      color: rgb(221, 160, 221);
    }

    #sbo-rt-content code.string,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.structure,
    #sbo-rt-content .chocolate {
      color: rgb(210, 106, 30);
    }

    #sbo-rt-content code.tag,
    #sbo-rt-content .darkcyan {
      color: rgb(0, 139, 139);
    }

    #sbo-rt-content code.todo,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.type,
    #sbo-rt-content .mediumslateblue {
      color: rgb(123, 104, 238);
    }

    #sbo-rt-content code.typedef,
    #sbo-rt-content .darkgreen {
      color: rgb(0, 100, 0);
    }

    #sbo-rt-content code.underlined {
      text-decoration: underline;
    }

    #sbo-rt-content pre code.hll {
      background-color: #ffc
    }

    #sbo-rt-content pre code.c {
      color: #09F;
      font-style: italic
    }

    #sbo-rt-content pre code.err {
      color: #A00
    }

    #sbo-rt-content pre code.k {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.o {
      color: #555
    }

    #sbo-rt-content pre code.cm {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cp {
      color: #099
    }

    #sbo-rt-content pre code.c1 {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cs {
      color: #35586C;
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content pre code.gd {
      background-color: #FCC
    }

    #sbo-rt-content pre code.ge {
      font-style: italic
    }

    #sbo-rt-content pre code.gr {
      color: #F00
    }

    #sbo-rt-content pre code.gh {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gi {
      background-color: #CFC
    }

    #sbo-rt-content pre code.go {
      color: #000
    }

    #sbo-rt-content pre code.gp {
      color: #009;
      font-weight: bold
    }

    #sbo-rt-content pre code.gs {
      font-weight: bold
    }

    #sbo-rt-content pre code.gu {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gt {
      color: #9C6
    }

    #sbo-rt-content pre code.kc {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kd {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kn {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kp {
      color: #069
    }

    #sbo-rt-content pre code.kr {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kt {
      color: #078;
      font-weight: bold
    }

    #sbo-rt-content pre code.m {
      color: #F60
    }

    #sbo-rt-content pre code.s {
      color: #C30
    }

    #sbo-rt-content pre code.na {
      color: #309
    }

    #sbo-rt-content pre code.nb {
      color: #366
    }

    #sbo-rt-content pre code.nc {
      color: #0A8;
      font-weight: bold
    }

    #sbo-rt-content pre code.no {
      color: #360
    }

    #sbo-rt-content pre code.nd {
      color: #99F
    }

    #sbo-rt-content pre code.ni {
      color: #999;
      font-weight: bold
    }

    #sbo-rt-content pre code.ne {
      color: #C00;
      font-weight: bold
    }

    #sbo-rt-content pre code.nf {
      color: #C0F
    }

    #sbo-rt-content pre code.nl {
      color: #99F
    }

    #sbo-rt-content pre code.nn {
      color: #0CF;
      font-weight: bold
    }

    #sbo-rt-content pre code.nt {
      color: #309;
      font-weight: bold
    }

    #sbo-rt-content pre code.nv {
      color: #033
    }

    #sbo-rt-content pre code.ow {
      color: #000;
      font-weight: bold
    }

    #sbo-rt-content pre code.w {
      color: #bbb
    }

    #sbo-rt-content pre code.mf {
      color: #F60
    }

    #sbo-rt-content pre code.mh {
      color: #F60
    }

    #sbo-rt-content pre code.mi {
      color: #F60
    }

    #sbo-rt-content pre code.mo {
      color: #F60
    }

    #sbo-rt-content pre code.sb {
      color: #C30
    }

    #sbo-rt-content pre code.sc {
      color: #C30
    }

    #sbo-rt-content pre code.sd {
      color: #C30;
      font-style: italic
    }

    #sbo-rt-content pre code.s2 {
      color: #C30
    }

    #sbo-rt-content pre code.se {
      color: #C30;
      font-weight: bold
    }

    #sbo-rt-content pre code.sh {
      color: #C30
    }

    #sbo-rt-content pre code.si {
      color: #A00
    }

    #sbo-rt-content pre code.sx {
      color: #C30
    }

    #sbo-rt-content pre code.sr {
      color: #3AA
    }

    #sbo-rt-content pre code.s1 {
      color: #C30
    }

    #sbo-rt-content pre code.ss {
      color: #A60
    }

    #sbo-rt-content pre code.bp {
      color: #366
    }

    #sbo-rt-content pre code.vc {
      color: #033
    }

    #sbo-rt-content pre code.vg {
      color: #033
    }

    #sbo-rt-content pre code.vi {
      color: #033
    }

    #sbo-rt-content pre code.il {
      color: #F60
    }

    #sbo-rt-content pre code.g {
      color: #050
    }

    #sbo-rt-content pre code.l {
      color: #C60
    }

    #sbo-rt-content pre code.l {
      color: #F90
    }

    #sbo-rt-content pre code.n {
      color: #008
    }

    #sbo-rt-content pre code.nx {
      color: #008
    }

    #sbo-rt-content pre code.py {
      color: #96F
    }

    #sbo-rt-content pre code.p {
      color: #000
    }

    #sbo-rt-content pre code.x {
      color: #F06
    }

    #sbo-rt-content div.blockquote_sampler_toc {
      width: 95%;
      margin: 5px 5px 5px 10px !important
    }

    #sbo-rt-content div {
      font-family: serif;
      text-align: left
    }

    #sbo-rt-content .gray-background,
    #sbo-rt-content .reverse-video {
      background: #2E2E2E;
      color: #FFF
    }

    #sbo-rt-content .light-gray-background {
      background: #A0A0A0
    }

    #sbo-rt-content .preserve-whitespace {
      white-space: pre-wrap
    }

    #sbo-rt-content span.gray {
      color: #4C4C4C
    }

    #sbo-rt-content .width-10 {
      width: 10vw !important
    }

    #sbo-rt-content .width-20 {
      width: 20vw !important
    }

    #sbo-rt-content .width-30 {
      width: 30vw !important
    }

    #sbo-rt-content .width-40 {
      width: 40vw !important
    }

    #sbo-rt-content .width-50 {
      width: 50vw !important
    }

    #sbo-rt-content .width-60 {
      width: 60vw !important
    }

    #sbo-rt-content .width-70 {
      width: 70vw !important
    }

    #sbo-rt-content .width-80 {
      width: 80vw !important
    }

    #sbo-rt-content .width-90 {
      width: 90vw !important
    }

    #sbo-rt-content .width-full,
    #sbo-rt-content .width-100 {
      width: 100vw !important
    }

    #sbo-rt-content div[data-type="equation"].fifty-percent img {
      width: 50%
    }
  </style>
  <style type="text/css" id="font-styles">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-size: &lt;
      %=font_size %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="font-family">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-family: &lt;
      %=font_family %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="column-width">
    #sbo-rt-content {
      max-width: &lt;
      %=column_width %&gt;
      % !important;
      margin: 0 auto !important;
    }
  </style>

  <style type="text/css">
    body {
      background-color: #fbfbfb !important;
      margin: 1em;
    }

    #sbo-rt-content * {
      text-indent: 0pt !important;
    }

    #sbo-rt-content .bq {
      margin-right: 1em !important;
    }

    #sbo-rt-content * {
      word-wrap: break-word !important;
      word-break: break-word !important;
    }

    #sbo-rt-content table,
    #sbo-rt-content pre {
      overflow-x: unset !important;
      overflow: unset !important;
      overflow-y: unset !important;
      white-space: pre-wrap !important;
    }
  </style>
</head>

<body>
  <div id="sbo-rt-content">
    <section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 9. Unsupervised Learning Techniques">
      <div class="chapter" id="unsupervised_learning_chapter">
        <h1><span class="label">Chapter 9. </span>Unsupervised Learning Techniques</h1>


        <p>Although most of the applications of Machine Learning today are based on supervised learning (and as a
          result, this is where most of the investments go to), the vast majority of the available data is actually
          unlabeled: we have the input features <strong>X</strong>, but we do not have the labels <strong>y</strong>.
          Yann LeCun famously said that “if intelligence was a cake, unsupervised learning would be the cake, supervised
          learning would be the icing on the cake, and reinforcement learning would be the cherry on the cake”. In other
          words, there is a huge potential in unsupervised learning that we have only barely started to sink our teeth
          into.</p>

        <p>For example, say you want to create a system that will take a few pictures of each item on a manufacturing
          production line and detect which items are defective. You can fairly easily create a system that will take
          pictures automatically, and this might give you thousands of pictures every day. You can then build a
          reasonably large dataset in just a few weeks. But wait, there are no labels! If you want to train a regular
          binary classifier that will predict whether an item is defective or not, you will need to label every single
          picture as “defective” or “normal”. This will generally require human experts to sit down and manually go
          through all the pictures. This is a long, costly and tedious task, so it will usually only be done on a small
          subset of the available pictures. As a result, the labeled dataset will be quite small, and the classifier’s
          performance will be disappointing. Moreover, every time the company makes any change to its products, the
          whole process will need to be started over from scratch. Wouldn’t it be great if the algorithm could just
          exploit the unlabeled data without needing humans to label every picture? Enter unsupervised learning.</p>

        <p>In <a data-type="xref" href="ch08.xhtml#dimensionality_chapter">Chapter 8</a>, we looked at the most common
          unsupervised learning task: dimensionality reduction. In this chapter, we will look at a few more unsupervised
          learning tasks and algorithms:</p>

        <ul>
          <li>
            <p><em>Clustering</em>: the goal is to group similar instances together into <em>clusters</em>. This is a
              great tool for data analysis, customer segmentation, recommender systems, search engines, image
              segmentation, semi-supervised learning, dimensionality reduction, and more.</p>
          </li>
          <li>
            <p><em>Anomaly detection</em>: the objective is to learn what “normal” data looks like, and use this to
              detect abnormal instances, such as defective items on a production line or a new trend in a time series.
            </p>
          </li>
          <li>
            <p><em>Density estimation</em>: this is the task of estimating the <em>probability density function</em>
              (PDF) of the random process that generated the dataset. This is commonly used for anomaly detection:
              instances located in very low-density regions are likely to be anomalies. It is also useful for data
              analysis and visualization.</p>
          </li>
        </ul>

        <p>Ready for some cake? We will start with clustering, using K-Means and DBSCAN, and then we will discuss
          Gaussian mixture models and see how they can be used for density estimation, clustering, and anomaly
          detection.</p>






        <section data-type="sect1" data-pdf-bookmark="Clustering">
          <div class="sect1" id="idm139656371865392">
            <h1>Clustering</h1>

            <p>As you enjoy a hike in the mountains, you stumble upon a plant you have never seen before. You look
              around and you notice a few more. They are not perfectly identical, yet they are sufficiently similar for
              you to know that they most likely belong to the same species (or at least the same genus). You may need a
              botanist to tell you what species that is, but you certainly don’t need an expert to identify groups of
              similar-looking objects. This is called <em>clustering</em>: it is the task of identifying similar
              instances and assigning them to <em>clusters</em>, i.e., groups of similar instances.</p>

            <p>Just like in classification, each instance gets assigned to a group. However, this is an unsupervised
              task. Consider <a data-type="xref" href="#classification_vs_clustering_diagram">Figure 9-1</a>: on the
              left is the iris dataset (introduced in <a data-type="xref"
                href="ch04.xhtml#linear_models_chapter">Chapter 4</a>), where each instance’s species (i.e., its class)
              is represented with a different marker. It is a labeled dataset, for which classification algorithms such
              as Logistic Regression, SVMs or Random Forest classifiers are well suited. On the right is the same
              dataset, but without the labels, so you cannot use a classification algorithm anymore. This is where
              clustering algorithms step in: many of them can easily detect the top left cluster. It is also quite easy
              to see with our own eyes, but it is not so obvious that the lower right cluster is actually composed of
              two distinct sub-clusters. That said, the dataset actually has two additional features (sepal length and
              width), not represented here, and clustering algorithms can make good use of all features, so in fact they
              identify the three clusters fairly well (e.g., using a Gaussian mixture model, only 5 instances out of 150
              are assigned to the wrong cluster).</p>

            <figure class="smallerseventy">
              <div id="classification_vs_clustering_diagram" class="figure">
                <img src="mlst_0901.png" alt="mlst 0901" width="2598" height="940" />
                <h6><span class="label">Figure 9-1. </span>Classification (left) versus clustering (right)</h6>
              </div>
            </figure>

            <p>Clustering is used in a wide variety of applications, including:</p>

            <ul>
              <li>
                <p>For customer segmentation: you can cluster your customers based on their purchases, their activity on
                  your website, and so on. This is useful to understand who your customers are and what they need, so
                  you can adapt your products and marketing campaigns to each segment. For example, this can be useful
                  in <em>recommender systems</em> to suggest content that other users in the same cluster enjoyed.</p>
              </li>
              <li>
                <p>For data analysis: when analyzing a new dataset, it is often useful to first discover clusters of
                  similar instances, as it is often easier to analyze clusters separately.</p>
              </li>
              <li>
                <p>As a dimensionality reduction technique: once a dataset has been clustered, it is usually possible to
                  measure each instance’s <em>affinity</em> with each cluster (affinity is any measure of how well an
                  instance fits into a cluster). Each instance’s feature vector <strong>x</strong> can then be replaced
                  with the vector of its cluster affinities. If there are <em>k</em> clusters, then this vector is
                  <em>k</em> dimensional. This is typically much lower dimensional than the original feature vector, but
                  it can preserve enough information for further processing.</p>
              </li>
              <li>
                <p>For <em>anomaly detection</em> (also called <em>outlier detection</em>): any instance that has a low
                  affinity to all the clusters is likely to be an anomaly. For example, if you have clustered the users
                  of your website based on their behavior, you can detect users with unusual behavior, such as an
                  unusual number of requests per second, and so on. Anomaly detection is particularly useful in
                  detecting defects in manufacturing, or for <em>fraud detection</em>.</p>
              </li>
              <li>
                <p>For semi-supervised learning: if you only have a few labels, you could perform clustering and
                  propagate the labels to all the instances in the same cluster. This can greatly increase the amount of
                  labels available for a subsequent supervised learning algorithm, and thus improve its performance.</p>
              </li>
              <li>
                <p>For search engines: for example, some search engines let you search for images that are similar to a
                  reference image. To build such a system, you would first apply a clustering algorithm to all the
                  images in your database: similar images would end up in the same cluster. Then when a user provides a
                  reference image, all you need to do is to find this image’s cluster using the trained clustering
                  model, and you can then simply return all the images from this cluster.</p>
              </li>
              <li>
                <p>To segment an image: by clustering pixels according to their color, then replacing each pixel’s color
                  with the mean color of its cluster, it is possible to reduce the number of different colors in the
                  image considerably. This technique is used in many object detection and tracking systems, as it makes
                  it easier to detect the contour of each object.</p>
              </li>
            </ul>

            <p>There is no universal definition of what a cluster is: it really depends on the context, and different
              algorithms will capture different kinds of clusters. For example, some algorithms look for instances
              centered around a particular point, called a <em>centroid</em>. Others look for continuous regions of
              densely packed instances: these clusters can take on any shape. Some algorithms are hierarchical, looking
              for clusters of clusters. And the list goes on.</p>

            <p>In this section, we will look at two popular clustering algorithms: K-Means and DBSCAN, and we will show
              some of their applications, such as non-linear dimensionality reduction, semi-supervised learning and
              anomaly detection.</p>








            <section data-type="sect2" data-pdf-bookmark="K-Means">
              <div class="sect2" id="idm139656371841552">
                <h2>K-Means</h2>

                <p>Consider the unlabeled dataset represented in <a data-type="xref"
                    href="#blobs_diagram">Figure 9-2</a>: you can clearly see 5 blobs of instances. The K-Means
                  algorithm is a simple algorithm capable of clustering this kind of dataset very quickly and
                  efficiently, often in just a few iterations. It was proposed by Stuart Lloyd at the Bell Labs in 1957
                  as a technique for pulse-code modulation, but it was only published outside of the company in 1982, in
                  a paper titled <a href="https://homl.info/36">“Least square quantization in PCM”</a>.<sup><a
                      data-type="noteref" id="idm139656371837632-marker"
                      href="ch09.xhtml#idm139656371837632">1</a></sup> By then, in 1965, Edward W. Forgy had published
                  virtually the same algorithm, so K-Means is sometimes referred to as Lloyd-Forgy.</p>

                <figure class="smallerseventy">
                  <div id="blobs_diagram" class="figure">
                    <img src="mlst_0902.png" alt="mlst 0902" width="2296" height="1084" />
                    <h6><span class="label">Figure 9-2. </span>An unlabeled dataset composed of five blobs of instances
                    </h6>
                  </div>
                </figure>

                <p>Let’s train a K-Means clusterer on this dataset. It will try to find each blob’s center and assign
                  each instance to the closest blob:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">KMeans</code>
<code class="n">k</code> <code class="o">=</code> <code class="mi">5</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">k</code><code class="p">)</code>
<code class="n">y_pred</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit_predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

                <p>Note that you have to specify the number of clusters <em>k</em> that the algorithm must find. In this
                  example, it is pretty obvious from looking at the data that <em>k</em> should be set to 5, but in
                  general it is not that easy. We will discuss this shortly.</p>

                <p>Each instance was assigned to one of the 5 clusters. In the context of clustering, an instance’s
                  <em>label</em> is the index of the cluster that this instance gets assigned to by the algorithm: this
                  is not to be confused with the class labels in classification (remember that clustering is an
                  unsupervised learning task). The <code>KMeans</code> instance preserves a copy of the labels of the
                  instances it was trained on, available via the <code>labels_</code> instance variable:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code>
<code class="go">array([4, 0, 1, ..., 2, 1, 0], dtype=int32)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="ow">is</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code>
<code class="go">True</code></pre>

                <p>We can also take a look at the 5 centroids that the algorithm found:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code>
<code class="go">array([[-2.80389616,  1.80117999],</code>
<code class="go">       [ 0.20876306,  2.25551336],</code>
<code class="go">       [-2.79290307,  2.79641063],</code>
<code class="go">       [-1.46679593,  2.28585348],</code>
<code class="go">       [-2.80037642,  1.30082566]])</code></pre>

                <p>Of course, you can easily assign new instances to the cluster whose centroid is closest:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mf">2.5</code><code class="p">]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([1, 1, 2, 2], dtype=int32)</code></pre>

                <p>If you plot the cluster’s decision boundaries, you get a Voronoi tessellation (see <a
                    data-type="xref" href="#voronoi_diagram">Figure 9-3</a>, where each centroid is represented with an
                  X):</p>

                <figure class="smallerseventy">
                  <div id="voronoi_diagram" class="figure">
                    <img src="mlst_0903.png" alt="mlst 0903" width="2296" height="1084" />
                    <h6><span class="label">Figure 9-3. </span>K-Means decision boundaries (Voronoi tessellation)</h6>
                  </div>
                </figure>

                <p>The vast majority of the instances were clearly assigned to the appropriate cluster, but a few
                  instances were probably mislabeled (especially near the boundary between the top left cluster and the
                  central cluster). Indeed, the K-Means algorithm does not behave very well when the blobs have very
                  different diameters since all it cares about when assigning an instance to a cluster is the distance
                  to the centroid.</p>

                <p>Instead of assigning each instance to a single cluster, which is called <em>hard clustering</em>, it
                  can be useful to just give each instance a score per cluster: this is called <em>soft clustering</em>.
                  For example, the score can be the distance between the instance and the centroid, or conversely it can
                  be a similarity score (or affinity) such as the Gaussian Radial Basis Function (introduced in <a
                    data-type="xref" href="ch05.xhtml#svm_chapter">Chapter 5</a>). In the <code>KMeans</code> class, the
                  <code>transform()</code> method measures the distance from each instance to every centroid:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([[2.81093633, 0.32995317, 2.9042344 , 1.49439034, 2.88633901],</code>
<code class="go">       [5.80730058, 2.80290755, 5.84739223, 4.4759332 , 5.84236351],</code>
<code class="go">       [1.21475352, 3.29399768, 0.29040966, 1.69136631, 1.71086031],</code>
<code class="go">       [0.72581411, 3.21806371, 0.36159148, 1.54808703, 1.21567622]])</code></pre>

                <p>In this example, the first instance in <code>X_new</code> is located at a distance of 2.81 from the
                  first centroid, 0.33 from the second centroid, 2.90 from the third centroid, 1.49 from the fourth
                  centroid and 2.87 from the fifth centroid. If you have a high-dimensional dataset and you transform it
                  this way, you end up with a <em>k</em>-dimensional dataset: this can be a very efficient non-linear
                  dimensionality reduction technique.</p>










                <section data-type="sect3" data-pdf-bookmark="The K-Means Algorithm">
                  <div class="sect3" id="idm139656371543456">
                    <h3>The K-Means Algorithm</h3>

                    <p>So how does the algorithm work? Well it is really quite simple. Suppose you were given the
                      centroids: you could easily label all the instances in the dataset by assigning each of them to
                      the cluster whose centroid is closest. Conversely, if you were given all the instance labels, you
                      could easily locate all the centroids by computing the mean of the instances for each cluster. But
                      you are given neither the labels nor the centroids, so how can you proceed? Well, just start by
                      placing the centroids randomly (e.g., by picking <em>k</em> instances at random and using their
                      locations as centroids). Then label the instances, update the centroids, label the instances,
                      update the centroids, and so on until the centroids stop moving. The algorithm is guaranteed to
                      converge in a finite number of steps (usually quite small), it will not oscillate
                      foreverfootenote:[This can be proven by pointing out that the mean squared distance between the
                      instances and their closest centroid can only go down at each step.]. You can see the algorithm in
                      action in <a data-type="xref" href="#kmeans_algorithm_diagram">Figure 9-4</a>: the centroids are
                      initialized randomly (top left), then the instances are labeled (top right), then the centroids
                      are updated (center left), the instances are relabeled (center right), and so on. As you can see,
                      in just 3 iterations the algorithm has reached a clustering that seems close to optimal.</p>

                    <figure class="smallerseventy">
                      <div id="kmeans_algorithm_diagram" class="figure">
                        <img src="mlst_0904.png" alt="mlst 0904" width="2896" height="2296" />
                        <h6><span class="label">Figure 9-4. </span>The K-Means algorithm</h6>
                      </div>
                    </figure>
                    <div data-type="note" epub:type="note">
                      <h6>Note</h6>
                      <p>The computational complexity of the algorithm is generally linear with regards to the number of
                        instances <em>m</em>, the number of clusters <em>k</em> and the number of dimensions <em>n</em>.
                        However, this is only true when the data has a clustering structure. If it does not, then in the
                        worst case scenario the complexity can increase exponentially with the number of instances. In
                        practice, however, this rarely happens, and K-Means is generally one of the fastest clustering
                        algorithms.</p>
                    </div>

                    <p>Unfortunately, although the algorithm is guaranteed to converge, it may not converge to the right
                      solution (i.e., it may converge to a local optimum): this depends on the centroid initialization.
                      For example, <a data-type="xref" href="#kmeans_variability_diagram">Figure 9-5</a> shows two
                      sub-optimal solutions that the algorithm can converge to if you are not lucky with the random
                      initialization step:</p>

                    <figure class="smallerseventy">
                      <div id="kmeans_variability_diagram" class="figure">
                        <img src="mlst_0905.png" alt="mlst 0905" width="2857" height="856" />
                        <h6><span class="label">Figure 9-5. </span>Sub-optimal solutions due to unlucky centroid
                          initializations</h6>
                      </div>
                    </figure>

                    <p>Let’s look at a few ways you can mitigate this risk by improving the centroid initialization.</p>
                  </div>
                </section>













                <section data-type="sect3" data-pdf-bookmark="Centroid Initialization Methods">
                  <div class="sect3" id="idm139656371541024">
                    <h3>Centroid Initialization Methods</h3>

                    <p>If you happen to know approximately where the centroids should be (e.g., if you ran another
                      clustering algorithm earlier), then you can set the <code>init</code> hyperparameter to a NumPy
                      array containing the list of centroids, and set <code>n_init</code> to 1:</p>

                    <pre data-type="programlisting"
                      data-code-language="python"><code class="n">good_init</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">3</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">],</code> <code class="p">[</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">2</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">]])</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">5</code><code class="p">,</code> <code class="n">init</code><code class="o">=</code><code class="n">good_init</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code></pre>

                    <p>Another solution is to run the algorithm multiple times with different random initializations and
                      keep the best solution. This is controlled by the <code>n_init</code> hyperparameter: by default,
                      it is equal to 10, which means that the whole algorithm described earlier actually runs 10 times
                      when you call <code>fit()</code>, and Scikit-Learn keeps the best solution. But how exactly does
                      it know which solution is the best? Well of course it uses a performance metric! It is called the
                      model’s <em>inertia</em>: this is the mean squared distance between each instance and its closest
                      centroid. It is roughly equal to 223.3 for the model on the left of <a data-type="xref"
                        href="#kmeans_variability_diagram">Figure 9-5</a>, 237.5 for the model on the right of <a
                        data-type="xref" href="#kmeans_variability_diagram">Figure 9-5</a>, and 211.6 for the model in
                      <a data-type="xref" href="#voronoi_diagram">Figure 9-3</a>. The <code>KMeans</code> class runs the
                      algorithm <code>n_init</code> times and keeps the model with the lowest inertia: in this example,
                      the model in <a data-type="xref" href="#voronoi_diagram">Figure 9-3</a> will be selected (unless
                      we are very unlucky with <code>n_init</code> consecutive random initializations). If you are
                      curious, a model’s inertia is accessible via the <code>inertia_</code> instance variable:</p>

                    <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">inertia_</code>
<code class="go">211.59853725816856</code></pre>

                    <p>The <code>score()</code> method returns the negative inertia. Why negative? Well, it is because a
                      predictor’s <code>score()</code> method must always respect the "<em>great is better</em>" rule.
                    </p>

                    <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">kmeans</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">-211.59853725816856</code></pre>

                    <p>An important improvement to the K-Means algorithm, called <em>K-Means+\+</em>, was proposed in a
                      <a href="https://homl.info/37">2006 paper</a> by David Arthur and Sergei Vassilvitskii:<sup><a
                          data-type="noteref" id="idm139656371448480-marker"
                          href="ch09.xhtml#idm139656371448480">2</a></sup> they introduced a smarter initialization step
                      that tends to select centroids that are distant from one another, and this makes the K-Means
                      algorithm much less likely to converge to a sub-optimal solution. They showed that the additional
                      computation required for the smarter initialization step is well worth it since it makes it
                      possible to drastically reduce the number of times the algorithm needs to be run to find the
                      optimal solution. Here is the K-Means++ initialization algorithm:</p>

                    <ul>
                      <li>
                        <p>Take one centroid <strong>c</strong><sup>(1)</sup>, chosen uniformly at random from the
                          dataset.</p>
                      </li>
                      <li>
                        <p>Take a new centroid <strong>c</strong><sup>(<em>i</em>)</sup>, choosing an instance
                          <strong>x</strong><sup>(<em>i</em>)</sup> with probability: <math
                            xmlns="http://www.w3.org/1998/Math/MathML"
                            alttext="upper D left-parenthesis bold x Superscript left-parenthesis i right-parenthesis Baseline right-parenthesis squared dollar-sign slash dollar-sign sigma-summation Underscript j equals 1 Overscript m Endscripts upper D left-parenthesis bold x Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis squared">
                            <mrow>
                              <mi>D</mi>
                              <msup>
                                <mrow>
                                  <mo>(</mo>
                                  <msup>
                                    <mi>𝐱</mi>
                                    <mrow>
                                      <mo>(</mo>
                                      <mi>i</mi>
                                      <mo>)</mo>
                                    </mrow>
                                  </msup>
                                  <mo>)</mo>
                                </mrow>
                                <mn>2</mn>
                              </msup>
                            </mrow>
                          </math><math xmlns="http://www.w3.org/1998/Math/MathML">
                            <mrow>
                              <munderover>
                                <mo>∑</mo>
                                <mrow>
                                  <mi>j</mi>
                                  <mo>=</mo>
                                  <mn>1</mn>
                                </mrow>
                                <mi>m</mi>
                              </munderover>
                              <msup>
                                <mrow>
                                  <mi>D</mi>
                                  <mo>(</mo>
                                  <msup>
                                    <mi>𝐱</mi>
                                    <mrow>
                                      <mo>(</mo>
                                      <mi>j</mi>
                                      <mo>)</mo>
                                    </mrow>
                                  </msup>
                                  <mo>)</mo>
                                </mrow>
                                <mn>2</mn>
                              </msup>
                            </mrow>
                          </math> where D(<strong>x</strong><sup>(<em>i</em>)</sup>) is the distance between the
                          instance <strong>x</strong><sup>(<em>i</em>)</sup> and the closest centroid that was already
                          chosen. This probability distribution ensures that instances further away from already chosen
                          centroids are much more likely be selected as centroids.</p>
                      </li>
                      <li>
                        <p>Repeat the previous step until all <em>k</em> centroids have been chosen.</p>
                      </li>
                    </ul>

                    <p>The <code>KMeans</code> class actually uses this initialization method by default. If you want to
                      force it to use the original method (i.e., picking <em>k</em> instances randomly to define the
                      initial centroids), then you can set the <code>init</code> hyperparameter to
                      <code>"random"</code>. You will rarely need to do this.</p>
                  </div>
                </section>













                <section data-type="sect3" data-pdf-bookmark="Accelerated K-Means and Mini-batch K-Means">
                  <div class="sect3" id="idm139656371647280">
                    <h3>Accelerated K-Means and Mini-batch K-Means</h3>

                    <p>Another important improvement to the K-Means algorithm was proposed in a <a
                        href="https://homl.info/38">2003 paper</a> by Charles Elkan.<sup><a data-type="noteref"
                          id="idm139656371500976-marker" href="ch09.xhtml#idm139656371500976">3</a></sup> It
                      considerably accelerates the algorithm by avoiding many unnecessary distance calculations: this is
                      achieved by exploiting the triangle inequality (i.e., the straight line is always the
                      shortest<sup><a data-type="noteref" id="idm139656371500048-marker"
                          href="ch09.xhtml#idm139656371500048">4</a></sup>) and by keeping track of lower and upper
                      bounds for distances between instances and centroids. This is the algorithm used by default by the
                      <code>KMeans</code> class (but you can force it to use the original algorithm by setting the
                      <code>algorithm</code> hyperparameter to <code>"full"</code>, although you probably will never
                      need to).</p>

                    <p>Yet another important variant of the K-Means algorithm was proposed in a <a
                        href="https://homl.info/39">2010 paper</a> by David Sculley.<sup><a data-type="noteref"
                          id="idm139656371496800-marker" href="ch09.xhtml#idm139656371496800">5</a></sup> Instead of
                      using the full dataset at each iteration, the algorithm is capable of using mini-batches, moving
                      the centroids just slightly at each iteration. This speeds up the algorithm typically by a factor
                      of 3 or 4 and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn
                      implements this algorithm in the <code>MiniBatchKMeans</code> class. You can just use this class
                      like the <code>KMeans</code> class:</p>

                    <pre data-type="programlisting"
                      data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">MiniBatchKMeans</code>

<code class="n">minibatch_kmeans</code> <code class="o">=</code> <code class="n">MiniBatchKMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">minibatch_kmeans</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

                    <p>If the dataset does not fit in memory, the simplest option is to use the <code>memmap</code>
                      class, as we did for incremental PCA in <a data-type="xref"
                        href="ch08.xhtml#dimensionality_chapter">Chapter 8</a>. Alternatively, you can pass one
                      mini-batch at a time to the <code>partial_fit()</code> method, but this will require much more
                      work, since you will need to perform multiple initializations and select the best one yourself
                      (see the notebook for an example).</p>

                    <p>Although the Mini-batch K-Means algorithm is much faster than the regular K-Means algorithm, its
                      inertia is generally slightly worse, especially as the number of clusters increases. You can see
                      this in <a data-type="xref" href="#minibatch_kmeans_vs_kmeans">Figure 9-6</a>: the plot on the
                      left compares the inertias of Mini-batch K-Means and regular K-Means models trained on the
                      previous dataset using various numbers of clusters <em>k</em>. The difference between the two
                      curves remains fairly constant, but this difference becomes more and more significant as
                      <em>k</em> increases, since the inertia becomes smaller and smaller. However, in the plot on the
                      right, you can see that Mini-batch K-Means is much faster than regular K-Means, and this
                      difference increases with <em>k</em>.</p>

                    <figure class="smallerseventy">
                      <div id="minibatch_kmeans_vs_kmeans" class="figure">
                        <img src="mlst_0906.png" alt="mlst 0906" width="2903" height="1088" />
                        <h6><span class="label">Figure 9-6. </span>Mini-batch K-Means vs K-Means: worse inertia as
                          <em>k</em> increases (left) but much faster (right)</h6>
                      </div>
                    </figure>
                  </div>
                </section>













                <section data-type="sect3" data-pdf-bookmark="Finding the Optimal Number of Clusters">
                  <div class="sect3" id="idm139656371384976">
                    <h3>Finding the Optimal Number of Clusters</h3>

                    <p>So far, we have set the number of clusters <em>k</em> to 5 because it was obvious by looking at
                      the data that this is the correct number of clusters. But in general, it will not be so easy to
                      know how to set <em>k</em>, and the result might be quite bad if you set it to the wrong value.
                      For example, as you can see in <a data-type="xref" href="#bad_n_clusters_diagram">Figure 9-7</a>,
                      setting <em>k</em> to 3 or 8 results in fairly bad models:</p>

                    <figure class="smallerseventy">
                      <div id="bad_n_clusters_diagram" class="figure">
                        <img src="mlst_0907.png" alt="mlst 0907" width="2896" height="852" />
                        <h6><span class="label">Figure 9-7. </span>Bad choices for the number of clusters</h6>
                      </div>
                    </figure>

                    <p>You might be thinking that we could just pick the model with the lowest inertia, right?
                      Unfortunately, it is not that simple. The inertia for <em>k</em>=3 is 653.2, which is much higher
                      than for <em>k</em>=5 (which was 211.6), but with <em>k</em>=8, the inertia is just 119.1. The
                      inertia is not a good performance metric when trying to choose <em>k</em> since it keeps getting
                      lower as we increase <em>k</em>. Indeed, the more clusters there are, the closer each instance
                      will be to its closest centroid, and therefore the lower the inertia will be. Let’s plot the
                      inertia as a function of <em>k</em> (see <a data-type="xref"
                        href="#inertia_vs_k_diagram">Figure 9-8</a>):</p>

                    <figure class="smallerseventy">
                      <div id="inertia_vs_k_diagram" class="figure">
                        <img src="mlst_0908.png" alt="mlst 0908" width="2293" height="923" />
                        <h6><span class="label">Figure 9-8. </span>Selecting the number of clusters <em>k</em> using the
                          “elbow rule”</h6>
                      </div>
                    </figure>

                    <p>As you can see, the inertia drops very quickly as we increase <em>k</em> up to 4, but then it
                      decreases much more slowly as we keep increasing <em>k</em>. This curve has roughly the shape of
                      an arm, and there is an “elbow” at <em>k</em>=4 so if we did not know better, it would be a good
                      choice: any lower value would be dramatic, while any higher value would not help much, and we
                      might just be splitting perfectly good clusters in half for no good reason.</p>

                    <p>This technique for choosing the best value for the number of clusters is rather coarse. A more
                      precise approach (but also more computationally expensive) is to use the <em>silhouette
                        score</em>, which is the mean <em>silhouette coefficient</em> over all the instances. An
                      instance’s silhouette coefficient is equal to (<em>b</em> – <em>a</em>) / max(<em>a</em>,
                      <em>b</em>) where <em>a</em> is the mean distance to the other instances in the same cluster (it
                      is the mean intra-cluster distance), and <em>b</em> is the mean nearest-cluster distance, that is
                      the mean distance to the instances of the next closest cluster (defined as the one that minimizes
                      <em>b</em>, excluding the instance’s own cluster). The silhouette coefficient can vary between -1
                      and +1: a coefficient close to +1 means that the instance is well inside its own cluster and far
                      from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary,
                      and finally a coefficient close to -1 means that the instance may have been assigned to the wrong
                      cluster. To compute the silhouette score, you can use Scikit-Learn’s
                      <code>silhouette_score()</code> function, giving it all the instances in the dataset, and the
                      labels they were assigned:</p>

                    <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">silhouette_score</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">silhouette_score</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">)</code>
<code class="go">0.655517642572828</code></pre>

                    <p>Let’s compare the silhouette scores for different numbers of clusters (see <a data-type="xref"
                        href="#silhouette_score_vs_k_diagram">Figure 9-9</a>):</p>

                    <figure class="smallerseventy">
                      <div id="silhouette_score_vs_k_diagram" class="figure">
                        <img src="mlst_0909.png" alt="mlst 0909" width="2299" height="784" />
                        <h6><span class="label">Figure 9-9. </span>Selecting the number of clusters <em>k</em> using the
                          silhouette score</h6>
                      </div>
                    </figure>

                    <p>As you can see, this visualization is much richer than the previous one: in particular, although
                      it confirms that <em>k</em>=4 is a very good choice, it also underlines the fact that <em>k</em>=5
                      is quite good as well, and much better than <em>k</em>=6 or 7. This was not visible when comparing
                      inertias.</p>

                    <p>An even more informative visualization is obtained when you plot every instance’s silhouette
                      coefficient, sorted by the cluster they are assigned to and by the value of the coefficient. This
                      is called a <em>silhouette diagram</em> (see <a data-type="xref"
                        href="#silhouette_analysis_diagram">Figure 9-10</a>):</p>

                    <figure class="smallerseventy">
                      <div id="silhouette_analysis_diagram" class="figure">
                        <img src="mlst_0910.png" alt="mlst 0910" width="3202" height="2589" />
                        <h6><span class="label">Figure 9-10. </span>Silouhette analysis: comparing the silhouette
                          diagrams for various values of <em>k</em></h6>
                      </div>
                    </figure>

                    <p>The vertical dashed lines represent the silhouette score for each number of clusters. When most
                      of the instances in a cluster have a lower coefficient than this score (i.e., if many of the
                      instances stop short of the dashed line, ending to the left of it), then the cluster is rather bad
                      since this means its instances are much too close to other clusters. We can see that when
                      <em>k</em>=3 and when <em>k</em>=6, we get bad clusters. But when <em>k</em>=4 or <em>k</em>=5,
                      the clusters look pretty good – most instances extend beyond the dashed line, to the right and
                      closer to 1.0. When <em>k</em>=4, the cluster at index 1 (the third from the top), is rather big,
                      while when <em>k</em>=5, all clusters have similar sizes, so even though the overall silhouette
                      score from <em>k</em>=4 is slightly greater than for <em>k</em>=5, it seems like a good idea to
                      use <em>k</em>=5 to get clusters of similar sizes.</p>
                  </div>
                </section>



              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Limits of K-Means">
              <div class="sect2" id="idm139656371840736">
                <h2>Limits of K-Means</h2>

                <p>Despite its many merits, most notably being fast and scalable, K-Means is not perfect. As we saw, it
                  is necessary to run the algorithm several times to avoid sub-optimal solutions, plus you need to
                  specify the number of clusters, which can be quite a hassle. Moreover, K-Means does not behave very
                  well when the clusters have varying sizes, different densities, or non-spherical shapes. For example,
                  <a data-type="xref" href="#bad_kmeans_diagram">Figure 9-11</a> shows how K-Means clusters a dataset
                  containing three ellipsoidal clusters of different dimensions, densities and orientations:</p>

                <figure class="smallerseventy">
                  <div id="bad_kmeans_diagram" class="figure">
                    <img src="mlst_0911.png" alt="mlst 0911" width="2895" height="856" />
                    <h6><span class="label">Figure 9-11. </span>K-Means fails to cluster these ellipsoidal blobs
                      properly</h6>
                  </div>
                </figure>

                <p>As you can see, neither of these solutions are any good. The solution on the left is better, but it
                  still chops off 25% of the middle cluster and assigns it to the cluster on the right. The solution on
                  the right is just terrible, even though its inertia is lower. So depending on the data, different
                  clustering algorithms may perform better. For example, on these types of elliptical clusters, Gaussian
                  mixture models work great.</p>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>It is important to scale the input features before you run K-Means, or else the clusters may be
                    very stretched, and K-Means will perform poorly. Scaling the features does not guarantee that all
                    the clusters will be nice and spherical, but it generally improves things.</p>
                </div>

                <p>Now let’s look at a few ways we can benefit from clustering. We will use K-Means, but feel free to
                  experiment with other clustering algorithms.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Using clustering for image segmentation">
              <div class="sect2" id="idm139656371321904">
                <h2>Using clustering for image segmentation</h2>

                <p><em>Image segmentation</em> is the task of partitioning an image into multiple segments. In
                  <em>semantic segmentation</em>, all pixels that are part of the same object type get assigned to the
                  same segment. For example, in a self-driving car’s vision system, all pixels that are part of a
                  pedestrian’s image might be assigned to the “pedestrian” segment (there would just be one segment
                  containing all the pedestrians). In <em>instance segmentation</em>, all pixels that are part of the
                  same individual object are assigned to the same segment. In this case there would be a different
                  segment for each pedestrian. The state of the art in semantic or instance segmentation today is
                  achieved using complex architectures based on convolutional neural networks (see Chapter 14). Here, we
                  are going to do something much simpler: <em>color segmentation</em>. We will simply assign pixels to
                  the same segment if they have a similar color. In some applications, this may be sufficient, for
                  example if you want to analyze satellite images to measure how much total forest area there is in a
                  region, color segmentation may be just fine.</p>

                <p>First, let’s load the image (see the upper left image in <a data-type="xref"
                    href="#image_segmentation_diagram">Figure 9-12</a>) using Matplotlib’s <code>imread()</code>
                  function:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">matplotlib.image</code> <code class="kn">import</code> <code class="n">imread</code>  <code class="c"># you could also use `imageio.imread()`</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">image</code> <code class="o">=</code> <code class="n">imread</code><code class="p">(</code><code class="n">os</code><code class="o">.</code><code class="n">path</code><code class="o">.</code><code class="n">join</code><code class="p">(</code><code class="s">"images"</code><code class="p">,</code><code class="s">"clustering"</code><code class="p">,</code><code class="s">"ladybug.png"</code><code class="p">))</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">image</code><code class="o">.</code><code class="n">shape</code>
<code class="go">(533, 800, 3)</code></pre>

                <p>The image is represented as a 3D array: the first dimension’s size is the height, the second is the
                  width, and the third is the number of color channels, in this case red, green and blue (RGB). In other
                  words, for each pixel there is a 3D vector containing the intensities of red, green and blue, each
                  between 0.0 and 1.0 (or between 0 and 255 if you use <code>imageio.imread()</code>). Some images may
                  have less channels, such as grayscale images (one channel), or more channels, such as images with an
                  additional <em>alpha channel</em> for transparency, or satellite images which often contain channels
                  for many light frequencies (e.g., infrared). The following code reshapes the array to get a long list
                  of RGB colors, then it clusters these colors using K-Means. For example, it may identify a color
                  cluster for all shades of green. Next, for each color (e.g., dark green), it looks for the mean color
                  of the pixel’s color cluster. For example, all shades of green may be replaced with the same light
                  green color (assuming the mean color of the green cluster is light green). Finally it reshapes this
                  long list of colors to get the same shape as the original image. And we’re done!</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">X</code> <code class="o">=</code> <code class="n">image</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">3</code><code class="p">)</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">8</code><code class="p">)</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">segmented_img</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">cluster_centers_</code><code class="p">[</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">]</code>
<code class="n">segmented_img</code> <code class="o">=</code> <code class="n">segmented_img</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="n">image</code><code class="o">.</code><code class="n">shape</code><code class="p">)</code></pre>

                <p>This outputs the image shown in the upper right of <a data-type="xref"
                    href="#image_segmentation_diagram">Figure 9-12</a>. You can experiment with various numbers of
                  clusters, as shown in the figure. When you use less than 8 clusters, notice that the ladybug’s flashy
                  red color fails to get a cluster of its own: it gets merged with colors from the environment. This is
                  due to the fact that the ladybug is quite small, much smaller than the rest of the image, so even
                  though its color is flashy, K-Means fails to dedicate a cluster to it: as mentioned earlier, K-Means
                  prefers clusters of similar sizes.</p>

                <figure class="smallerseventy">
                  <div id="image_segmentation_diagram" class="figure">
                    <img src="mlst_0912.png" alt="mlst 0912" width="2325" height="1156" />
                    <h6><span class="label">Figure 9-12. </span>Image segmentation using K-Means with various numbers of
                      color clusters</h6>
                  </div>
                </figure>

                <p>That was not too hard, was it? Now let’s look at another application of clustering: preprocessing.
                </p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Using Clustering for Preprocessing">
              <div class="sect2" id="idm139656371198736">
                <h2>Using Clustering for Preprocessing</h2>

                <p>Clustering can be an efficient approach to dimensionality reduction, in particular as a preprocessing
                  step before a supervised learning algorithm. For example, let’s tackle the <em>digits dataset</em>
                  which is a simple MNIST-like dataset containing 1,797 grayscale 8×8 images representing digits 0 to 9.
                  First, let’s load the dataset:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_digits</code>

<code class="n">X_digits</code><code class="p">,</code> <code class="n">y_digits</code> <code class="o">=</code> <code class="n">load_digits</code><code class="p">(</code><code class="n">return_X_y</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code></pre>

                <p>Now, let’s split it into a training set and a test set:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="n">X_train</code><code class="p">,</code> <code class="n">X_test</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_test</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X_digits</code><code class="p">,</code> <code class="n">y_digits</code><code class="p">)</code></pre>

                <p>Next, let’s fit a Logistic Regression model:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>

<code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

                <p>Let’s evaluate its accuracy on the test set:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9666666666666667</code></pre>

                <p>Okay, that’s our baseline: 96.7% accuracy. Let’s see if we can do better by using K-Means as a
                  preprocessing step. We will create a pipeline that will first cluster the training set into 50
                  clusters and replace the images with their distances to these 50 clusters, then apply a logistic
                  regression model.</p>
                <div data-type="warning" epub:type="warning">
                  <h6>Warning</h6>
                  <p>Although it is tempting to define the number of clusters to 10, since there are 10 different
                    digits, it is unlikely to perform well, because there are several different ways to write each
                    digit.</p>
                </div>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code>

<code class="n">pipeline</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
    <code class="p">(</code><code class="s2">"kmeans"</code><code class="p">,</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="mi">50</code><code class="p">)),</code>
    <code class="p">(</code><code class="s2">"log_reg"</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">()),</code>
<code class="p">])</code>
<code class="n">pipeline</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

                <p>Now let’s evaluate this classification pipeline:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">pipeline</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9822222222222222</code></pre>

                <p>How about that? We almost divided the error rate by a factor of 2!</p>

                <p>But we chose the number of clusters <em>k</em> completely arbitrarily, we can surely do better. Since
                  K-Means is just a preprocessing step in a classification pipeline, finding a good value for <em>k</em>
                  is much simpler than earlier: there’s no need to perform silhouette analysis or minimize the inertia,
                  the best value of <em>k</em> is simply the one that results in the best classification performance
                  during cross-validation. Let’s use <code>GridSearchCV</code> to find the optimal number of clusters:
                </p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>

<code class="n">param_grid</code> <code class="o">=</code> <code class="nb">dict</code><code class="p">(</code><code class="n">kmeans__n_clusters</code><code class="o">=</code><code class="nb">range</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">100</code><code class="p">))</code>
<code class="n">grid_clf</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">pipeline</code><code class="p">,</code> <code class="n">param_grid</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">verbose</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">grid_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code></pre>

                <p>Let’s look at best value for <em>k</em>, and the performance of the resulting pipeline:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">grid_clf</code><code class="o">.</code><code class="n">best_params_</code>
<code class="go">{'kmeans__n_clusters': 90}</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">grid_clf</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9844444444444445</code></pre>

                <p>With <em>k</em>=90 clusters, we get a small accuracy boost, reaching 98.4% accuracy on the test set.
                  Cool!</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Using Clustering for Semi-Supervised Learning">
              <div class="sect2" id="idm139656371197792">
                <h2>Using Clustering for Semi-Supervised Learning</h2>

                <p>Another use case for clustering is in semi-supervised learning, when we have plenty of unlabeled
                  instances and very few labeled instances. Let’s train a logistic regression model on a sample of 50
                  labeled instances from the digits dataset:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">n_labeled</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:</code><code class="n">n_labeled</code><code class="p">],</code> <code class="n">y_train</code><code class="p">[:</code><code class="n">n_labeled</code><code class="p">])</code></pre>

                <p>What is the performance of this model on the test set?</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.8266666666666667</code></pre>

                <p>The accuracy is just 82.7%: it should come as no surprise that this is much lower than earlier, when
                  we trained the model on the full training set. Let’s see how we can do better. First, let’s cluster
                  the training set into 50 clusters, then for each cluster let’s find the image closest to the centroid.
                  We will call these images the representative images:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">k</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">kmeans</code> <code class="o">=</code> <code class="n">KMeans</code><code class="p">(</code><code class="n">n_clusters</code><code class="o">=</code><code class="n">k</code><code class="p">)</code>
<code class="n">X_digits_dist</code> <code class="o">=</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">representative_digit_idx</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmin</code><code class="p">(</code><code class="n">X_digits_dist</code><code class="p">,</code> <code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">X_representative_digits</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="n">representative_digit_idx</code><code class="p">]</code></pre>

                <p><a data-type="xref" href="#representative_images_diagram">Figure 9-13</a> shows these 50
                  representative images:</p>

                <figure class="smallerseventy">
                  <div id="representative_images_diagram" class="figure">
                    <img src="mlst_0913.png" alt="mlst 0913" width="1776" height="453" />
                    <h6><span class="label">Figure 9-13. </span>Fifty representative digit images (one per cluster)</h6>
                  </div>
                </figure>

                <p>Now let’s look at each image and manually label it:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">y_representative_digits</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([</code><code class="mi">4</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">8</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="o">...</code><code class="p">,</code> <code class="mi">7</code><code class="p">,</code> <code class="mi">6</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">])</code></pre>

                <p>Now we have a dataset with just 50 labeled instances, but instead of being completely random
                  instances, each of them is a representative image of its cluster. Let’s see if the performance is any
                  better:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_representative_digits</code><code class="p">,</code> <code class="n">y_representative_digits</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9244444444444444</code></pre>

                <p>Wow! We jumped from 82.7% accuracy to 92.4%, although we are still only training the model on 50
                  instances. Since it is often costly and painful to label instances, especially when it has to be done
                  manually by experts, it is a good idea to label representative instances rather than just random
                  instances.</p>

                <p>But perhaps we can go one step further: what if we propagated the labels to all the other instances
                  in the same cluster? This is called <em>label propagation</em>:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">y_train_propagated</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">empty</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">),</code> <code class="n">dtype</code><code class="o">=</code><code class="n">np</code><code class="o">.</code><code class="n">int32</code><code class="p">)</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">k</code><code class="p">):</code>
    <code class="n">y_train_propagated</code><code class="p">[</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="o">==</code><code class="n">i</code><code class="p">]</code> <code class="o">=</code> <code class="n">y_representative_digits</code><code class="p">[</code><code class="n">i</code><code class="p">]</code></pre>

                <p>Now let’s train the model again and look at its performance:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">y_train_propagated</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9288888888888889</code></pre>

                <p>We got a tiny little accuracy boost. Better than nothing, but not astounding. The problem is that we
                  propagated each representative instance’s label to all the instances in the same cluster, including
                  the instances located close to the cluster boundaries, which are more likely to be mislabeled. Let’s
                  see what happens if we only propagate the labels to the 20% of the instances that are closest to the
                  centroids:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">percentile_closest</code> <code class="o">=</code> <code class="mi">20</code>
<code class="err">​</code>
<code class="n">X_cluster_dist</code> <code class="o">=</code> <code class="n">X_digits_dist</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">arange</code><code class="p">(</code><code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)),</code> <code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code><code class="p">]</code>
<code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">k</code><code class="p">):</code>
    <code class="n">in_cluster</code> <code class="o">=</code> <code class="p">(</code><code class="n">kmeans</code><code class="o">.</code><code class="n">labels_</code> <code class="o">==</code> <code class="n">i</code><code class="p">)</code>
    <code class="n">cluster_dist</code> <code class="o">=</code> <code class="n">X_cluster_dist</code><code class="p">[</code><code class="n">in_cluster</code><code class="p">]</code>
    <code class="n">cutoff_distance</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">percentile</code><code class="p">(</code><code class="n">cluster_dist</code><code class="p">,</code> <code class="n">percentile_closest</code><code class="p">)</code>
    <code class="n">above_cutoff</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_cluster_dist</code> <code class="o">&gt;</code> <code class="n">cutoff_distance</code><code class="p">)</code>
    <code class="n">X_cluster_dist</code><code class="p">[</code><code class="n">in_cluster</code> <code class="o">&amp;</code> <code class="n">above_cutoff</code><code class="p">]</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>

<code class="n">partially_propagated</code> <code class="o">=</code> <code class="p">(</code><code class="n">X_cluster_dist</code> <code class="o">!=</code> <code class="o">-</code><code class="mi">1</code><code class="p">)</code>
<code class="n">X_train_partially_propagated</code> <code class="o">=</code> <code class="n">X_train</code><code class="p">[</code><code class="n">partially_propagated</code><code class="p">]</code>
<code class="n">y_train_partially_propagated</code> <code class="o">=</code> <code class="n">y_train_propagated</code><code class="p">[</code><code class="n">partially_propagated</code><code class="p">]</code></pre>

                <p>Now let’s train the model again on this partially propagated dataset:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_partially_propagated</code><code class="p">,</code> <code class="n">y_train_partially_propagated</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">score</code><code class="p">(</code><code class="n">X_test</code><code class="p">,</code> <code class="n">y_test</code><code class="p">)</code>
<code class="go">0.9422222222222222</code></pre>

                <p>Nice! With just 50 labeled instances (only 5 examples per class on average!), we got 94.2%
                  performance, which is pretty close to the performance of logistic regression on the fully labeled
                  digits dataset (which was 96.7%). This is because the propagated labels are actually pretty good,
                  their accuracy is very close to 99%:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">y_train_partially_propagated</code> <code class="o">==</code> <code class="n">y_train</code><code class="p">[</code><code class="n">partially_propagated</code><code class="p">])</code>
<code class="go">0.9896907216494846</code></pre>
                <aside data-type="sidebar" epub:type="sidebar">
                  <div class="sidebar" id="idm139656370314816">
                    <h5>Active Learning</h5>
                    <p>To continue improving your model and your training set, the next step could be to do a few rounds
                      of <em>active learning</em>: this is when a human expert interacts with the learning algorithm,
                      providing labels when the algorithm needs them. There are many different strategies for active
                      learning, but one of the most common ones is called <em>uncertainty sampling</em>:</p>

                    <ul>
                      <li>
                        <p>The model is trained on the labeled instances gathered so far, and this model is used to make
                          predictions on all the unlabeled instances.</p>
                      </li>
                      <li>
                        <p>The instances for which the model is most uncertain (i.e., when its estimated probability is
                          lowest) must be labeled by the expert.</p>
                      </li>
                      <li>
                        <p>Then you just iterate this process again and again, until the performance improvement stops
                          being worth the labeling effort.</p>
                      </li>
                    </ul>

                    <p>Other strategies include labeling the instances that would result in the largest model change, or
                      the largest drop in the model’s validation error, or the instances that different models disagree
                      on (e.g., an SVM, a Random Forest, and so on).</p>
                  </div>
                </aside>

                <p>Before we move on to Gaussian mixture models, let’s take a look at DBSCAN, another popular clustering
                  algorithm that illustrates a very different approach based on local density estimation. This approach
                  allows the algorithm to identify clusters of arbitrary shapes.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="DBSCAN">
              <div class="sect2" id="idm139656370712368">
                <h2>DBSCAN</h2>

                <p>This algorithm defines clusters as continuous regions of high density. It is actually quite simple:
                </p>

                <ul>
                  <li>
                    <p>For each instance, the algorithm counts how many instances are located within a small distance ε
                      (epsilon) from it. This region is called the instance’s <em>ε-neighborhood</em>.</p>
                  </li>
                  <li>
                    <p>If an instance has at least <code>min_samples</code> instances in its ε-neighborhood (including
                      itself), then it is considered a <em>core instance</em>. In other words, core instances are those
                      that are located in dense regions.</p>
                  </li>
                  <li>
                    <p>All instances in the neighborhood of a core instance belong to the same cluster. This may include
                      other core instances, therefore a long sequence of neighboring core instances forms a single
                      cluster.</p>
                  </li>
                  <li>
                    <p>Any instance that is not a core instance and does not have one in its neighborhood is considered
                      an anomaly.</p>
                  </li>
                </ul>

                <p>This algorithm works well if all the clusters are dense enough, and they are well separated by
                  low-density regions. The <code>DBSCAN</code> class in Scikit-Learn is as simple to use as you might
                  expect. Let’s test it on the moons dataset, introduced in <a data-type="xref"
                    href="ch05.xhtml#svm_chapter">Chapter 5</a>:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.cluster</code> <code class="kn">import</code> <code class="n">DBSCAN</code>
<code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">make_moons</code>

<code class="n">X</code><code class="p">,</code> <code class="n">y</code> <code class="o">=</code> <code class="n">make_moons</code><code class="p">(</code><code class="n">n_samples</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">noise</code><code class="o">=</code><code class="mf">0.05</code><code class="p">)</code>
<code class="n">dbscan</code> <code class="o">=</code> <code class="n">DBSCAN</code><code class="p">(</code><code class="n">eps</code><code class="o">=</code><code class="mf">0.05</code><code class="p">,</code> <code class="n">min_samples</code><code class="o">=</code><code class="mi">5</code><code class="p">)</code>
<code class="n">dbscan</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

                <p>The labels of all the instances are now available in the <code>labels_</code> instance variable:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">dbscan</code><code class="o">.</code><code class="n">labels_</code>
<code class="go">array([ 0,  2, -1, -1,  1,  0,  0,  0, ...,  3,  2,  3,  3,  4,  2,  6,  3])</code></pre>

                <p>Notice that some instances have a cluster index equal to -1: this means that they are considered as
                  anomalies by the algorithm. The indices of the core instances are available in the
                  <code>core_sample_indices_</code> instance variable, and the core instances themselves are available
                  in the <code>components_</code> instance variable:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="nb">len</code><code class="p">(</code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code><code class="p">)</code>
<code class="go">808</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code>
<code class="go">array([ 0,  4,  5,  6,  7,  8, 10, 11, ..., 992, 993, 995, 997, 998, 999])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">dbscan</code><code class="o">.</code><code class="n">components_</code>
<code class="go">array([[-0.02137124,  0.40618608],</code>
<code class="go">       [-0.84192557,  0.53058695],</code>
<code class="go">                  ...</code>
<code class="go">       [-0.94355873,  0.3278936 ],</code>
<code class="go">       [ 0.79419406,  0.60777171]])</code></pre>

                <p>This clustering is represented in the left plot of <a data-type="xref"
                    href="#dbscan_diagram">Figure 9-14</a>. As you can see, it identified quite a lot of anomalies, plus
                  7 different clusters. How disappointing! Fortunately, if we widen each instance’s neighborhood by
                  increasing <code>eps</code> to 0.2, we get the clustering on the right, which looks perfect. Let’s
                  continue with this model.</p>

                <figure class="smallerseventy">
                  <div id="dbscan_diagram" class="figure">
                    <img src="mlst_0914.png" alt="mlst 0914" width="2596" height="856" />
                    <h6><span class="label">Figure 9-14. </span>DBSCAN clustering using two different neighborhood
                      radiuses</h6>
                  </div>
                </figure>

                <p>Somewhat surprisingly, the DBSCAN class does not have a <code>predict()</code> method, although it
                  has a <code>fit_predict()</code> method. In other words, it cannot predict which cluster a new
                  instance belongs to. The rationale for this decision is that several classification algorithms could
                  make sense here, and it is easy enough to train one, for example a <code>KNeighborsClassifier</code>:
                </p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.neighbors</code> <code class="kn">import</code> <code class="n">KNeighborsClassifier</code>

<code class="n">knn</code> <code class="o">=</code> <code class="n">KNeighborsClassifier</code><code class="p">(</code><code class="n">n_neighbors</code><code class="o">=</code><code class="mi">50</code><code class="p">)</code>
<code class="n">knn</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">dbscan</code><code class="o">.</code><code class="n">components_</code><code class="p">,</code> <code class="n">dbscan</code><code class="o">.</code><code class="n">labels_</code><code class="p">[</code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code><code class="p">])</code></pre>

                <p>Now, given a few new instances, we can predict which cluster they most likely belong to, and even
                  estimate a probability for each cluster. Note that we only trained them on the core instances, but we
                  could also have chosen to train them on all the instances, or all but the anomalies: this choice
                  depends on the final task.</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="o">-</code><code class="mf">0.5</code><code class="p">,</code> <code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">0</code><code class="p">,</code> <code class="mf">0.5</code><code class="p">],</code> <code class="p">[</code><code class="mi">1</code><code class="p">,</code> <code class="o">-</code><code class="mf">0.1</code><code class="p">],</code> <code class="p">[</code><code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">knn</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([1, 0, 1, 0])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">knn</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([[0.18, 0.82],</code>
<code class="go">       [1.  , 0.  ],</code>
<code class="go">       [0.12, 0.88],</code>
<code class="go">       [1.  , 0.  ]])</code></pre>

                <p>The decision boundary is represented on <a data-type="xref"
                    href="#cluster_classification_diagram">Figure 9-15</a> (the crosses represent the 4 instances in
                  <code>X_new</code>). Notice that since there is no anomaly in the KNN’s training set, the classifier
                  always chooses a cluster, even when that cluster is far away. However, it is fairly straightforward to
                  introduce a maximum distance, in which case the two instances that are far away from both clusters are
                  classified as anomalies. To do this, we can use the <code>kneighbors()</code> method of the
                  <code>KNeighborsClassifier</code>: given a set of instances, it returns the distances and the indices
                  of the <em>k</em> nearest neighbors in the training set (two matrices, each with <em>k</em> columns):
                </p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">y_dist</code><code class="p">,</code> <code class="n">y_pred_idx</code> <code class="o">=</code> <code class="n">knn</code><code class="o">.</code><code class="n">kneighbors</code><code class="p">(</code><code class="n">X_new</code><code class="p">,</code> <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">1</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code> <code class="o">=</code> <code class="n">dbscan</code><code class="o">.</code><code class="n">labels_</code><code class="p">[</code><code class="n">dbscan</code><code class="o">.</code><code class="n">core_sample_indices_</code><code class="p">][</code><code class="n">y_pred_idx</code><code class="p">]</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code><code class="p">[</code><code class="n">y_dist</code> <code class="o">&gt;</code> <code class="mf">0.2</code><code class="p">]</code> <code class="o">=</code> <code class="o">-</code><code class="mi">1</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_pred</code><code class="o">.</code><code class="n">ravel</code><code class="p">()</code>
<code class="go">array([-1,  0,  1, -1])</code></pre>

                <figure class="smallerseventy">
                  <div id="cluster_classification_diagram" class="figure">
                    <img src="mlst_0915.png" alt="mlst 0915" width="1696" height="784" />
                    <h6><span class="label">Figure 9-15. </span>cluster_classification_diagram</h6>
                  </div>
                </figure>

                <p>In short, DBSCAN is a very simple yet powerful algorithm, capable of identifying any number of
                  clusters, of any shape, it is robust to outliers, and it has just two hyperparameters
                  (<code>eps</code> and <code>min_samples</code>). However, if the density varies significantly across
                  the clusters, it can be impossible for it to capture all the clusters properly. Moreover, its
                  computational complexity is roughly O(<em>m</em> log <em>m</em>), making it pretty close to linear
                  with regards to the number of instances. However, Scikit-Learn’s implementation can require up to
                  O(<em>m</em><sup>2</sup>) memory if <code>eps</code> is large.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Other Clustering Algorithms">
              <div class="sect2" id="idm139656370288560">
                <h2>Other Clustering Algorithms</h2>

                <p>Scikit-Learn implements several more clustering algorithms that you should take a look at. We cannot
                  cover them all in detail here, but here is a brief overview:</p>

                <ul>
                  <li>
                    <p><em>Agglomerative clustering</em>: a hierarchy of clusters is built from the bottom up. Think of
                      many tiny bubbles floating on water and gradually attaching to each other until there’s just one
                      big group of bubbles. Similarly, at each iteration agglomerative clustering connects the nearest
                      pair of clusters (starting with individual instances). If you draw a tree with a branch for every
                      pair of clusters that merged, you get a binary tree of clusters, where the leaves are the
                      individual instances. This approach scales very well to large numbers of instances or clusters, it
                      can capture clusters of various shapes, it produces a flexible and informative cluster tree
                      instead of forcing you to choose a particular cluster scale, and it can be used with any pairwise
                      distance. It can scale nicely to large numbers of instances if you provide a connectivity matrix.
                      This is a sparse <em>m</em> by <em>m</em> matrix that indicates which pairs of instances are
                      neighbors (e.g., returned by <code>sklearn.neighbors.kneighbors_graph()</code>). Without a
                      connectivity matrix, the algorithm does not scale well to large datasets.</p>
                  </li>
                  <li>
                    <p><em>Birch</em>: this algorithm was designed specifically for very large datasets, and it can be
                      faster than batch K-Means, with similar results, as long as the number of features is not too
                      large (&lt;20). It builds a tree structure during training containing just enough information to
                      quickly assign each new instance to a cluster, without having to store all the instances in the
                      tree: this allows it to use limited memory, while handle huge datasets.</p>
                  </li>
                  <li>
                    <p><em>Mean-shift</em>: this algorithm starts by placing a circle centered on each instance, then
                      for each circle it computes the mean of all the instances located within it, and it shifts the
                      circle so that it is centered on the mean. Next, it iterates this mean-shift step until all the
                      circles stop moving (i.e., until each of them is centered on the mean of the instances it
                      contains). This algorithm shifts the circles in the direction of higher density, until each of
                      them has found a local density maximum. Finally, all the instances whose circles have settled in
                      the same place (or close enough) are assigned to the same cluster. This has some of the same
                      features as DBSCAN, in particular it can find any number of clusters of any shape, it has just one
                      hyperparameter (the radius of the circles, called the bandwidth) and it relies on local density
                      estimation. However, it tends to chop clusters into pieces when they have internal density
                      variations. Unfortunately, its computational complexity is O(<em>m</em><sup>2</sup>), so it is not
                      suited for large datasets.</p>
                  </li>
                  <li>
                    <p><em>Affinity propagation</em>: this algorithm uses a voting system, where instances vote for
                      similar instances to be their representatives, and once the algorithm converges, each
                      representative and its voters form a cluster. This algorithm can detect any number of clusters of
                      different sizes. Unfortunately, this algorithm has a computational complexity of
                      O(<em>m</em><sup>2</sup>), so it is not suited for large datasets.</p>
                  </li>
                  <li>
                    <p><em>Spectral clustering</em>: this algorithm takes a similarity matrix between the instances and
                      creates a low-dimensional embedding from it (i.e., it reduces its dimensionality), then it uses
                      another clustering algorithm in this low-dimensional space (Scikit-Learn’s implementation uses
                      K-Means). Spectral clustering can capture complex cluster structures, and it can also be used to
                      cut graphs (e.g., to identify clusters of friends on a social network), however it does not scale
                      well to large number of instances, and it does not behave well when the clusters have very
                      different sizes.</p>
                  </li>
                </ul>

                <p>Now let’s dive into Gaussian mixture models, which can be used for density estimation, clustering and
                  anomaly detection.</p>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Gaussian Mixtures">
          <div class="sect1" id="idm139656371864768">
            <h1>Gaussian Mixtures</h1>

            <p>A <em>Gaussian mixture model</em> (GMM) is a probabilistic model that assumes that the instances were
              generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances
              generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. Each
              cluster can have a different ellipsoidal shape, size, density and orientation, just like in <a
                data-type="xref" href="#bad_kmeans_diagram">Figure 9-11</a>. When you observe an instance, you know it
              was generated from one of the Gaussian distributions, but you are not told which one, and you do not know
              what the parameters of these distributions are.</p>

            <p>There are several GMM variants: in the simplest variant, implemented in the <code>GaussianMixture</code>
              class, you must know in advance the number <em>k</em> of Gaussian distributions. The dataset
              <strong>X</strong> is assumed to have been generated through the following probabilistic process:</p>

            <ul>
              <li>
                <p>For each instance, a cluster is picked randomly among <em>k</em> clusters. The probability of
                  choosing the <em>j</em><sup>th</sup> cluster is defined by the cluster’s weight
                  <em>ϕ</em><sup>(<em>j</em>)</sup>.<sup><a data-type="noteref" id="idm139656369752080-marker"
                      href="ch09.xhtml#idm139656369752080">6</a></sup> The index of the cluster chosen for the
                  <em>i</em><sup>th</sup> instance is noted <em>z</em><sup>(<em>i</em>)</sup>.</p>
              </li>
              <li>
                <p>If <em>z</em><sup>(<em>i</em>)</sup>=<em>j</em>, meaning the <em>i</em><sup>th</sup> instance has
                  been assigned to the <em>j</em><sup>th</sup> cluster, the location
                  <strong>x</strong><sup>(<em>i</em>)</sup> of this instance is sampled randomly from the Gaussian
                  distribution with mean <strong>μ</strong><sup>(<em>j</em>)</sup> and covariance matrix
                  <strong>Σ</strong><sup>(<em>j</em>)</sup>. This is noted <math
                    xmlns="http://www.w3.org/1998/Math/MathML"
                    alttext="bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis">
                    <mrow>
                      <msup>
                        <mi>𝐱</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>i</mi>
                          <mo>)</mo>
                        </mrow>
                      </msup>
                      <mo>∼</mo>
                      <mi>𝒩</mi>
                      <mrow>
                        <mo>(</mo>
                        <msup>
                          <mrow>
                            <mi>μ</mi>
                          </mrow>
                          <mrow>
                            <mo>(</mo>
                            <mi>j</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                        <mo>,</mo>
                        <msup>
                          <mrow>
                            <mi>Σ</mi>
                          </mrow>
                          <mrow>
                            <mo>(</mo>
                            <mi>j</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </math>.</p>
              </li>
            </ul>

            <p>This generative process can be represented as a <em>graphical model</em> (see <a data-type="xref"
                href="#gmm_plate_diagram">Figure 9-16</a>). This is a graph which represents the structure of the
              conditional dependencies between random variables.</p>

            <figure class="smallerseventy">
              <div id="gmm_plate_diagram" class="figure">
                <img src="mlst_0916.png" alt="mlst 0916" width="1675" height="883" />
                <h6><span class="label">Figure 9-16. </span>Gaussian mixture model</h6>
              </div>
            </figure>

            <p>Here is how to interpret it:<sup><a data-type="noteref" id="idm139656369861744-marker"
                  href="ch09.xhtml#idm139656369861744">7</a></sup></p>

            <ul>
              <li>
                <p>The circles represent random variables.</p>
              </li>
              <li>
                <p>The squares represent fixed values (i.e., parameters of the model).</p>
              </li>
              <li>
                <p>The large rectangles are called <em>plates</em>: they indicate that their content is repeated several
                  times.</p>
              </li>
              <li>
                <p>The number indicated at the bottom right hand side of each plate indicates how many times its content
                  is repeated, so there are <em>m</em> random variables <em>z</em><sup>(<em>i</em>)</sup> (from
                  <em>z</em><sup>(1)</sup> to <em>z</em><sup>(<em>m</em>)</sup>) and <em>m</em> random variables
                  <strong>x</strong><sup>(<em>i</em>)</sup>, and <em>k</em> means
                  <strong>μ</strong><sup>(<em>j</em>)</sup> and <em>k</em> covariance matrices
                  <strong>Σ</strong><sup>(<em>j</em>)</sup>, but just one weight vector <strong>ϕ</strong> (containing
                  all the weights <em>ϕ</em><sup>(1)</sup> to <em>ϕ</em><sup>(<em>k</em>)</sup>).</p>
              </li>
              <li>
                <p>Each variable <em>z</em><sup>(<em>i</em>)</sup> is drawn from the <em>categorical distribution</em>
                  with weights <strong>ϕ</strong>. Each variable <strong>x</strong><sup>(<em>i</em>)</sup> is drawn from
                  the normal distribution with the mean and covariance matrix defined by its cluster
                  <em>z</em><sup>(<em>i</em>)</sup>.</p>
              </li>
              <li>
                <p>The solid arrows represent conditional dependencies. For example, the probability distribution for
                  each random variable <em>z</em><sup>(<em>i</em>)</sup> depends on the weight vector
                  <strong>ϕ</strong>. Note that when an arrow crosses a plate boundary, it means that it applies to all
                  the repetitions of that plate, so for example the weight vector <strong>ϕ</strong> conditions the
                  probability distributions of all the random variables <strong>x</strong><sup>(1)</sup> to
                  <strong>x</strong><sup>(<em>m</em>)</sup>.</p>
              </li>
              <li>
                <p>The squiggly arrow from <em>z</em><sup>(<em>i</em>)</sup> to
                  <strong>x</strong><sup>(<em>i</em>)</sup> represents a switch: depending on the value of
                  <em>z</em><sup>(<em>i</em>)</sup>, the instance <strong>x</strong><sup>(<em>i</em>)</sup> will be
                  sampled from a different Gaussian distribution. For example, if
                  <em>z</em><sup>(<em>i</em>)</sup>=<em>j</em>, then <math xmlns="http://www.w3.org/1998/Math/MathML"
                    alttext="bold x Superscript left-parenthesis i right-parenthesis Baseline tilde script upper N left-parenthesis mu Superscript left-parenthesis j right-parenthesis Baseline comma bold upper Sigma Superscript left-parenthesis j right-parenthesis Baseline right-parenthesis">
                    <mrow>
                      <msup>
                        <mi>𝐱</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>i</mi>
                          <mo>)</mo>
                        </mrow>
                      </msup>
                      <mo>∼</mo>
                      <mi>𝒩</mi>
                      <mrow>
                        <mo>(</mo>
                        <msup>
                          <mi>μ</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>j</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                        <mo>,</mo>
                        <msup>
                          <mi>Σ</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>j</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </math>.</p>
              </li>
              <li>
                <p>Shaded nodes indicate that the value is known, so in this case only the random variables
                  <strong>x</strong><sup>(<em>i</em>)</sup> have known values: they are called <em>observed
                    variables</em>. The unknown random variables <em>z</em><sup>(<em>i</em>)</sup> are called <em>latent
                    variables</em>.</p>
              </li>
            </ul>

            <p>So what can you do with such a model? Well, given the dataset <strong>X</strong>, you typically want to
              start by estimating the weights <strong>ϕ</strong> and all the distribution parameters
              <strong>μ</strong><sup>(1)</sup> to <strong>μ</strong><sup>(<em>k</em>)</sup> and
              <strong>Σ</strong><sup>(1)</sup> to <strong>Σ</strong><sup>(<em>k</em>)</sup>. Scikit-Learn’s
              <code>GaussianMixture</code> class makes this trivial:</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.mixture</code> <code class="kn">import</code> <code class="n">GaussianMixture</code>

<code class="n">gm</code> <code class="o">=</code> <code class="n">GaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
<code class="n">gm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

            <p>Let’s look at the parameters that the algorithm estimated:</p>

            <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">weights_</code>
<code class="go">array([0.20965228, 0.4000662 , 0.39028152])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">means_</code>
<code class="go">array([[ 3.39909717,  1.05933727],</code>
<code class="go">       [-1.40763984,  1.42710194],</code>
<code class="go">       [ 0.05135313,  0.07524095]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">covariances_</code>
<code class="go">array([[[ 1.14807234, -0.03270354],</code>
<code class="go">        [-0.03270354,  0.95496237]],</code>

<code class="go">       [[ 0.63478101,  0.72969804],</code>
<code class="go">        [ 0.72969804,  1.1609872 ]],</code>

<code class="go">       [[ 0.68809572,  0.79608475],</code>
<code class="go">        [ 0.79608475,  1.21234145]]])</code></pre>

            <p>Great, it worked fine! Indeed, the weights that were used to generate the data were 0.2, 0.4 and 0.4, and
              similarly, the means and covariance matrices were very close to those found by the algorithm. But how?
              This class relies on the <em>Expectation-Maximization</em> (EM) algorithm, which has many similarities
              with the K-Means algorithm: it also initializes the cluster parameters randomly, then it repeats two steps
              until convergence, first assigning instances to clusters (this is called the <em>expectation step</em>)
              then updating the clusters (this is called the <em>maximization step</em>). Sounds familiar? Indeed, in
              the context of clustering you can think of EM as a generalization of K-Means which not only finds the
              cluster centers (<strong>μ</strong><sup>(1)</sup> to <strong>μ</strong><sup>(<em>k</em>)</sup>), but also
              their size, shape and orientation (<strong>Σ</strong><sup>(1)</sup> to
              <strong>Σ</strong><sup>(<em>k</em>)</sup>), as well as their relative weights (<em>ϕ</em><sup>(1)</sup> to
              <em>ϕ</em><sup>(<em>k</em>)</sup>). Unlike K-Means, EM uses soft cluster assignments rather than hard
              assignments: for each instance during the expectation step, the algorithm estimates the probability that
              it belongs to each cluster (based on the current cluster parameters). Then, during the maximization step,
              each cluster is updated using <em>all</em> the instances in the dataset, with each instance weighted by
              the estimated probability that it belongs to that cluster. These probabilities are called the
              <em>responsibilities</em> of the clusters for the instances. During the maximization step, each cluster’s
              update will mostly be impacted by the instances it is most responsible for.</p>
            <div data-type="warning" epub:type="warning">
              <h6>Warning</h6>
              <p>Unfortunately, just like K-Means, EM can end up converging to poor solutions, so it needs to be run
                several times, keeping only the best solution. This is why we set <code>n_init</code> to 10. Be careful:
                by default <code>n_init</code> is only set to 1.</p>
            </div>

            <p>You can check whether or not the algorithm converged and how many iterations it took:</p>

            <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">converged_</code>
<code class="go">True</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">n_iter_</code>
<code class="go">3</code></pre>

            <p>Okay, now that you have an estimate of the location, size, shape, orientation and relative weight of each
              cluster, the model can easily assign each instance to the most likely cluster (hard clustering) or
              estimate the probability that it belongs to a particular cluster (soft clustering). For this, just use the
              <code>predict()</code> method for hard clustering, or the <code>predict_proba()</code> method for soft
              clustering:</p>

            <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">array([2, 2, 1, ..., 0, 0, 0])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">array([[2.32389467e-02, 6.77397850e-07, 9.76760376e-01],</code>
<code class="go">       [1.64685609e-02, 6.75361303e-04, 9.82856078e-01],</code>
<code class="go">       [2.01535333e-06, 9.99923053e-01, 7.49319577e-05],</code>
<code class="go">       ...,</code>
<code class="go">       [9.99999571e-01, 2.13946075e-26, 4.28788333e-07],</code>
<code class="go">       [1.00000000e+00, 1.46454409e-41, 5.12459171e-16],</code>
<code class="go">       [1.00000000e+00, 8.02006365e-41, 2.27626238e-15]])</code></pre>

            <p>It is a <em>generative model</em>, meaning you can actually sample new instances from it (note that they
              are ordered by cluster index):</p>

            <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code><code class="p">,</code> <code class="n">y_new</code> <code class="o">=</code> <code class="n">gm</code><code class="o">.</code><code class="n">sample</code><code class="p">(</code><code class="mi">6</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code>
<code class="go">array([[ 2.95400315,  2.63680992],</code>
<code class="go">       [-1.16654575,  1.62792705],</code>
<code class="go">       [-1.39477712, -1.48511338],</code>
<code class="go">       [ 0.27221525,  0.690366  ],</code>
<code class="go">       [ 0.54095936,  0.48591934],</code>
<code class="go">       [ 0.38064009, -0.56240465]])</code>

<code class="gp">&gt;&gt;&gt; </code><code class="n">y_new</code>
<code class="go">array([0, 1, 2, 2, 2, 2])</code></pre>

            <p>It is also possible to estimate the density of the model at any given location. This is achieved using
              the <code>score_samples()</code> method: for each instance it is given, this method estimates the log of
              the <em>probability density function</em> (PDF) at that location. The greater the score, the higher the
              density:</p>

            <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">score_samples</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">array([-2.60782346, -3.57106041, -3.33003479, ..., -3.51352783,</code>
<code class="go">       -4.39802535, -3.80743859])</code></pre>

            <p>If you compute the exponential of these scores, you get the value of the PDF at the location of the given
              instances. These are <em>not</em> probabilities, but probability <em>densities</em>: they can take on any
              positive value, not just between 0 and 1. To estimate the probability that an instance will fall within a
              particular region, you would have to integrate the PDF over that region (if you do so over the entire
              space of possible instance locations, the result will be 1).</p>

            <p><a data-type="xref" href="#gaussian_mixtures_diagram">Figure 9-17</a> shows the cluster means, the
              decision boundaries (dashed lines), and the density contours of this model:</p>

            <figure class="smallerseventy">
              <div id="gaussian_mixtures_diagram" class="figure">
                <img src="mlst_0917.png" alt="mlst 0917" width="2295" height="1087" />
                <h6><span class="label">Figure 9-17. </span>Cluster means, decision boundaries and density contours of a
                  trained Gaussian mixture model</h6>
              </div>
            </figure>

            <p>Nice! The algorithm clearly found an excellent solution. Of course, we made its task easy by actually
              generating the data using a set of 2D Gaussian distributions (unfortunately, real life data is not always
              so Gaussian and low-dimensional), and we also gave the algorithm the correct number of clusters. When
              there are many dimensions, or many clusters, or few instances, EM can struggle to converge to the optimal
              solution. You might need to reduce the difficulty of the task by limiting the number of parameters that
              the algorithm has to learn: one way to do this is to limit the range of shapes and orientations that the
              clusters can have. This can be achieved by imposing constraints on the covariance matrices. To do this,
              just set the <code>covariance_type</code> hyperparameter to one of the following values:</p>

            <ul>
              <li>
                <p><code>"spherical"</code>: all clusters must be spherical, but they can have different diameters
                  (i.e., different variances).</p>
              </li>
              <li>
                <p><code>"diag"</code>: clusters can take on any ellipsoidal shape of any size, but the ellipsoid’s axes
                  must be parallel to the coordinate axes (i.e., the covariance matrices must be diagonal).</p>
              </li>
              <li>
                <p><code>"tied"</code>: all clusters must have the same ellipsoidal shape, size and orientation (i.e.,
                  all clusters share the same covariance matrix).</p>
              </li>
            </ul>

            <p>By default, <code>covariance_type</code> is equal to <code>"full"</code>, which means that each cluster
              can take on any shape, size and orientation (it has its own unconstrained covariance matrix). <a
                data-type="xref" href="#covariance_type_diagram">Figure 9-18</a> plots the solutions found by the EM
              algorithm when <code>covariance_type</code> is set to <code>"tied"</code> or "<code>spherical</code>“.</p>

            <figure class="smallerseventy">
              <div id="covariance_type_diagram" class="figure">
                <img src="mlst_0918.png" alt="mlst 0918" width="2595" height="1096" />
                <h6><span class="label">Figure 9-18. </span>covariance_type_diagram</h6>
              </div>
            </figure>
            <div data-type="note" epub:type="note">
              <h6>Note</h6>
              <p>The computational complexity of training a <code>GaussianMixture</code> model depends on the number of
                instances <em>m</em>, the number of dimensions <em>n</em>, the number of clusters <em>k</em>, and the
                constraints on the covariance matrices. If <code>covariance_type</code> is <code>"spherical</code> or
                <code>"diag"</code>, it is O(<em>kmn</em>), assuming the data has a clustering structure. If
                <code>covariance_type</code> is <code>"tied"</code> or <code>"full"</code>, it is
                O(<em>kmn</em><sup>2</sup> + <em>kn</em><sup>3</sup>), so it will not scale to large numbers of
                features.</p>
            </div>

            <p>Gaussian mixture models can also be used for anomaly detection. Let’s see how.</p>








            <section data-type="sect2" data-pdf-bookmark="Anomaly Detection using Gaussian Mixtures">
              <div class="sect2" id="idm139656369657248">
                <h2>Anomaly Detection using Gaussian Mixtures</h2>

                <p><em>Anomaly detection</em> (also called <em>outlier detection</em>) is the task of detecting
                  instances that deviate strongly from the norm. These instances are of course called <em>anomalies</em>
                  or <em>outliers</em>, while the normal instances are called <em>inliers</em>. Anomaly detection is
                  very useful in a wide variety of applications, for example in fraud detection, or for detecting
                  defective products in manufacturing, or to remove outliers from a dataset before training another
                  model, which can significantly improve the performance of the resulting model.</p>

                <p>Using a Gaussian mixture model for anomaly detection is quite simple: any instance located in a
                  low-density region can be considered an anomaly. You must define what density threshold you want to
                  use. For example, in a manufacturing company that tries to detect defective products, the ratio of
                  defective products is usually well-known. Say it is equal to 4%, then you can set the density
                  threshold to be the value that results in having 4% of the instances located in areas below that
                  threshold density. If you notice that you get too many false positives (i.e., perfectly good products
                  that are flagged as defective), you can lower the threshold. Conversely, if you have too many false
                  negatives (i.e., defective products that the system does not flag as defective), you can increase the
                  threshold. This is the usual precision/recall tradeoff (see <a data-type="xref"
                    href="ch03.xhtml#classification_chapter">Chapter 3</a>). Here is how you would identify the outliers
                  using the 4th percentile lowest density as the threshold (i.e., approximately 4% of the instances will
                  be flagged as anomalies):</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">densities</code> <code class="o">=</code> <code class="n">gm</code><code class="o">.</code><code class="n">score_samples</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">density_threshold</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">percentile</code><code class="p">(</code><code class="n">densities</code><code class="p">,</code> <code class="mi">4</code><code class="p">)</code>
<code class="n">anomalies</code> <code class="o">=</code> <code class="n">X</code><code class="p">[</code><code class="n">densities</code> <code class="o">&lt;</code> <code class="n">density_threshold</code><code class="p">]</code></pre>

                <p>These anomalies are represented as stars on <a data-type="xref"
                    href="#mixture_anomaly_detection_diagram">Figure 9-19</a>:</p>

                <figure class="smallerseventy">
                  <div id="mixture_anomaly_detection_diagram" class="figure">
                    <img src="mlst_0919.png" alt="mlst 0919" width="2295" height="1087" />
                    <h6><span class="label">Figure 9-19. </span>Anomaly detection using a Gaussian mixture model</h6>
                  </div>
                </figure>

                <p>A closely related task is <em>novelty detection</em>: it differs from anomaly detection in that the
                  algorithm is assumed to be trained on a “clean” dataset, uncontaminated by outliers, whereas anomaly
                  detection does not make this assumption. Indeed, outlier detection is often precisely used to clean up
                  a dataset.</p>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>Gaussian mixture models try to fit all the data, including the outliers, so if you have too many of
                    them, this will bias the model’s view of “normality”: some outliers may wrongly be considered as
                    normal. If this happens, you can try to fit the model once, use it to detect and remove the most
                    extreme outliers, then fit the model again on the cleaned up dataset. Another approach is to use
                    robust covariance estimation methods (see the <code>EllipticEnvelope</code> class).</p>
                </div>

                <p>Just like K-Means, the <code>GaussianMixture</code> algorithm requires you to specify the number of
                  clusters. So how can you find it?</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Selecting the Number of Clusters">
              <div class="sect2" id="idm139656369370576">
                <h2>Selecting the Number of Clusters</h2>

                <p>With K-Means, you could use the inertia or the silhouette score to select the appropriate number of
                  clusters, but with Gaussian mixtures, it is not possible to use these metrics because they are not
                  reliable when the clusters are not spherical or have different sizes. Instead, you can try to find the
                  model that minimizes a <em>theoretical information criterion</em> such as the <em>Bayesian information
                    criterion</em> (BIC) or the <em>Akaike information criterion</em> (AIC), defined in <a
                    data-type="xref" href="#information_criteria_equation">Equation 9-1</a>.</p>
                <div data-type="equation" id="information_criteria_equation">
                  <h5><span class="label">Equation 9-1. </span>Bayesian information criterion (BIC) and Akaike
                    information criterion (AIC)</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <mi>B</mi>
                            <mi>I</mi>
                            <mi>C</mi>
                            <mo>=</mo>
                            <mspace width="0.166667em" />
                          </mrow>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="0.166667em" />
                            <mo form="prefix">log</mo>
                            <mrow>
                              <mo>(</mo>
                              <mi>m</mi>
                              <mo>)</mo>
                            </mrow>
                            <mi>p</mi>
                            <mo>-</mo>
                            <mn>2</mn>
                            <mo form="prefix">log</mo>
                            <mrow>
                              <mo>(</mo>
                              <mover accent="true">
                                <mi>L</mi>
                                <mo>^</mo>
                              </mover>
                              <mo>)</mo>
                            </mrow>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <mi>A</mi>
                            <mi>I</mi>
                            <mi>C</mi>
                            <mo>=</mo>
                            <mspace width="0.166667em" />
                          </mrow>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="0.166667em" />
                            <mn>2</mn>
                            <mi>p</mi>
                            <mo>-</mo>
                            <mn>2</mn>
                            <mo form="prefix">log</mo>
                            <mo>(</mo>
                            <mover accent="true">
                              <mi>L</mi>
                              <mo>^</mo>
                            </mover>
                            <mo>)</mo>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>

                <ul>
                  <li>
                    <p><em>m</em> is the number of instances, as always.</p>
                  </li>
                  <li>
                    <p><em>p</em> is the number of parameters learned by the model.</p>
                  </li>
                  <li>
                    <p><math xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove upper L With caret">
                        <mover accent="true">
                          <mi>L</mi>
                          <mo>^</mo>
                        </mover>
                      </math> is the maximized value of the <em>likelihood function</em> of the model.</p>
                  </li>
                </ul>

                <p>Both the BIC and the AIC penalize models that have more parameters to learn (e.g., more clusters),
                  and reward models that fit the data well. They often end up selecting the same model, but when they
                  differ, the model selected by the BIC tends to be simpler (fewer parameters) than the one selected by
                  the AIC, but it does not fit the data quite as well (this is especially true for larger datasets).</p>
                <aside data-type="sidebar" epub:type="sidebar">
                  <div class="sidebar" id="idm139656369332464">
                    <h5>Likelihood function</h5>
                    <p>The terms “probability” and “likelihood” are often used interchangeably in the English language,
                      but they have very different meanings in statistics: given a statistical model with some
                      parameters <strong>θ</strong>, the word “probability” is used to describe how plausible a future
                      outcome <strong>x</strong> is (knowing the parameter values <strong>θ</strong>), while the word
                      “likelihood” is used to describe how plausible a particular set of parameter values
                      <strong>θ</strong> are, after the outcome <strong>x</strong> is known.</p>

                    <p>Consider a one-dimensional mixture model of two Gaussian distributions centered at -4 and +1. For
                      simplicity, this toy model has a single parameter <em>θ</em> that controls the standard deviations
                      of both distributions. The top left contour plot in <a data-type="xref"
                        href="#likelihood_function_diagram">Figure 9-20</a> shows the entire model
                      <em>f</em>(<em>x</em>; <em>θ</em>) as a function of both <em>x</em> and <em>θ</em>. To estimate
                      the probability distribution of a future outcome <em>x</em>, you need to set the model parameter
                      <em>θ</em>. For example, if you set it to <em>θ</em>=1.3 (the horizontal line), you get the
                      probability density function <em>f</em>(<em>x</em>; <em>θ</em>=1.3) shown in the lower left plot.
                      Say you want to estimate the probability that <em>x</em> will fall between -2 and +2, you must
                      calculate the integral of the PDF on this range (i.e., the surface of the shaded region). On the
                      other hand, if you have observed a single instance <em>x</em>=2.5 (the vertical line in the upper
                      left plot), you get the likelihood function noted <math xmlns="http://www.w3.org/1998/Math/MathML"
                        alttext="script upper L">
                        <mi>ℒ</mi>
                      </math>(<em>θ</em>|<em>x</em>=2.5)=f(<em>x</em>=2.5; <em>θ</em>) represented in the upper right
                      plot.</p>

                    <p>In short, the PDF is a function of <em>x</em> (with <em>θ</em> fixed) while the likelihood
                      function is a function of <em>θ</em> (with <em>x</em> fixed). It is important to understand that
                      the likelihood function is <em>not</em> a probability distribution: if you integrate a probability
                      distribution over all possible values of <em>x</em>, you always get 1, but if you integrate the
                      likelihood function over all possible values of <em>θ</em>, the result can be any positive value.
                    </p>

                    <figure class="smallerseventy">
                      <div id="likelihood_function_diagram" class="figure">
                        <img src="mlst_0920.png" alt="mlst 0920" width="2270" height="1228" />
                        <h6><span class="label">Figure 9-20. </span>A model’s parametric function (top left), and some
                          derived functions: a PDF (lower left), a likelihood function (top right) and a log likelihood
                          function (lower right)</h6>
                      </div>
                    </figure>

                    <p>Given a dataset <strong>X</strong>, a common task is to try to estimate the most likely values
                      for the model parameters. To do this, you must find the values that maximize the likelihood
                      function, given <strong>X</strong>. In this example, if you have observed a single instance
                      <em>x</em>=2.5, the <em>maximum likelihood estimate</em> (MLE) of <em>θ</em> is <math
                        xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove theta With caret">
                        <mover accent="true">
                          <mi>θ</mi>
                          <mo>^</mo>
                        </mover>
                      </math>=1.5. If a prior probability distribution <em>g</em> over <em>θ</em> exists, it is possible
                      to take it into account by maximizing <math xmlns="http://www.w3.org/1998/Math/MathML"
                        alttext="script upper L">
                        <mi>ℒ</mi>
                      </math>(<em>θ</em>|<em>x</em>)g(<em>θ</em>) rather than just maximizing <math
                        xmlns="http://www.w3.org/1998/Math/MathML" alttext="script upper L">
                        <mi>ℒ</mi>
                      </math>(<em>θ</em>|<em>x</em>). This is called maximum a-posteriori (MAP) estimation. Since MAP
                      constrains the parameter values, you can think of it as a regularized version of MLE.</p>

                    <p>Notice that it is equivalent to maximize the likelihood function or to maximize its logarithm
                      (represented in the lower right hand side of <a data-type="xref"
                        href="#likelihood_function_diagram">Figure 9-20</a>): indeed, the logarithm is a strictly
                      increasing function, so if <em>θ</em> maximizes the log likelihood, it also maximizes the
                      likelihood. It turns out that it is generally easier to maximize the log likelihood. For example,
                      if you observed several independent instances <em>x</em><sup>(1)</sup> to
                      <em>x</em><sup>(<em>m</em>)</sup>, you would need to find the value of <em>θ</em> that maximizes
                      the product of the individual likelihood functions. But it is equivalent, and much simpler, to
                      maximize the sum (not the product) of the log likelihood functions, thanks to the magic of the
                      logarithm which converts products into sums: log(<em>ab</em>)=log(<em>a</em>)+log(<em>b</em>).</p>

                    <p>Once you have estimated <math xmlns="http://www.w3.org/1998/Math/MathML"
                        alttext="ModifyingAbove theta With caret">
                        <mover accent="true">
                          <mi>θ</mi>
                          <mo>^</mo>
                        </mover>
                      </math>, the value of <em>θ</em> that maximizes the likelihood function, then you are ready to
                      compute <math xmlns="http://www.w3.org/1998/Math/MathML"
                        alttext="ModifyingAbove upper L With caret equals script upper L left-parenthesis ModifyingAbove theta With caret comma bold upper X right-parenthesis">
                        <mrow>
                          <mover accent="true">
                            <mi>L</mi>
                            <mo>^</mo>
                          </mover>
                          <mo>=</mo>
                          <mi>ℒ</mi>
                          <mrow>
                            <mo>(</mo>
                            <mover accent="true">
                              <mi>θ</mi>
                              <mo>^</mo>
                            </mover>
                            <mo>,</mo>
                            <mi>𝐗</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </math>. This is the value which is used to compute the AIC and BIC: you can think of it as a
                      measure of how well the model fits the data.</p>
                  </div>
                </aside>

                <p>To compute the BIC and AIC, just call the <code>bic()</code> or <code>aic()</code> methods:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">bic</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">8189.74345832983</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">gm</code><code class="o">.</code><code class="n">aic</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="go">8102.518178214792</code></pre>

                <p><a data-type="xref" href="#aic_bic_vs_k_diagram">Figure 9-21</a> shows the BIC for different numbers
                  of clusters <em>k</em>. As you can see, both the BIC and the AIC are lowest when <em>k</em>=3, so it
                  is most likely the best choice. Note that we could also search for the best value for the
                  <code>covariance_type</code> hyperparameter. For example, if it is <code>"spherical"</code> rather
                  than <code>"full"</code>, then the model has much fewer parameters to learn, but it does not fit the
                  data as well.</p>

                <figure class="smallerseventy">
                  <div id="aic_bic_vs_k_diagram" class="figure">
                    <img src="mlst_0921.png" alt="mlst 0921" width="2298" height="773" />
                    <h6><span class="label">Figure 9-21. </span>AIC and BIC for different numbers of clusters <em>k</em>
                    </h6>
                  </div>
                </figure>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Bayesian Gaussian Mixture Models">
              <div class="sect2" id="idm139656369369952">
                <h2>Bayesian Gaussian Mixture Models</h2>

                <p>Rather than manually searching for the optimal number of clusters, it is possible to use instead the
                  <code>BayesianGaussianMixture</code> class which is capable of giving weights equal (or close) to zero
                  to unnecessary clusters. Just set the number of clusters <code>n_components</code> to a value that you
                  have good reason to believe is greater than the optimal number of clusters (this assumes some minimal
                  knowledge about the problem at hand), and the algorithm will eliminate the unnecessary clusters
                  automatically. For example, let’s set the number of clusters to 10 and see what happens:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.mixture</code> <code class="kn">import</code> <code class="n">BayesianGaussianMixture</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bgm</code> <code class="o">=</code> <code class="n">BayesianGaussianMixture</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">n_init</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">random_state</code><code class="o">=</code><code class="mi">42</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">bgm</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">round</code><code class="p">(</code><code class="n">bgm</code><code class="o">.</code><code class="n">weights_</code><code class="p">,</code> <code class="mi">2</code><code class="p">)</code>
<code class="go">array([0.4 , 0.21, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])</code></pre>

                <p>Perfect: the algorithm automatically detected that only 3 clusters are needed, and the resulting
                  clusters are almost identical to the ones in <a data-type="xref"
                    href="#gaussian_mixtures_diagram">Figure 9-17</a>.</p>

                <p>In this model, the cluster parameters (including the weights, means and covariance matrices) are not
                  treated as fixed model parameters anymore, but as latent random variables, like the cluster
                  assignments (see <a data-type="xref" href="#bgm_plate_diagram">Figure 9-22</a>). So <strong>z</strong>
                  now includes both the cluster parameters and the cluster assignments.</p>

                <figure class="smallerseventy">
                  <div id="bgm_plate_diagram" class="figure">
                    <img src="mlst_0922.png" alt="mlst 0922" width="1867" height="1008" />
                    <h6><span class="label">Figure 9-22. </span>Bayesian Gaussian mixture model</h6>
                  </div>
                </figure>

                <p>Prior knowledge about the latent variables <strong>z</strong> can be encoded in a probability
                  distribution <em>p</em>(<strong>z</strong>) called the <em>prior</em>. For example, we may have a
                  prior belief that the clusters are likely to be few (low concentration), or conversely, that they are
                  more likely to be plentiful (high concentration). This can be adjusted using the
                  <code>weight_concentration_prior</code> hyperparameter. Setting it to 0.01 or 1000 gives very
                  different clusterings (see <a data-type="xref"
                    href="#mixture_concentration_prior_diagram">Figure 9-23</a>). However, the more data we have, the
                  less the priors matter. In fact, to plot diagrams with such large differences, you must use very
                  strong priors and little data.</p>

                <figure class="smallerseventy">
                  <div id="mixture_concentration_prior_diagram" class="figure">
                    <img src="mlst_0923.png" alt="mlst 0923" width="2553" height="1096" />
                    <h6><span class="label">Figure 9-23. </span>Using different concentration priors</h6>
                  </div>
                </figure>
                <div data-type="note" epub:type="note">
                  <h6>Note</h6>
                  <p>The fact that you see only 3 regions in the right plot although there are 4 centroids is not a bug:
                    the weight of the top-right cluster is much larger than the weight of the lower-right cluster, so
                    the probability that any given point in this region belongs to the top-right cluster is greater than
                    the probability that it belongs to the lower-right cluster, even near the lower-right cluster.</p>
                </div>

                <p>Bayes’ theorem (<a data-type="xref" href="#bayes_theorem_equation">Equation 9-2</a>) tells us how to
                  update the probability distribution over the latent variables after we observe some data
                  <strong>X</strong>. It computes the <em>posterior</em> distribution
                  <em>p</em>(<strong>z</strong>|<strong>X</strong>), which is the conditional probability of
                  <strong>z</strong> given <strong>X</strong>.</p>
                <div data-type="equation" id="bayes_theorem_equation">
                  <h5><span class="label">Equation 9-2. </span>Bayes’ theorem</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mi>p</mi>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">z</mi>
                        <mo>|</mo>
                        <mi mathvariant="bold">X</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <mtext>Posterior</mtext>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mrow>
                            <mtext>Likelihood</mtext>
                            <mo>×</mo>
                            <mtext>Prior</mtext>
                          </mrow>
                          <mtext>Evidence</mtext>
                        </mfrac>
                      </mstyle>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mrow>
                            <mi>p</mi>
                            <mo>(</mo>
                            <mi mathvariant="bold">X</mi>
                            <mo>|</mo>
                            <mi mathvariant="bold">z</mi>
                            <mo>)</mo>
                            <mspace width="0.166667em" />
                            <mi>p</mi>
                            <mo>(</mo>
                            <mi mathvariant="bold">z</mi>
                            <mo>)</mo>
                          </mrow>
                          <mrow>
                            <mi>p</mi>
                            <mo>(</mo>
                            <mi mathvariant="bold">X</mi>
                            <mo>)</mo>
                          </mrow>
                        </mfrac>
                      </mstyle>
                    </mrow>
                  </math>
                </div>

                <p>Unfortunately, in a Gaussian mixture model (and many other problems), the denominator
                  <em>p</em>(<strong>x</strong>) is intractable, as it requires integrating over all the possible values
                  of <strong>z</strong> (<a data-type="xref" href="#evidence_integral_equation">Equation 9-3</a>). This
                  means considering all possible combinations of cluster parameters and cluster assignments.</p>
                <div data-type="equation" id="evidence_integral_equation">
                  <h5><span class="label">Equation 9-3. </span>The evidence <em>p</em>(<strong>X</strong>) is often
                    intractable</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mi>p</mi>
                      <mo>(</mo>
                      <mi mathvariant="bold">X</mi>
                      <mo>)</mo>
                      <mo>=</mo>
                      <mo>∫</mo>
                      <mrow>
                        <mi>p</mi>
                        <mo>(</mo>
                        <mi mathvariant="bold">X</mi>
                        <mo>|</mo>
                        <mi mathvariant="bold">z</mi>
                        <mo>)</mo>
                        <mi>p</mi>
                        <mo>(</mo>
                        <mi mathvariant="bold">z</mi>
                        <mo>)</mo>
                        <mi>d</mi>
                        <mi mathvariant="bold">z</mi>
                      </mrow>
                    </mrow>
                  </math>
                </div>

                <p>This is one of the central problems in Bayesian statistics, and there are several approaches to
                  solving it. One of them is <em>variational inference</em>, which picks a family of distributions
                  <em>q</em>(<strong>z</strong>; <strong>λ</strong>) with its own <em>variational parameters</em>
                  <strong>λ</strong> (lambda), then it optimizes these parameters to make <em>q</em>(<strong>z</strong>)
                  a good approximation of <em>p</em>(<strong>z</strong>|<strong>X</strong>). This is achieved by finding
                  the value of <strong>λ</strong> that minimizes the KL divergence from <em>q</em>(<strong>z</strong>)
                  to <em>p</em>(<strong>z</strong>|<strong>X</strong>), noted D<sub>KL</sub>(<em>q</em>‖<em>p</em>). The
                  KL divergence equation is shown in (see <a data-type="xref"
                    href="#variational_kl_divergence_equation">Equation 9-4</a>), and it can be rewritten as the log of
                  the evidence (log <em>p</em>(<strong>X</strong>)) minus the <em>evidence lower bound</em> (ELBO).
                  Since the log of the evidence does not depend on <em>q</em>, it is a constant term, so minimizing the
                  KL divergence just requires maximizing the ELBO.</p>
                <div data-type="equation" id="variational_kl_divergence_equation">
                  <h5><span class="label">Equation 9-4. </span>KL divergence from <em>q</em>(<strong>z</strong>) to
                    <em>p</em>(<strong>z</strong>|<strong>X</strong>)</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mtable displaystyle="true">
                      <mtr>
                        <mtd columnalign="right">
                          <mrow>
                            <mrow>
                              <msub>
                                <mi>D</mi>
                                <mrow>
                                  <mi>K</mi>
                                  <mi>L</mi>
                                </mrow>
                              </msub>
                              <mrow>
                                <mo>(</mo>
                                <mi>q</mi>
                                <mo>∥</mo>
                                <mi>p</mi>
                                <mo>)</mo>
                              </mrow>
                            </mrow>
                            <mo>=</mo>
                          </mrow>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="0.166667em" />
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mstyle scriptlevel="0" displaystyle="true">
                                <mfrac>
                                  <mrow>
                                    <mi>q</mi>
                                    <mo>(</mo>
                                    <mi mathvariant="bold">z</mi>
                                    <mo>)</mo>
                                  </mrow>
                                  <mrow>
                                    <mi>p</mi>
                                    <mo>(</mo>
                                    <mi mathvariant="bold">z</mi>
                                    <mspace width="0.166667em" />
                                    <mo>|</mo>
                                    <mspace width="0.166667em" />
                                    <mi mathvariant="bold">X</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </mfrac>
                              </mstyle>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mo>=</mo>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="0.166667em" />
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mi>q</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">z</mi>
                              <mo>)</mo>
                              <mo>-</mo>
                              <mo form="prefix">log</mo>
                              <mi>p</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">z</mi>
                              <mspace width="0.166667em" />
                              <mo>|</mo>
                              <mspace width="0.166667em" />
                              <mi mathvariant="bold">X</mi>
                              <mo>)</mo>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mo>=</mo>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="0.166667em" />
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mi>q</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi mathvariant="bold">z</mi>
                                <mo>)</mo>
                              </mrow>
                              <mo>-</mo>
                              <mo form="prefix">log</mo>
                              <mstyle scriptlevel="0" displaystyle="true">
                                <mfrac>
                                  <mrow>
                                    <mi>p</mi>
                                    <mo>(</mo>
                                    <mi mathvariant="bold">z</mi>
                                    <mo>,</mo>
                                    <mi mathvariant="bold">X</mi>
                                    <mo>)</mo>
                                  </mrow>
                                  <mrow>
                                    <mi>p</mi>
                                    <mo>(</mo>
                                    <mi mathvariant="bold">X</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </mfrac>
                              </mstyle>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mo>=</mo>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="0.166667em" />
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mi>q</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">z</mi>
                              <mo>)</mo>
                              <mo>-</mo>
                              <mo form="prefix">log</mo>
                              <mi>p</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">z</mi>
                              <mo>,</mo>
                              <mi mathvariant="bold">X</mi>
                              <mo>)</mo>
                              <mo>+</mo>
                              <mo form="prefix">log</mo>
                              <mi>p</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">X</mi>
                              <mo>)</mo>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mo>=</mo>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="0.166667em" />
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mi>q</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">z</mi>
                              <mo>)</mo>
                            </mfenced>
                            <mo>-</mo>
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mi>p</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">z</mi>
                              <mo>,</mo>
                              <mi mathvariant="bold">X</mi>
                              <mo>)</mo>
                            </mfenced>
                            <mo>+</mo>
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mi>p</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">X</mi>
                              <mo>)</mo>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mo>=</mo>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="0.166667em" />
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mspace width="0.166667em" />
                              <mi>p</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">X</mi>
                              <mo>)</mo>
                            </mfenced>
                            <mo>-</mo>
                            <mfenced separators="" open="(" close=")">
                              <msub>
                                <mi>𝔼</mi>
                                <mi>q</mi>
                              </msub>
                              <mfenced separators="" open="[" close="]">
                                <mo form="prefix">log</mo>
                                <mspace width="0.166667em" />
                                <mi>p</mi>
                                <mo>(</mo>
                                <mi mathvariant="bold">z</mi>
                                <mo>,</mo>
                                <mi mathvariant="bold">X</mi>
                                <mo>)</mo>
                              </mfenced>
                              <mo>-</mo>
                              <msub>
                                <mi>𝔼</mi>
                                <mi>q</mi>
                              </msub>
                              <mfenced separators="" open="[" close="]">
                                <mo form="prefix">log</mo>
                                <mspace width="0.166667em" />
                                <mi>q</mi>
                                <mo>(</mo>
                                <mi mathvariant="bold">z</mi>
                                <mo>)</mo>
                              </mfenced>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="right">
                          <mo>=</mo>
                        </mtd>
                        <mtd columnalign="left">
                          <mrow>
                            <mspace width="0.166667em" />
                            <mo form="prefix">log</mo>
                            <mspace width="0.166667em" />
                            <mi>p</mi>
                            <mo>(</mo>
                            <mi mathvariant="bold">X</mi>
                            <mo>)</mo>
                            <mo>-</mo>
                            <mtext>ELBO</mtext>
                            <mspace width="1.em" />
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd />
                        <mtd columnalign="left">
                          <mrow>
                            <mtext>where</mtext>
                            <mspace width="4.pt" />
                            <mtext>ELBO</mtext>
                            <mo>=</mo>
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mspace width="0.166667em" />
                              <mi>p</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">z</mi>
                              <mo>,</mo>
                              <mi mathvariant="bold">X</mi>
                              <mo>)</mo>
                            </mfenced>
                            <mo>-</mo>
                            <msub>
                              <mi>𝔼</mi>
                              <mi>q</mi>
                            </msub>
                            <mfenced separators="" open="[" close="]">
                              <mo form="prefix">log</mo>
                              <mspace width="0.166667em" />
                              <mi>q</mi>
                              <mo>(</mo>
                              <mi mathvariant="bold">z</mi>
                              <mo>)</mo>
                            </mfenced>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </math>
                </div>

                <p>In practice, there are different techniques to maximize the ELBO. In <em>mean field variational
                    inference</em>, it is necessary to pick the family of distributions <em>q</em>(<strong>z</strong>;
                  <strong>λ</strong>) and the prior <em>p</em>(<em>z</em>) very carefully to ensure that the equation
                  for the ELBO simplifies to a form that can actually be computed. Unfortunately, there is no general
                  way to do this, it depends on the task and requires some mathematical skills. For example, the
                  distributions and lower bound equations used in Scikit-Learn’s <code>BayesianGaussianMixture</code>
                  class are presented in the <a href="https://homl.info/40">documentation</a>. From these equations it
                  is possible to derive update equations for the cluster parameters and assignment variables: these are
                  then used very much like in the Expectation-Maximization algorithm. In fact, the computational
                  complexity of the <code>BayesianGaussianMixture</code> class is similar to that of the
                  <code>GaussianMixture</code> class (but generally significantly slower). A simpler approach to
                  maximizing the ELBO is called <em>black box stochastic variational inference</em> (BBSVI): at each
                  iteration, a few samples are drawn from <em>q</em> and they are used to estimate the gradients of the
                  ELBO with regards to the variational parameters <strong>λ</strong>, which are then used in a gradient
                  ascent step. This approach makes it possible to use Bayesian inference with any kind of model
                  (provided it is differentiable), even deep neural networks: this is called Bayesian deep learning.</p>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>If you want to dive deeper into Bayesian statistics, check out the <a
                      href="https://homl.info/bda"><em>Bayesian Data Analysis</em> book</a> by Andrew Gelman, John
                    Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin.</p>
                </div>

                <p>Gaussian mixture models work great on clusters with ellipsoidal shapes, but if you try to fit a
                  dataset with different shapes, you may have bad surprises. For example, let’s see what happens if we
                  use a Bayesian Gaussian mixture model to cluster the moons dataset (see <a data-type="xref"
                    href="#moons_vs_bgm_diagram">Figure 9-24</a>):</p>

                <figure class="smallerseventy">
                  <div id="moons_vs_bgm_diagram" class="figure">
                    <img src="mlst_0924.png" alt="mlst 0924" width="2596" height="844" />
                    <h6><span class="label">Figure 9-24. </span>moons_vs_bgm_diagram</h6>
                  </div>
                </figure>

                <p>Oops, the algorithm desperately searched for ellipsoids, so it found 8 different clusters instead of
                  2. The density estimation is not too bad, so this model could perhaps be used for anomaly detection,
                  but it failed to identify the two moons. Let’s now look at a few clustering algorithms capable of
                  dealing with arbitrarily shaped clusters.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Other Anomaly Detection and Novelty Detection Algorithms">
              <div class="sect2" id="idm139656369236848">
                <h2>Other Anomaly Detection and Novelty Detection Algorithms</h2>

                <p>Scikit-Learn also implements a few algorithms dedicated to anomaly detection or novelty detection:
                </p>

                <ul>
                  <li>
                    <p><em>Fast-MCD</em> (minimum covariance determinant), implemented by the
                      <code>EllipticEnvelope</code> class: this algorithm is useful for outlier detection, in particular
                      to cleanup a dataset. It assumes that the normal instances (inliers) are generated from a single
                      Gaussian distribution (not a mixture), but it also assumes that the dataset is contaminated with
                      outliers that were not generated from this Gaussian distribution. When it estimates the parameters
                      of the Gaussian distribution (i.e., the shape of the elliptic envelope around the inliers), it is
                      careful to ignore the instances that are most likely outliers. This gives a better estimation of
                      the elliptic envelope, and thus makes it better at identifying the outliers.</p>
                  </li>
                  <li>
                    <p><em>Isolation forest</em>: this is an efficient algorithm for outlier detection, especially in
                      high-dimensional datasets. The algorithm builds a Random Forest in which each Decision Tree is
                      grown randomly: at each node, it picks a feature randomly, then it picks a random threshold value
                      (between the min and max value) to split the dataset in two. The dataset gradually gets chopped
                      into pieces this way, until all instances end up isolated from the other instances. An anomaly is
                      usually far from other instances, so on average (across all the Decision Trees) it tends to get
                      isolated in less steps than normal instances.</p>
                  </li>
                  <li>
                    <p><em>Local outlier factor</em> (LOF): this algorithm is also good for outlier detection. It
                      compares the density of instances around a given instance to the density around its neighbors. An
                      anomaly is often more isolated than its <em>k</em> nearest neighbors.</p>
                  </li>
                  <li>
                    <p><em>One-class SVM</em>: this algorithm is better suited for novelty detection. Recall that a
                      kernelized SVM classifier separates two classes by first (implicitly) mapping all the instances to
                      a high-dimensional space, then separating the two classes using a linear SVM classifier within
                      this high-dimensional space (see <a data-type="xref" href="ch05.xhtml#svm_chapter">Chapter 5</a>).
                      Since we just have one class of instances, the one-class SVM algorithm instead tries to separate
                      the instances in high-dimensional space from the origin. In the original space, this will
                      correspond to finding a small region that encompasses all the instances. If a new instance does
                      not fall within this region, it is an anomaly. There are a few hyperparameters to tweak: the usual
                      ones for a kernelized SVM, plus a margin hyperparameter that corresponds to the probability of a
                      new instance being mistakenly considered as novel, when it is in fact normal. It works great,
                      especially with high-dimensional datasets, but just like all SVMs, it does not scale to large
                      datasets.</p>
                  </li>
                </ul>
              </div>
            </section>





          </div>
        </section>







        <div data-type="footnotes">
          <p data-type="footnote" id="idm139656371837632"><sup><a
                href="ch09.xhtml#idm139656371837632-marker">1</a></sup> “Least square quantization in PCM,” Stuart P.
            Lloyd. (1982).</p>
          <p data-type="footnote" id="idm139656371448480"><sup><a
                href="ch09.xhtml#idm139656371448480-marker">2</a></sup> “k-means\++: The advantages of careful seeding,”
            David Arthur and Sergei Vassilvitskii (2006).</p>
          <p data-type="footnote" id="idm139656371500976"><sup><a
                href="ch09.xhtml#idm139656371500976-marker">3</a></sup> “Using the Triangle Inequality to Accelerate
            k-Means,” Charles Elkan (2003).</p>
          <p data-type="footnote" id="idm139656371500048"><sup><a
                href="ch09.xhtml#idm139656371500048-marker">4</a></sup> The triangle inequality is AC ≤ AB + BC where A,
            B and C are three points, and AB, AC and BC are the distances between these points.</p>
          <p data-type="footnote" id="idm139656371496800"><sup><a
                href="ch09.xhtml#idm139656371496800-marker">5</a></sup> “Web-Scale K-Means Clustering,” David Sculley
            (2010).</p>
          <p data-type="footnote" id="idm139656369752080"><sup><a
                href="ch09.xhtml#idm139656369752080-marker">6</a></sup> Phi (ϕ or φ) is the 21<sup>st</sup> letter of
            the Greek alphabet.</p>
          <p data-type="footnote" id="idm139656369861744"><sup><a
                href="ch09.xhtml#idm139656369861744-marker">7</a></sup> Most of these notations are standard, but a few
            additional notations were taken from the Wikipedia article on <a
              href="https://en.wikipedia.org/wiki/Plate_notation">plate notation</a>.</p>
        </div>
      </div>
    </section>
  </div>



</body>

</html>