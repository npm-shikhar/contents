<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd"
  xmlns:epub="http://www.idpf.org/2007/ops">

<head>
  <link href="Style00.css" rel="stylesheet" type="text/css" />
  <link href="Style01.css" rel="stylesheet" type="text/css" />
  <link href="Style02.css" rel="stylesheet" type="text/css" />
  <link href="Style03.css" rel="stylesheet" type="text/css" />
  <style type="text/css" title="ibis-book">
    @charset "utf-8";

    #sbo-rt-content html,
    #sbo-rt-content div,
    #sbo-rt-content div,
    #sbo-rt-content span,
    #sbo-rt-content applet,
    #sbo-rt-content object,
    #sbo-rt-content iframe,
    #sbo-rt-content h1,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5,
    #sbo-rt-content h6,
    #sbo-rt-content p,
    #sbo-rt-content blockquote,
    #sbo-rt-content pre,
    #sbo-rt-content a,
    #sbo-rt-content abbr,
    #sbo-rt-content acronym,
    #sbo-rt-content address,
    #sbo-rt-content big,
    #sbo-rt-content cite,
    #sbo-rt-content code,
    #sbo-rt-content del,
    #sbo-rt-content dfn,
    #sbo-rt-content em,
    #sbo-rt-content img,
    #sbo-rt-content ins,
    #sbo-rt-content kbd,
    #sbo-rt-content q,
    #sbo-rt-content s,
    #sbo-rt-content samp,
    #sbo-rt-content small,
    #sbo-rt-content strike,
    #sbo-rt-content strong,
    #sbo-rt-content sub,
    #sbo-rt-content sup,
    #sbo-rt-content tt,
    #sbo-rt-content var,
    #sbo-rt-content b,
    #sbo-rt-content u,
    #sbo-rt-content i,
    #sbo-rt-content center,
    #sbo-rt-content dl,
    #sbo-rt-content dt,
    #sbo-rt-content dd,
    #sbo-rt-content ol,
    #sbo-rt-content ul,
    #sbo-rt-content li,
    #sbo-rt-content fieldset,
    #sbo-rt-content form,
    #sbo-rt-content label,
    #sbo-rt-content legend,
    #sbo-rt-content table,
    #sbo-rt-content caption,
    #sbo-rt-content tdiv,
    #sbo-rt-content tfoot,
    #sbo-rt-content thead,
    #sbo-rt-content tr,
    #sbo-rt-content th,
    #sbo-rt-content td,
    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content canvas,
    #sbo-rt-content details,
    #sbo-rt-content embed,
    #sbo-rt-content figure,
    #sbo-rt-content figcaption,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content output,
    #sbo-rt-content ruby,
    #sbo-rt-content section,
    #sbo-rt-content summary,
    #sbo-rt-content time,
    #sbo-rt-content mark,
    #sbo-rt-content audio,
    #sbo-rt-content video {
      margin: 0;
      padding: 0;
      border: 0;
      font-size: 100%;
      font: inherit;
      vertical-align: baseline
    }

    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content details,
    #sbo-rt-content figcaption,
    #sbo-rt-content figure,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content section {
      display: block
    }

    #sbo-rt-content div {
      line-height: 1
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style: none
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content q {
      quotes: none
    }

    #sbo-rt-content blockquote:before,
    #sbo-rt-content blockquote:after,
    #sbo-rt-content q:before,
    #sbo-rt-content q:after {
      content: none
    }

    #sbo-rt-content table {
      border-collapse: collapse;
      border-spacing: 0
    }

    @page {
      margin: 5px !important
    }

    #sbo-rt-content p {
      margin: 10px 0 0;
      line-height: 125%;
      text-align: left
    }

    #sbo-rt-content p.byline {
      text-align: left;
      margin: -33px auto 35px;
      font-style: italic;
      font-weight: bold
    }

    #sbo-rt-content div.preface p+p.byline {
      margin: 1em 0 0 !important
    }

    #sbo-rt-content div.preface p.byline+p.byline {
      margin: 0 !important
    }

    #sbo-rt-content div.sect1&gt;

    p.byline {
      margin: -.25em 0 1em
    }

    #sbo-rt-content div.sect1&gt;

    p.byline+p.byline {
      margin-top: -1em
    }

    #sbo-rt-content em {
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content em strong,
    #sbo-rt-content strong em {
      font-weight: bold;
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content strong,
    #sbo-rt-content span.bold {
      font-weight: bold
    }

    #sbo-rt-content em.replaceable {
      font-style: italic
    }

    #sbo-rt-content strong.userinput {
      font-weight: bold;
      font-style: normal
    }

    #sbo-rt-content span.bolditalic {
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content a.ulink,
    #sbo-rt-content a.xref,
    #sbo-rt-content a.email,
    #sbo-rt-content a.link,
    #sbo-rt-content a {
      text-decoration: none;
      color: #8e0012
    }

    #sbo-rt-content span.lineannotation {
      font-style: italic;
      color: #a62a2a;
      font-family: serif
    }

    #sbo-rt-content span.underline {
      text-decoration: underline
    }

    #sbo-rt-content span.strikethrough {
      text-decoration: line-through
    }

    #sbo-rt-content span.smallcaps {
      font-variant: small-caps
    }

    #sbo-rt-content span.cursor {
      background: #000;
      color: #fff
    }

    #sbo-rt-content span.smaller {
      font-size: 75%
    }

    #sbo-rt-content .boxedtext,
    #sbo-rt-content .keycap {
      border-style: solid;
      border-width: 1px;
      border-color: #000;
      padding: 1px
    }

    #sbo-rt-content span.gray50 {
      color: #7F7F7F;
    }

    #sbo-rt-content h1,
    #sbo-rt-content div.toc-title,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5 {
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      font-weight: bold;
      text-align: left;
      page-break-after: avoid !important;
      font-family: sans-serif, "DejaVuSans"
    }

    #sbo-rt-content div.toc-title {
      font-size: 1.5em;
      margin-top: 20px !important;
      margin-bottom: 30px !important
    }

    #sbo-rt-content section[data-type="sect1"] h1 {
      font-size: 1.3em;
      color: #8e0012;
      margin: 40px 0 8px 0
    }

    #sbo-rt-content section[data-type="sect2"] h2 {
      font-size: 1.1em;
      margin: 30px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect3"] h3 {
      font-size: 1em;
      color: #555;
      margin: 20px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect4"] h4 {
      font-size: 1em;
      font-weight: normal;
      font-style: italic;
      margin: 15px 0 6px 0 !important
    }

    #sbo-rt-content section[data-type="chapter"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="preface"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="appendix"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="glossary"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="bibliography"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="index"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      line-height: 1;
      margin-bottom: 50px;
      color: #000;
      padding-bottom: 10px;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content span.label,
    #sbo-rt-content span.keep-together {
      font-size: inherit;
      font-weight: inherit
    }

    #sbo-rt-content div[data-type="part"] h1 {
      font-size: 2em;
      text-align: center;
      margin-top: 0 !important;
      margin-bottom: 50px;
      padding: 50px 0 10px 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content img.width-ninety {
      width: 90%
    }

    #sbo-rt-content img {
      max-width: 95%;
      margin: 0 auto;
      padding: 0
    }

    #sbo-rt-content div.figure {
      background-color: transparent;
      text-align: center !important;
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content figure {
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.figure h6,
    #sbo-rt-content figure h6,
    #sbo-rt-content figure figcaption {
      font-size: .9rem !important;
      text-align: center;
      font-weight: normal !important;
      font-style: italic;
      font-family: serif !important;
      text-transform: none !important;
      letter-spacing: normal !important;
      color: #000 !important;
      padding-top: 10px !important;
      page-break-before: avoid
    }

    #sbo-rt-content div.informalfigure {
      text-align: center !important;
      padding: 5px 0 !important
    }

    #sbo-rt-content div.sidebar {
      margin: 15px 0 10px 0 !important;
      border: 1px solid #DCDCDC;
      background-color: #F7F7F7;
      padding: 15px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content aside[data-type="sidebar"] {
      margin: 15px 0 10px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar-title,
    #sbo-rt-content aside[data-type="sidebar"] h5 {
      font-weight: bold;
      font-size: 1em;
      font-family: sans-serif;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar ol,
    #sbo-rt-content div.sidebar ul,
    #sbo-rt-content aside[data-type="sidebar"] ol,
    #sbo-rt-content aside[data-type="sidebar"] ul {
      margin-left: 1.25em !important
    }

    #sbo-rt-content div.sidebar div.figure p.title,
    #sbo-rt-content aside[data-type="sidebar"] figcaption,
    #sbo-rt-content div.sidebar div.informalfigure div.caption {
      font-size: 90%;
      text-align: center;
      font-weight: normal;
      font-style: italic;
      font-family: serif !important;
      color: #000;
      padding: 5px !important;
      page-break-before: avoid;
      page-break-after: avoid
    }

    #sbo-rt-content div.sidebar div.tip,
    #sbo-rt-content div.sidebar div[data-type="tip"],
    #sbo-rt-content div.sidebar div.note,
    #sbo-rt-content div.sidebar div[data-type="note"],
    #sbo-rt-content div.sidebar div.warning,
    #sbo-rt-content div.sidebar div[data-type="warning"],
    #sbo-rt-content div.sidebar div[data-type="caution"],
    #sbo-rt-content div.sidebar div[data-type="important"] {
      margin: 20px auto 20px auto !important;
      font-size: 90%;
      width: 85%
    }

    #sbo-rt-content aside[data-type="sidebar"] p.byline {
      font-size: 90%;
      font-weight: bold;
      font-style: italic;
      text-align: center;
      text-indent: 0;
      margin: 5px auto 6px;
      page-break-after: avoid
    }

    #sbo-rt-content pre {
      white-space: pre-wrap;
      font-family: "Ubuntu Mono", monospace;
      margin: 25px 0 25px 20px;
      font-size: 85%;
      display: block;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content div.note pre.programlisting,
    #sbo-rt-content div.tip pre.programlisting,
    #sbo-rt-content div.warning pre.programlisting,
    #sbo-rt-content div.caution pre.programlisting,
    #sbo-rt-content div.important pre.programlisting {
      margin-bottom: 0
    }

    #sbo-rt-content code {
      font-family: "Ubuntu Mono", monospace;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content code strong em,
    #sbo-rt-content code em strong,
    #sbo-rt-content pre em strong,
    #sbo-rt-content pre strong em,
    #sbo-rt-content strong code em code,
    #sbo-rt-content em code strong code,
    #sbo-rt-content span.bolditalic code {
      font-weight: bold;
      font-style: italic;
      font-family: "Ubuntu Mono BoldItal", monospace
    }

    #sbo-rt-content code em,
    #sbo-rt-content em code,
    #sbo-rt-content pre em,
    #sbo-rt-content em.replaceable {
      font-family: "Ubuntu Mono Ital", monospace;
      font-style: italic
    }

    #sbo-rt-content code strong,
    #sbo-rt-content strong code,
    #sbo-rt-content pre strong,
    #sbo-rt-content strong.userinput {
      font-family: "Ubuntu Mono Bold", monospace;
      font-weight: bold
    }

    #sbo-rt-content div[data-type="example"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div[data-type="example"] h1,
    #sbo-rt-content div[data-type="example"] h2,
    #sbo-rt-content div[data-type="example"] h3,
    #sbo-rt-content div[data-type="example"] h4,
    #sbo-rt-content div[data-type="example"] h5,
    #sbo-rt-content div[data-type="example"] h6 {
      font-style: italic;
      font-weight: normal;
      text-align: left !important;
      text-transform: none !important;
      font-family: serif !important;
      margin: 10px 0 5px 0 !important;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content li pre.example {
      padding: 10px 0 !important
    }

    #sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],
    #sbo-rt-content div[data-type="example"] pre[data-type="screen"] {
      margin: 0
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      margin: 50px 0 10px 0 !important;
      line-height: 1;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] h2,
    #sbo-rt-content section[data-type="titlepage"] p.subtitle,
    #sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"] {
      font-size: 1.3em;
      font-weight: normal;
      text-align: center;
      margin-top: .5em;
      color: #555
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h2[data-type="author"],
    #sbo-rt-content section[data-type="titlepage"] p.author {
      font-size: 1.3em;
      font-family: serif !important;
      font-weight: bold;
      margin: 50px 0 !important;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] p.edition {
      text-align: center;
      text-transform: uppercase;
      margin-top: 2em
    }

    #sbo-rt-content section[data-type="titlepage"] {
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"]:after {
      content: url(css_assets/titlepage_footer_ebook.png);
      margin: 0 auto;
      max-width: 80%
    }

    #sbo-rt-content div.book div.titlepage div.publishername {
      margin-top: 60%;
      margin-bottom: 20px;
      text-align: center;
      font-size: 1.25em
    }

    #sbo-rt-content div.book div.titlepage div.locations p {
      margin: 0;
      text-align: center
    }

    #sbo-rt-content div.book div.titlepage div.locations p.cities {
      font-size: 80%;
      text-align: center;
      margin-top: 5px
    }

    #sbo-rt-content section.preface[title="Dedication"]&gt;

    div.titlepage h2.title {
      text-align: center;
      text-transform: uppercase;
      font-size: 1.5em;
      margin-top: 50px;
      margin-bottom: 50px
    }

    #sbo-rt-content ul.stafflist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.stafflist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content ul.printings li {
      list-style-type: none
    }

    #sbo-rt-content section.preface[title="Dedication"] p {
      font-style: italic;
      text-align: center
    }

    #sbo-rt-content div.colophon h1.title {
      font-size: 1.3em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon h2.subtitle {
      margin: 0 !important;
      color: #000;
      font-family: serif !important;
      font-size: 1em;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.author h3.author {
      font-size: 1.1em;
      font-family: serif !important;
      margin: 10px 0 0 !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h4,
    #sbo-rt-content div.colophon div.editor h3.editor {
      color: #000;
      font-size: .8em;
      margin: 15px 0 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h3.editor {
      font-size: .8em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.publisher {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon div.publisher p,
    #sbo-rt-content div.colophon div.publisher span.publishername {
      margin: 0;
      font-size: .8em
    }

    #sbo-rt-content div.legalnotice p,
    #sbo-rt-content div.timestamp p {
      font-size: .8em
    }

    #sbo-rt-content div.timestamp p {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon[title="About the Author"] h1.title,
    #sbo-rt-content div.colophon[title="Colophon"] h1.title {
      font-size: 1.5em;
      margin: 0 !important;
      font-family: sans-serif !important
    }

    #sbo-rt-content section.chapter div.titlepage div.author {
      margin: 10px 0 10px 0
    }

    #sbo-rt-content section.chapter div.titlepage div.author div.affiliation {
      font-style: italic
    }

    #sbo-rt-content div.attribution {
      margin: 5px 0 0 50px !important
    }

    #sbo-rt-content h3.author span.orgname {
      display: none
    }

    #sbo-rt-content div.epigraph {
      margin: 10px 0 10px 20px !important;
      page-break-inside: avoid;
      font-size: 90%
    }

    #sbo-rt-content div.epigraph p {
      font-style: italic
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content div.blockquote {
      margin: 10px !important;
      page-break-inside: avoid;
      font-size: 95%
    }

    #sbo-rt-content blockquote p,
    #sbo-rt-content div.blockquote p {
      font-style: italic;
      margin: .75em 0 0 !important
    }

    #sbo-rt-content blockquote div.attribution,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      margin: 5px 0 10px 30px !important;
      text-align: right;
      width: 80%
    }

    #sbo-rt-content blockquote div.attribution p,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      font-style: normal;
      margin-top: 5px
    }

    #sbo-rt-content blockquote div.attribution p:before,
    #sbo-rt-content blockquote p[data-type="attribution"]:before {
      font-style: normal;
      content: "—";
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none
    }

    #sbo-rt-content p.right {
      text-align: right;
      margin: 0
    }

    #sbo-rt-content div[data-type="footnotes"] {
      border-top: 1px solid black;
      margin-top: 2em
    }

    #sbo-rt-content sub,
    #sbo-rt-content sup {
      font-size: 75%;
      line-height: 0;
      position: relative
    }

    #sbo-rt-content sup {
      top: -.5em
    }

    #sbo-rt-content sub {
      bottom: -.25em
    }

    #sbo-rt-content p[data-type="footnote"] {
      font-size: 90% !important;
      line-height: 1.2em !important;
      margin-left: 2.5em !important;
      text-indent: -2.3em !important
    }

    #sbo-rt-content p[data-type="footnote"] sup {
      display: inline-block !important;
      position: static !important;
      width: 2em !important;
      text-align: right !important;
      font-size: 100% !important;
      padding-right: .5em !important
    }

    #sbo-rt-content p[data-type="footnote"] a[href$="-marker"] {
      font-family: sans-serif !important;
      font-size: 90% !important;
      color: #8e0012 !important
    }

    #sbo-rt-content a[data-type="noteref"] {
      font-family: sans-serif !important;
      color: #8e0012;
      margin-left: 0;
      padding-left: 0
    }

    #sbo-rt-content div.refentry p.refname {
      font-size: 1em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold;
      margin-bottom: 5px;
      overflow: auto;
      width: 100%
    }

    #sbo-rt-content div.refentry {
      width: 100%;
      display: block;
      margin-top: 2em
    }

    #sbo-rt-content div.refsynopsisdiv {
      display: block;
      clear: both
    }

    #sbo-rt-content div.refentry header {
      page-break-inside: avoid !important;
      display: block;
      break-inside: avoid !important;
      padding-top: 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content div.refsect1 h6 {
      font-size: .9em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold
    }

    #sbo-rt-content div.refsect1 {
      margin-top: 3em
    }

    #sbo-rt-content dt {
      padding-top: 10px !important;
      padding-bottom: 0 !important
    }

    #sbo-rt-content dd {
      margin-left: 1.5em !important;
      margin-bottom: .25em
    }

    #sbo-rt-content dd ol,
    #sbo-rt-content dd ul {
      padding-left: 1em
    }

    #sbo-rt-content dd li {
      margin-top: 0;
      margin-bottom: 0
    }

    #sbo-rt-content dd,
    #sbo-rt-content li {
      text-align: left
    }

    #sbo-rt-content ul,
    #sbo-rt-content ul&gt;
    li,
    #sbo-rt-content ol ul,
    #sbo-rt-content ol ul&gt;
    li,
    #sbo-rt-content ul ol ul,
    #sbo-rt-content ul ol ul&gt;

    li {
      list-style-type: disc
    }

    #sbo-rt-content ul ul,
    #sbo-rt-content ul ul&gt;

    li {
      list-style-type: square
    }

    #sbo-rt-content ul ul ul,
    #sbo-rt-content ul ul ul&gt;

    li {
      list-style-type: circle
    }

    #sbo-rt-content ol,
    #sbo-rt-content ol&gt;
    li,
    #sbo-rt-content ol ul ol,
    #sbo-rt-content ol ul ol&gt;
    li,
    #sbo-rt-content ul ol,
    #sbo-rt-content ul ol&gt;

    li {
      list-style-type: decimal
    }

    #sbo-rt-content ol ol,
    #sbo-rt-content ol ol&gt;

    li {
      list-style-type: lower-alpha
    }

    #sbo-rt-content ol ol ol,
    #sbo-rt-content ol ol ol&gt;

    li {
      list-style-type: lower-roman
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style-position: outside;
      margin: 15px 0 15px 1.25em;
      padding-left: 2.25em
    }

    #sbo-rt-content ol li,
    #sbo-rt-content ul li {
      margin: .5em 0 .65em;
      line-height: 125%
    }

    #sbo-rt-content div.orderedlistalpha {
      list-style-type: upper-alpha
    }

    #sbo-rt-content table.simplelist,
    #sbo-rt-content ul.simplelist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.simplelist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content table.simplelist td {
      border: none
    }

    #sbo-rt-content table.simplelist tr {
      border-bottom: none
    }

    #sbo-rt-content table.simplelist tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content dl.calloutlist p:first-child {
      margin-top: -25px !important
    }

    #sbo-rt-content dl.calloutlist dd {
      padding-left: 0;
      margin-top: -25px
    }

    #sbo-rt-content dl.calloutlist img,
    #sbo-rt-content a.co img {
      padding: 0
    }

    #sbo-rt-content div.toc ol {
      margin-top: 8px !important;
      margin-bottom: 8px !important;
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol ol {
      margin-left: 30px !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol li {
      list-style-type: none
    }

    #sbo-rt-content div.toc a {
      color: #8e0012
    }

    #sbo-rt-content div.toc ol a {
      font-size: 1em;
      font-weight: bold
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;

    ol a {
      font-weight: bold;
      font-size: 1em
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;
    ol&gt;
    li&gt;

    ol a {
      text-decoration: none;
      font-weight: normal;
      font-size: 1em
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"],
    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      margin: 30px !important;
      font-size: 90%;
      padding: 10px 8px 20px 8px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.tip ol,
    #sbo-rt-content div.tip ul,
    #sbo-rt-content div[data-type="tip"] ol,
    #sbo-rt-content div[data-type="tip"] ul,
    #sbo-rt-content div.note ol,
    #sbo-rt-content div.note ul,
    #sbo-rt-content div[data-type="note"] ol,
    #sbo-rt-content div[data-type="note"] ul,
    #sbo-rt-content div.warning ol,
    #sbo-rt-content div.warning ul,
    #sbo-rt-content div[data-type="warning"] ol,
    #sbo-rt-content div[data-type="warning"] ul,
    #sbo-rt-content div[data-type="caution"] ol,
    #sbo-rt-content div[data-type="caution"] ul,
    #sbo-rt-content div[data-type="important"] ol,
    #sbo-rt-content div[data-type="important"] ul {
      margin-left: 1.5em !important
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"] {
      border: 1px solid #BEBEBE;
      background-color: transparent
    }

    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      border: 1px solid #BC8F8F
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="note"] h1,
    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1,
    #sbo-rt-content div[data-type="important"] h6 {
      font-weight: bold;
      font-size: 110%;
      font-family: sans-serif !important;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px !important
    }

    #sbo-rt-content div[data-type="tip"] figure h6,
    #sbo-rt-content div[data-type="note"] figure h6,
    #sbo-rt-content div[data-type="warning"] figure h6,
    #sbo-rt-content div[data-type="caution"] figure h6,
    #sbo-rt-content div[data-type="important"] figure h6 {
      font-family: serif !important
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div[data-type="note"] h1 {
      color: #737373
    }

    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="important"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1 {
      color: #C67171
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,
    #sbo-rt-content div.safarienabled {
      background-color: transparent;
      margin: 8px 0 0 !important;
      border: 0 solid #BEBEBE;
      font-size: 100%;
      padding: 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,
    #sbo-rt-content div.safarienabled h6 {
      display: none
    }

    #sbo-rt-content div.table,
    #sbo-rt-content table {
      margin: 15px 0 30px 0 !important;
      max-width: 95%;
      border: none !important;
      background: none;
      display: table !important
    }

    #sbo-rt-content div.table,
    #sbo-rt-content div.informaltable,
    #sbo-rt-content table {
      page-break-inside: avoid
    }

    #sbo-rt-content tr,
    #sbo-rt-content tr td {
      border-bottom: 1px solid #c3c3c3
    }

    #sbo-rt-content thead td,
    #sbo-rt-content thead th {
      border-bottom: #9d9d9d 1px solid !important;
      border-top: #9d9d9d 1px solid !important
    }

    #sbo-rt-content tr:nth-of-type(even) {
      background-color: #f1f6fc
    }

    #sbo-rt-content thead {
      font-family: sans-serif;
      font-weight: bold
    }

    #sbo-rt-content td,
    #sbo-rt-content th {
      display: table-cell;
      padding: .3em;
      text-align: left;
      vertical-align: middle;
      font-size: 80%
    }

    #sbo-rt-content div.informaltable table {
      margin: 10px auto !important
    }

    #sbo-rt-content div.informaltable table tr {
      border-bottom: none
    }

    #sbo-rt-content div.informaltable table tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content div.informaltable td,
    #sbo-rt-content div.informaltable th {
      border: #9d9d9d 1px solid
    }

    #sbo-rt-content div.table-title,
    #sbo-rt-content table caption {
      font-weight: normal;
      font-style: italic;
      font-family: serif;
      font-size: 1em;
      margin: 10px 0 10px 0 !important;
      padding: 0;
      page-break-after: avoid;
      text-align: left !important
    }

    #sbo-rt-content table code {
      font-size: smaller
    }

    #sbo-rt-content table.border tbody&gt;
    tr:last-child&gt;

    td {
      border-bottom: transparent
    }

    #sbo-rt-content div.equation,
    #sbo-rt-content div[data-type="equation"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div.equation-title,
    #sbo-rt-content div[data-type="equation"] h5 {
      font-style: italic;
      font-weight: normal;
      font-family: serif !important;
      font-size: 90%;
      margin: 20px 0 10px 0 !important;
      page-break-after: avoid
    }

    #sbo-rt-content div.equation-contents {
      margin-left: 20px
    }

    #sbo-rt-content div[data-type="equation"] math {
      font-size: calc(.35em + 1vw)
    }

    #sbo-rt-content span.inlinemediaobject {
      height: .85em;
      display: inline-block;
      margin-bottom: .2em
    }

    #sbo-rt-content span.inlinemediaobject img {
      margin: 0;
      height: .85em
    }

    #sbo-rt-content div.informalequation {
      margin: 20px 0 20px 20px;
      width: 75%
    }

    #sbo-rt-content div.informalequation img {
      width: 75%
    }

    #sbo-rt-content div.index {
      text-indent: 0
    }

    #sbo-rt-content div.index h3 {
      padding: .25em;
      margin-top: 1em !important;
      background-color: #F0F0F0
    }

    #sbo-rt-content div.index li {
      line-height: 130%;
      list-style-type: none
    }

    #sbo-rt-content div.index a.indexterm {
      color: #8e0012 !important
    }

    #sbo-rt-content div.index ul {
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.index ul ul {
      margin-left: 1em !important;
      margin-top: 0 !important
    }

    #sbo-rt-content code.boolean,
    #sbo-rt-content .navy {
      color: rgb(0, 0, 128);
    }

    #sbo-rt-content code.character,
    #sbo-rt-content .olive {
      color: rgb(128, 128, 0);
    }

    #sbo-rt-content code.comment,
    #sbo-rt-content .blue {
      color: rgb(0, 0, 255);
    }

    #sbo-rt-content code.conditional,
    #sbo-rt-content .limegreen {
      color: rgb(50, 205, 50);
    }

    #sbo-rt-content code.constant,
    #sbo-rt-content .darkorange {
      color: rgb(255, 140, 0);
    }

    #sbo-rt-content code.debug,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.define,
    #sbo-rt-content .darkgoldenrod,
    #sbo-rt-content .gold {
      color: rgb(184, 134, 11);
    }

    #sbo-rt-content code.delimiter,
    #sbo-rt-content .dimgray {
      color: rgb(105, 105, 105);
    }

    #sbo-rt-content code.error,
    #sbo-rt-content .red {
      color: rgb(255, 0, 0);
    }

    #sbo-rt-content code.exception,
    #sbo-rt-content .salmon {
      color: rgb(250, 128, 11);
    }

    #sbo-rt-content code.float,
    #sbo-rt-content .steelblue {
      color: rgb(70, 130, 180);
    }

    #sbo-rt-content pre code.function,
    #sbo-rt-content .green {
      color: rgb(0, 128, 0);
    }

    #sbo-rt-content code.identifier,
    #sbo-rt-content .royalblue {
      color: rgb(65, 105, 225);
    }

    #sbo-rt-content code.ignore,
    #sbo-rt-content .gray {
      color: rgb(128, 128, 128);
    }

    #sbo-rt-content code.include,
    #sbo-rt-content .purple {
      color: rgb(128, 0, 128);
    }

    #sbo-rt-content code.keyword,
    #sbo-rt-content .sienna {
      color: rgb(160, 82, 45);
    }

    #sbo-rt-content code.label,
    #sbo-rt-content .deeppink {
      color: rgb(255, 20, 147);
    }

    #sbo-rt-content code.macro,
    #sbo-rt-content .orangered {
      color: rgb(255, 69, 0);
    }

    #sbo-rt-content code.number,
    #sbo-rt-content .brown {
      color: rgb(165, 42, 42);
    }

    #sbo-rt-content code.operator,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.preCondit,
    #sbo-rt-content .teal {
      color: rgb(0, 128, 128);
    }

    #sbo-rt-content code.preProc,
    #sbo-rt-content .fuschia {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.repeat,
    #sbo-rt-content .indigo {
      color: rgb(75, 0, 130);
    }

    #sbo-rt-content code.special,
    #sbo-rt-content .saddlebrown {
      color: rgb(139, 69, 19);
    }

    #sbo-rt-content code.specialchar,
    #sbo-rt-content .magenta {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.specialcomment,
    #sbo-rt-content .seagreen {
      color: rgb(46, 139, 87);
    }

    #sbo-rt-content code.statement,
    #sbo-rt-content .forestgreen {
      color: rgb(34, 139, 34);
    }

    #sbo-rt-content code.storageclass,
    #sbo-rt-content .plum {
      color: rgb(221, 160, 221);
    }

    #sbo-rt-content code.string,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.structure,
    #sbo-rt-content .chocolate {
      color: rgb(210, 106, 30);
    }

    #sbo-rt-content code.tag,
    #sbo-rt-content .darkcyan {
      color: rgb(0, 139, 139);
    }

    #sbo-rt-content code.todo,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.type,
    #sbo-rt-content .mediumslateblue {
      color: rgb(123, 104, 238);
    }

    #sbo-rt-content code.typedef,
    #sbo-rt-content .darkgreen {
      color: rgb(0, 100, 0);
    }

    #sbo-rt-content code.underlined {
      text-decoration: underline;
    }

    #sbo-rt-content pre code.hll {
      background-color: #ffc
    }

    #sbo-rt-content pre code.c {
      color: #09F;
      font-style: italic
    }

    #sbo-rt-content pre code.err {
      color: #A00
    }

    #sbo-rt-content pre code.k {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.o {
      color: #555
    }

    #sbo-rt-content pre code.cm {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cp {
      color: #099
    }

    #sbo-rt-content pre code.c1 {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cs {
      color: #35586C;
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content pre code.gd {
      background-color: #FCC
    }

    #sbo-rt-content pre code.ge {
      font-style: italic
    }

    #sbo-rt-content pre code.gr {
      color: #F00
    }

    #sbo-rt-content pre code.gh {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gi {
      background-color: #CFC
    }

    #sbo-rt-content pre code.go {
      color: #000
    }

    #sbo-rt-content pre code.gp {
      color: #009;
      font-weight: bold
    }

    #sbo-rt-content pre code.gs {
      font-weight: bold
    }

    #sbo-rt-content pre code.gu {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gt {
      color: #9C6
    }

    #sbo-rt-content pre code.kc {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kd {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kn {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kp {
      color: #069
    }

    #sbo-rt-content pre code.kr {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kt {
      color: #078;
      font-weight: bold
    }

    #sbo-rt-content pre code.m {
      color: #F60
    }

    #sbo-rt-content pre code.s {
      color: #C30
    }

    #sbo-rt-content pre code.na {
      color: #309
    }

    #sbo-rt-content pre code.nb {
      color: #366
    }

    #sbo-rt-content pre code.nc {
      color: #0A8;
      font-weight: bold
    }

    #sbo-rt-content pre code.no {
      color: #360
    }

    #sbo-rt-content pre code.nd {
      color: #99F
    }

    #sbo-rt-content pre code.ni {
      color: #999;
      font-weight: bold
    }

    #sbo-rt-content pre code.ne {
      color: #C00;
      font-weight: bold
    }

    #sbo-rt-content pre code.nf {
      color: #C0F
    }

    #sbo-rt-content pre code.nl {
      color: #99F
    }

    #sbo-rt-content pre code.nn {
      color: #0CF;
      font-weight: bold
    }

    #sbo-rt-content pre code.nt {
      color: #309;
      font-weight: bold
    }

    #sbo-rt-content pre code.nv {
      color: #033
    }

    #sbo-rt-content pre code.ow {
      color: #000;
      font-weight: bold
    }

    #sbo-rt-content pre code.w {
      color: #bbb
    }

    #sbo-rt-content pre code.mf {
      color: #F60
    }

    #sbo-rt-content pre code.mh {
      color: #F60
    }

    #sbo-rt-content pre code.mi {
      color: #F60
    }

    #sbo-rt-content pre code.mo {
      color: #F60
    }

    #sbo-rt-content pre code.sb {
      color: #C30
    }

    #sbo-rt-content pre code.sc {
      color: #C30
    }

    #sbo-rt-content pre code.sd {
      color: #C30;
      font-style: italic
    }

    #sbo-rt-content pre code.s2 {
      color: #C30
    }

    #sbo-rt-content pre code.se {
      color: #C30;
      font-weight: bold
    }

    #sbo-rt-content pre code.sh {
      color: #C30
    }

    #sbo-rt-content pre code.si {
      color: #A00
    }

    #sbo-rt-content pre code.sx {
      color: #C30
    }

    #sbo-rt-content pre code.sr {
      color: #3AA
    }

    #sbo-rt-content pre code.s1 {
      color: #C30
    }

    #sbo-rt-content pre code.ss {
      color: #A60
    }

    #sbo-rt-content pre code.bp {
      color: #366
    }

    #sbo-rt-content pre code.vc {
      color: #033
    }

    #sbo-rt-content pre code.vg {
      color: #033
    }

    #sbo-rt-content pre code.vi {
      color: #033
    }

    #sbo-rt-content pre code.il {
      color: #F60
    }

    #sbo-rt-content pre code.g {
      color: #050
    }

    #sbo-rt-content pre code.l {
      color: #C60
    }

    #sbo-rt-content pre code.l {
      color: #F90
    }

    #sbo-rt-content pre code.n {
      color: #008
    }

    #sbo-rt-content pre code.nx {
      color: #008
    }

    #sbo-rt-content pre code.py {
      color: #96F
    }

    #sbo-rt-content pre code.p {
      color: #000
    }

    #sbo-rt-content pre code.x {
      color: #F06
    }

    #sbo-rt-content div.blockquote_sampler_toc {
      width: 95%;
      margin: 5px 5px 5px 10px !important
    }

    #sbo-rt-content div {
      font-family: serif;
      text-align: left
    }

    #sbo-rt-content .gray-background,
    #sbo-rt-content .reverse-video {
      background: #2E2E2E;
      color: #FFF
    }

    #sbo-rt-content .light-gray-background {
      background: #A0A0A0
    }

    #sbo-rt-content .preserve-whitespace {
      white-space: pre-wrap
    }

    #sbo-rt-content span.gray {
      color: #4C4C4C
    }

    #sbo-rt-content .width-10 {
      width: 10vw !important
    }

    #sbo-rt-content .width-20 {
      width: 20vw !important
    }

    #sbo-rt-content .width-30 {
      width: 30vw !important
    }

    #sbo-rt-content .width-40 {
      width: 40vw !important
    }

    #sbo-rt-content .width-50 {
      width: 50vw !important
    }

    #sbo-rt-content .width-60 {
      width: 60vw !important
    }

    #sbo-rt-content .width-70 {
      width: 70vw !important
    }

    #sbo-rt-content .width-80 {
      width: 80vw !important
    }

    #sbo-rt-content .width-90 {
      width: 90vw !important
    }

    #sbo-rt-content .width-full,
    #sbo-rt-content .width-100 {
      width: 100vw !important
    }

    #sbo-rt-content div[data-type="equation"].fifty-percent img {
      width: 50%
    }
  </style>
  <style type="text/css" id="font-styles">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-size: &lt;
      %=font_size %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="font-family">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-family: &lt;
      %=font_family %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="column-width">
    #sbo-rt-content {
      max-width: &lt;
      %=column_width %&gt;
      % !important;
      margin: 0 auto !important;
    }
  </style>

  <style type="text/css">
    body {
      background-color: #fbfbfb !important;
      margin: 1em;
    }

    #sbo-rt-content * {
      text-indent: 0pt !important;
    }

    #sbo-rt-content .bq {
      margin-right: 1em !important;
    }

    #sbo-rt-content * {
      word-wrap: break-word !important;
      word-break: break-word !important;
    }

    #sbo-rt-content table,
    #sbo-rt-content pre {
      overflow-x: unset !important;
      overflow: unset !important;
      overflow-y: unset !important;
      white-space: pre-wrap !important;
    }
  </style>
</head>

<body>
  <div id="sbo-rt-content">
    <section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 6. Decision Trees">
      <div class="chapter" id="trees_chapter">
        <h1><span class="label">Chapter 6. </span>Decision Trees</h1>


        <p>Like <a data-type="indexterm" data-primary="Decision Trees" id="dt6" />SVMs, <em>Decision Trees</em> <a
            data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.tree.DecisionTreeRegressor"
            id="idm139656375603168" />are versatile Machine Learning algorithms that can perform both classification and
          regression tasks, and even multioutput tasks. They are very powerful algorithms, capable of fitting complex
          datasets. For example, in <a data-type="xref" href="ch02.xhtml#project_chapter">Chapter 2</a> you trained a
          <code>DecisionTreeRegressor</code> model on the California housing dataset, fitting it perfectly (actually
          overfitting it).
        </p>

        <p>Decision Trees are also the fundamental components of <a data-type="indexterm" data-primary="Random Forests"
            id="idm139656375600176" />Random Forests (see <a data-type="xref"
            href="ch07.xhtml#ensembles_chapter">Chapter 7</a>), which are among the most powerful Machine Learning
          algorithms available today.</p>

        <p>In this chapter we will start by discussing how to train, visualize, and make predictions with Decision
          Trees. Then we will go through the CART training algorithm used by Scikit-Learn, and we will discuss how to
          regularize trees and use them for regression tasks. Finally, we will discuss some of the limitations of
          Decision Trees.</p>






        <section data-type="sect1" data-pdf-bookmark="Training and Visualizing a Decision Tree">
          <div class="sect1" id="idm139656375597616">
            <h1>Training and Visualizing a Decision Tree</h1>

            <p>To <a data-type="indexterm" data-primary="Decision Trees" data-secondary="training and visualizing"
                id="dt6tav" />understand Decision Trees, let’s just build one and take a look at how it makes
              predictions. The following code trains a <code>DecisionTreeClassifier</code> on the <a
                data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.datasets.load_iris()"
                id="idm139656375594176" />iris dataset (see <a data-type="xref"
                href="ch04.xhtml#linear_models_chapter">Chapter 4</a>):</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.datasets</code> <code class="kn">import</code> <code class="n">load_iris</code>
<code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeClassifier</code>

<code class="n">iris</code> <code class="o">=</code> <code class="n">load_iris</code><code class="p">()</code>
<code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">data</code><code class="p">[:,</code> <code class="mi">2</code><code class="p">:]</code> <code class="c1"># petal length and width</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">iris</code><code class="o">.</code><code class="n">target</code>

<code class="n">tree_clf</code> <code class="o">=</code> <code class="n">DecisionTreeClassifier</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">tree_clf</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

            <p>You can visualize the trained Decision Tree by first using the <code>export_graphviz()</code> <a
                data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.tree.export_graphviz()"
                id="idm139656375545264" />method <a data-type="indexterm" data-primary="graphviz"
                id="idm139656375544224" />to output a graph definition file called <em>iris_tree.dot</em>:</p>

            <pre data-type="programlisting" data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">export_graphviz</code>

<code class="n">export_graphviz</code><code class="p">(</code>
        <code class="n">tree_clf</code><code class="p">,</code>
        <code class="n">out_file</code><code class="o">=</code><code class="n">image_path</code><code class="p">(</code><code class="s2">"iris_tree.dot"</code><code class="p">),</code>
        <code class="n">feature_names</code><code class="o">=</code><code class="n">iris</code><code class="o">.</code><code class="n">feature_names</code><code class="p">[</code><code class="mi">2</code><code class="p">:],</code>
        <code class="n">class_names</code><code class="o">=</code><code class="n">iris</code><code class="o">.</code><code class="n">target_names</code><code class="p">,</code>
        <code class="n">rounded</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
        <code class="n">filled</code><code class="o">=</code><code class="bp">True</code>
    <code class="p">)</code></pre>

            <p>Then you can convert this <em>.dot</em> file to a variety of formats such as PDF or PNG using the
              <code>dot</code> command-line tool from the <em>graphviz</em> package.<sup><a data-type="noteref"
                  id="idm139656375394320-marker" href="ch06.xhtml#idm139656375394320">1</a></sup> This command line
              converts the <em>.dot</em> file to a <em>.png</em> image file:
            </p>

            <pre data-type="programlisting"
              data-code-language="shell-session"><code class="go">$ dot -Tpng iris_tree.dot -o iris_tree.png</code></pre>

            <p>Your first decision tree looks like <a data-type="xref" href="#iris_tree">Figure 6-1</a>.</p>

            <figure class="smallereighty">
              <div id="iris_tree" class="figure">
                <img src="mlst_0601.png" alt="mlst 0601" width="472" height="410" />
                <h6><span class="label">Figure 6-1. </span>Iris Decision Tree</h6>
              </div>
            </figure>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Making Predictions">
          <div class="sect1" id="idm139656375389408">
            <h1>Making Predictions</h1>

            <p>Let’s <a data-type="indexterm" data-primary="Decision Trees" data-secondary="training and visualizing"
                data-startref="dt6tav" id="idm139656375387424" /><a data-type="indexterm" data-primary="Decision Trees"
                data-secondary="predictions" id="dt6p" /><a data-type="indexterm" data-primary="predictions"
                id="p6wdt" />see how the tree represented in <a data-type="xref" href="#iris_tree">Figure 6-1</a> makes
              predictions. Suppose you find an iris flower and you want to classify it. You start at the <em>root
                node</em> (depth 0, at the top): this node asks whether the flower’s petal length is smaller than 2.45
              cm. If it is, then you move down to the root’s left child node (depth 1, left). In this case, it is a
              <em>leaf node</em> (i.e., it does not have any children nodes), so it does not ask any questions: you can
              simply look at the predicted class for that node and the Decision Tree predicts that your flower is an
              Iris-Setosa (<code>class=setosa</code>).
            </p>

            <p>Now suppose you find another flower, but this time the petal length is greater than 2.45 cm. You must
              move down to the root’s right child node (depth 1, right), which is not a leaf node, so it asks another
              question: is the petal width smaller than 1.75 cm? If it is, then your flower is most likely an
              Iris-Versicolor (depth 2, left). If not, it is likely an Iris-Virginica (depth 2, right). It’s really that
              simple.</p>
            <div data-type="note" epub:type="note">
              <h6>Note</h6>
              <p>One of the many qualities of Decision Trees is that they require very little data preparation. In
                particular, they don’t require feature scaling or centering at all.</p>
            </div>

            <p>A node’s <code>samples</code> attribute counts how many training instances it applies to. For example,
              100 training instances have a petal length greater than 2.45 cm (depth 1, right), among which 54 have a
              petal width smaller than 1.75 cm (depth 2, left). A node’s <code>value</code> attribute tells you how many
              training instances of each class this node applies to: for example, the bottom-right node applies to 0
              Iris-Setosa, 1 Iris-Versicolor, and 45 Iris-Virginica. Finally, a node’s <code>gini</code> attribute
              measures its <em>impurity</em>: a node is “pure” (<code>gini=0</code>) if all training instances it
              applies to belong to the same class. For example, since the depth-1 left node applies only to Iris-Setosa
              training instances, it is pure and its <code>gini</code> score is 0. <a data-type="xref"
                href="#gini_impurity">Equation 6-1</a> <a data-type="indexterm" data-primary="GINI impurity"
                id="idm139656375379840" /><a data-type="indexterm" data-primary="impurity measures"
                id="idm139656375379136" />shows how the training algorithm computes the gini score
              <em>G</em><sub><em>i</em></sub> of the i<sup>th</sup> node. For example, the depth-2 left node has a
              <code>gini</code> score equal to 1 – (0/54)<sup>2</sup> – (49/54)<sup>2</sup> – (5/54)<sup>2</sup> ≈
              0.168. Another <em>impurity measure</em> is discussed shortly.
            </p>
            <div data-type="equation" id="gini_impurity">
              <h5><span class="label">Equation 6-1. </span>Gini impurity</h5>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow>
                  <msub>
                    <mi>G</mi>
                    <mi>i</mi>
                  </msub>
                  <mo>=</mo>
                  <mn>1</mn>
                  <mo>-</mo>
                  <munderover>
                    <mo>∑</mo>
                    <mrow>
                      <mi>k</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mi>n</mi>
                  </munderover>
                  <msup>
                    <mrow>
                      <msub>
                        <mi>p</mi>
                        <mrow>
                          <mi>i</mi>
                          <mo>,</mo>
                          <mi>k</mi>
                        </mrow>
                      </msub>
                    </mrow>
                    <mn>2</mn>
                  </msup>
                </mrow>
              </math>
            </div>

            <ul>
              <li>
                <p><em>p</em><sub><em>i</em>,<em>k</em></sub> is the ratio of class <em>k</em> instances among the
                  training instances in the <em>i</em><sup>th</sup> node.</p>
              </li>
            </ul>
            <div data-type="note" epub:type="note">
              <h6>Note</h6>
              <p>Scikit-Learn <a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="CART algorithm"
                  id="slcartalgch6" /><a data-type="indexterm"
                  data-primary="CART (Classification and Regression Tree) algorithm" id="cartcartch6" />uses the <a
                  data-type="indexterm" data-primary="Decision Trees" data-secondary="numbers of children"
                  id="idm139656375508944" />CART algorithm, which produces <a data-type="indexterm"
                  data-primary="Decision Trees" data-secondary="binary trees" id="idm139656375507840" />only <em>binary
                  trees</em>: nonleaf nodes always have two children (i.e., questions only have yes/no answers).
                However, other algorithms such as ID3 can produce Decision Trees with nodes that have more than two
                children.</p>
            </div>

            <p><a data-type="xref" href="#decision_tree_decision_boundaries_plot">Figure 6-2</a> shows this <a
                data-type="indexterm" data-primary="Decision Trees" data-secondary="decision boundaries"
                id="idm139656375504880" /><a data-type="indexterm" data-primary="decision boundaries"
                id="idm139656375503904" />Decision Tree’s decision boundaries. The thick vertical line represents the
              decision boundary of the root node (depth 0): petal length = 2.45 cm. Since the left area is pure (only
              Iris-Setosa), it cannot be split any further. However, the right area is impure, so the depth-1 right node
              splits it at petal width = 1.75 cm (represented by the dashed line). Since <code>max_depth</code> was set
              to 2, the Decision Tree stops right there. However, if you set <code>max_depth</code> to 3, then the two
              depth-2 nodes would each add another decision boundary (represented by the dotted lines).</p>

            <figure class="smallerseventyfive">
              <div id="decision_tree_decision_boundaries_plot" class="figure">
                <img src="mlst_0602.png" alt="mlst 0602" width="2298" height="1101" />
                <h6><span class="label">Figure 6-2. </span>Decision Tree decision boundaries</h6>
              </div>
            </figure>
            <aside data-type="sidebar" epub:type="sidebar">
              <div class="sidebar" id="idm139656375499408">
                <h5>Model Interpretation: White Box Versus Black Box</h5>
                <p>As you can see Decision Trees are fairly intuitive and their decisions are easy to interpret. Such
                  models are often <a data-type="indexterm" data-primary="white box models"
                    id="idm139656375497984" />called <em>white box models</em>. In contrast, as we will see, Random
                  Forests or neural networks are generally <a data-type="indexterm" data-primary="black box models"
                    id="idm139656375496736" />considered <em>black box models</em>. They make great predictions, and you
                  can easily check the calculations that they performed to make these predictions; nevertheless, it is
                  usually hard to explain in simple terms why the predictions were made. For example, if a neural
                  network says that a particular person appears on a picture, it is hard to know what actually
                  contributed to this prediction: did the model recognize that person’s eyes? Her mouth? Her nose? Her
                  shoes? Or even the couch that she was sitting on? Conversely, Decision Trees provide nice and simple
                  classification rules that can even be applied manually if need be (e.g., for flower classification).
                </p>
              </div>
            </aside>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Estimating Class Probabilities">
          <div class="sect1" id="idm139656375388752">
            <h1>Estimating Class Probabilities</h1>

            <p>A <a data-type="indexterm" data-primary="Decision Trees" data-secondary="predictions"
                data-startref="dt6p" id="idm139656375493440" /><a data-type="indexterm" data-primary="predictions"
                data-startref="p6wdt" id="idm139656375492160" /><a data-type="indexterm" data-primary="Decision Trees"
                data-secondary="class probability estimates" id="idm139656375491216" /><a data-type="indexterm"
                data-primary="probabilities, estimating" id="idm139656375490208" />Decision Tree can also estimate the
              probability that an instance belongs to a particular class <em>k</em>: first it traverses the tree to find
              the leaf node for this instance, and then it returns the ratio of training instances of class <em>k</em>
              in this node. For example, suppose you have found a flower whose petals are 5 cm long and 1.5 cm wide. The
              corresponding leaf node is the depth-2 left node, so the Decision Tree should output the following
              probabilities: 0% for Iris-Setosa (0/54), 90.7% for Iris-Versicolor (49/54), and 9.3% for Iris-Virginica
              (5/54). And of course if you ask it to predict the class, it should output Iris-Versicolor (class 1) since
              it has the highest probability. Let’s check this:</p>

            <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">tree_clf</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">([[</code><code class="mi">5</code><code class="p">,</code> <code class="mf">1.5</code><code class="p">]])</code>
<code class="go">array([[0.        , 0.90740741, 0.09259259]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">tree_clf</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mi">5</code><code class="p">,</code> <code class="mf">1.5</code><code class="p">]])</code>
<code class="go">array([1])</code></pre>

            <p>Perfect! Notice that the estimated probabilities would be identical anywhere else in the bottom-right
              rectangle of <a data-type="xref" href="#decision_tree_decision_boundaries_plot">Figure 6-2</a>—for
              example, if the petals were 6 cm long and 1.5 cm wide (even though it seems obvious that it would most
              likely be an Iris-Virginica in this case).</p>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="The CART Training Algorithm">
          <div class="sect1" id="idm139656375476496">
            <h1>The CART Training Algorithm</h1>

            <p>Scikit-Learn <a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="CART algorithm"
                data-startref="slcartalgch6" id="idm139656375475088" /><a data-type="indexterm"
                data-primary="CART (Classification and Regression Tree) algorithm" data-startref="cartcartch6"
                id="idm139656375473488" />uses the <em>Classification And Regression Tree</em> (CART) algorithm to train
              Decision Trees (also called “growing” trees). The idea is really quite simple: the algorithm first splits
              the training set in two subsets using a single feature <em>k</em> and a threshold
              <em>t</em><sub><em>k</em></sub> (e.g., “petal length ≤ 2.45 cm”). How does it choose <em>k</em> and
              <em>t</em><sub><em>k</em></sub>? It searches for the pair (<em>k</em>, <em>t</em><sub><em>k</em></sub>)
              that produces the purest subsets (weighted by their size). The cost function that the algorithm tries to
              minimize is given by <a data-type="xref" href="#classification_cart_cost_function">Equation 6-2</a>.
            </p>
            <div data-type="equation" id="classification_cart_cost_function">
              <h5><span class="label">Equation 6-2. </span>CART cost function for classification</h5>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtable displaystyle="true">
                  <mtr>
                    <mtd />
                    <mtd columnalign="left">
                      <mrow>
                        <mi>J</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>k</mi>
                          <mo>,</mo>
                          <msub>
                            <mi>t</mi>
                            <mi>k</mi>
                          </msub>
                          <mo>)</mo>
                        </mrow>
                        <mo>=</mo>
                        <mstyle scriptlevel="0" displaystyle="true">
                          <mfrac>
                            <msub>
                              <mi>m</mi>
                              <mtext>left</mtext>
                            </msub>
                            <mi>m</mi>
                          </mfrac>
                        </mstyle>
                        <msub>
                          <mi>G</mi>
                          <mtext>left</mtext>
                        </msub>
                        <mo>+</mo>
                        <mstyle scriptlevel="0" displaystyle="true">
                          <mfrac>
                            <msub>
                              <mi>m</mi>
                              <mtext>right</mtext>
                            </msub>
                            <mi>m</mi>
                          </mfrac>
                        </mstyle>
                        <msub>
                          <mi>G</mi>
                          <mtext>right</mtext>
                        </msub>
                      </mrow>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd />
                    <mtd columnalign="left">
                      <mrow>
                        <mtext>where</mtext>
                        <mspace width="4.pt" />
                        <mfenced separators="" open="{" close="">
                          <mtable>
                            <mtr>
                              <mtd columnalign="left">
                                <mrow>
                                  <msub>
                                    <mi>G</mi>
                                    <mtext>left/right</mtext>
                                  </msub>
                                  <mspace width="4.pt" />
                                  <mtext>measures</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>the</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>impurity</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>of</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>the</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>left/right</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>subset,</mtext>
                                </mrow>
                              </mtd>
                            </mtr>
                            <mtr>
                              <mtd columnalign="left">
                                <mrow>
                                  <msub>
                                    <mi>m</mi>
                                    <mtext>left/right</mtext>
                                  </msub>
                                  <mspace width="4.pt" />
                                  <mtext>is</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>the</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>number</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>of</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>instances</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>in</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>the</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>left/right</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>subset.</mtext>
                                </mrow>
                              </mtd>
                            </mtr>
                          </mtable>
                        </mfenced>
                      </mrow>
                    </mtd>
                  </mtr>
                </mtable>
              </math>
            </div>

            <p>Once it has successfully split the training set in two, it splits the subsets using the same logic, then
              the sub-subsets and so on, recursively. It stops recursing once it reaches the maximum depth (defined by
              the <code>max_depth</code> hyperparameter), or if it cannot find a split that will reduce impurity. A few
              other hyperparameters (described in a moment) control additional stopping conditions
              (<code>min_samples_split</code>, <code>min_samples_leaf</code>, <code>min_weight_fraction_leaf</code>, and
              <code>max_leaf_nodes</code>).
            </p>
            <div data-type="warning" epub:type="warning">
              <h6>Warning</h6>
              <p>As you can see, the CART algorithm is <a data-type="indexterm" data-primary="greedy algorithm"
                  id="idm139656375278640" />a <em>greedy algorithm</em>: it greedily searches for an optimum split at
                the top level, then repeats the process at each level. It does not check whether or not the split will
                lead to the lowest possible impurity several levels down. A greedy algorithm often produces a reasonably
                good solution, but it is not guaranteed to be the optimal solution.</p>

              <p>Unfortunately, finding the optimal tree is known to be <a data-type="indexterm"
                  data-primary="NP-Complete problems" id="idm139656375276416" />an <em>NP-Complete</em> problem:<sup><a
                    data-type="noteref" id="idm139656375275168-marker" href="ch06.xhtml#idm139656375275168">2</a></sup>
                it requires <em>O</em>(exp(<em>m</em>)) time, making the problem intractable even for fairly small
                training sets. This is why we must settle for a “reasonably good” solution.</p>
            </div>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Computational Complexity">
          <div class="sect1" id="idm139656375272784">
            <h1>Computational Complexity</h1>

            <p>Making <a data-type="indexterm" data-primary="computational complexity" id="idm139656375271264" /><a
                data-type="indexterm" data-primary="Decision Trees" data-secondary="computational complexity"
                id="idm139656375270512" />predictions requires traversing the Decision Tree from the root to a leaf.
              Decision Trees are generally approximately balanced, so traversing the Decision Tree requires going
              through roughly <em>O</em>(<em>log</em><sub>2</sub>(<em>m</em>)) nodes.<sup><a data-type="noteref"
                  id="idm139656375267648-marker" href="ch06.xhtml#idm139656375267648">3</a></sup> Since each node only
              requires checking the value of one feature, the overall prediction complexity is just
              <em>O</em>(<em>log</em><sub>2</sub>(<em>m</em>)), independent of the number of features. So predictions
              are very fast, even when dealing with large training sets.
            </p>

            <p>However, the training algorithm compares all features (or less if <code>max_features</code> is set) on
              all samples at each node. This results in a training complexity of <em>O</em>(<em>n</em> × <em>m</em>
              <em>log</em>(<em>m</em>)). For small training sets (less than a few thousand instances), Scikit-Learn can
              speed up training by presorting the data (set <code>presort=True</code>), but this slows down training
              considerably for larger training sets.
            </p>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Gini Impurity or Entropy?">
          <div class="sect1" id="idm139656375258064">
            <h1>Gini Impurity or Entropy?</h1>

            <p>By default, the <a data-type="indexterm" data-primary="GINI impurity" id="idm139656375256768" /><a
                data-type="indexterm" data-primary="Decision Trees" data-secondary="GINI impurity"
                id="idm139656375255648" />Gini impurity measure <a data-type="indexterm"
                data-primary="impurity measures" id="idm139656375254576" />is used, but you can select the
              <em>entropy</em> <a data-type="indexterm" data-primary="entropy impurity measure"
                id="idm139656375253360" />impurity measure instead by setting the <code>criterion</code> hyperparameter
              to <code>"entropy"</code>. The concept of entropy originated in thermodynamics as a measure of molecular
              disorder: entropy approaches zero when molecules are still and well ordered. It later spread to a wide
              variety of domains, including Shannon’s <em>information theory</em>, <a data-type="indexterm"
                data-primary="Shannon's information theory" id="idm139656375250976" /><a data-type="indexterm"
                data-primary="information theory" id="idm139656375250224" />where it measures the average information
              content of a message:<sup><a data-type="noteref" id="idm139656375249424-marker"
                  href="ch06.xhtml#idm139656375249424">4</a></sup> <a data-type="indexterm"
                data-primary="information gain" id="idm139656375248288" />entropy is zero when all messages are
              identical. In Machine Learning, it is frequently used as an impurity measure: a set’s entropy is zero when
              it contains instances of only one class. <a data-type="xref" href="#entropy_function">Equation 6-3</a>
              shows the definition of the entropy of the i<sup>th</sup> node. For example, the depth-2 left node in <a
                data-type="xref" href="#iris_tree">Figure 6-1</a> has an entropy equal to <math
                xmlns="http://www.w3.org/1998/Math/MathML"
                alttext="minus StartFraction 49 Over 54 EndFraction log Subscript 2 Baseline left-parenthesis StartFraction 49 Over 54 EndFraction right-parenthesis minus five-fifty-fourths log Subscript 2 Baseline left-parenthesis five-fifty-fourths right-parenthesis">
                <mrow>
                  <mo>-</mo>
                  <mfrac>
                    <mn>49</mn>
                    <mn>54</mn>
                  </mfrac>
                  <msub>
                    <mo form="prefix">log</mo>
                    <mn>2</mn>
                  </msub>
                  <mrow>
                    <mo>(</mo>
                    <mfrac>
                      <mn>49</mn>
                      <mn>54</mn>
                    </mfrac>
                    <mo>)</mo>
                  </mrow>
                  <mo>-</mo>
                  <mfrac>
                    <mn>5</mn>
                    <mn>54</mn>
                  </mfrac>
                  <msub>
                    <mo form="prefix">log</mo>
                    <mn>2</mn>
                  </msub>
                  <mrow>
                    <mo>(</mo>
                    <mfrac>
                      <mn>5</mn>
                      <mn>54</mn>
                    </mfrac>
                    <mo>)</mo>
                  </mrow>
                </mrow>
              </math> ≈ 0.445.
            </p>
            <div data-type="equation" id="entropy_function">
              <h5><span class="label">Equation 6-3. </span>Entropy</h5>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow>
                  <msub>
                    <mi>H</mi>
                    <mi>i</mi>
                  </msub>
                  <mo>=</mo>
                  <mo>-</mo>
                  <munderover>
                    <mo>∑</mo>
                    <mfrac linethickness="0pt">
                      <mrow>
                        <mi>k</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                      </mrow>
                      <mrow>
                        <msub>
                          <mi>p</mi>
                          <mrow>
                            <mi>i</mi>
                            <mo>,</mo>
                            <mi>k</mi>
                          </mrow>
                        </msub>
                        <mo>≠</mo>
                        <mn>0</mn>
                      </mrow>
                    </mfrac>
                    <mi>n</mi>
                  </munderover>
                  <mrow>
                    <msub>
                      <mi>p</mi>
                      <mrow>
                        <mi>i</mi>
                        <mo>,</mo>
                        <mi>k</mi>
                      </mrow>
                    </msub>
                    <msub>
                      <mo form="prefix">log</mo>
                      <mn>2</mn>
                    </msub>
                    <mrow>
                      <mo>(</mo>
                      <msub>
                        <mi>p</mi>
                        <mrow>
                          <mi>i</mi>
                          <mo>,</mo>
                          <mi>k</mi>
                        </mrow>
                      </msub>
                      <mo>)</mo>
                    </mrow>
                  </mrow>
                </mrow>
              </math>
            </div>

            <p>So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big
              difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good
              default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own
              branch of the tree, while entropy tends to produce slightly more balanced trees.<sup><a
                  data-type="noteref" id="idm139656375198448-marker" href="ch06.xhtml#idm139656375198448">5</a></sup>
            </p>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Regularization Hyperparameters">
          <div class="sect1" id="idm139656375196944">
            <h1>Regularization Hyperparameters</h1>

            <p>Decision Trees <a data-type="indexterm" data-primary="Decision Trees"
                data-secondary="regularization hyperparameters" id="dt6rh" /><a data-type="indexterm"
                data-primary="regularization" data-secondary="Decision Trees" id="r6dt" />make very few assumptions
              about the training data (as opposed to linear models, which obviously assume that the data is linear, for
              example). If left unconstrained, the tree structure will adapt itself to the training data, fitting it
              very closely, and most likely overfitting it. Such a model is often called a <em>nonparametric model</em>,
              <a data-type="indexterm" data-primary="nonparametric models" id="idm139656375191936" />not because it does
              not have any parameters (it often has a lot) but because the number of parameters is not determined prior
              to training, so the model structure is free to stick closely to the data. In contrast, <a
                data-type="indexterm" data-primary="parametric models" id="idm139656375190848" />a <em>parametric
                model</em> such as a linear model has a predetermined number of parameters, so its degree of freedom is
              limited, reducing the risk of overfitting (but increasing the risk of underfitting).
            </p>

            <p>To avoid <a data-type="indexterm" data-primary="overfitting" id="idm139656375188960" />overfitting the
              training data, you need to restrict the Decision Tree’s freedom during training. As you know by now, this
              is called regularization. The regularization hyperparameters depend on the algorithm used, but generally
              you can at least restrict the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by
              the <code>max_depth</code> hyperparameter (the default value is <code>None</code>, which means unlimited).
              Reducing <code>max_depth</code> will <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="min_ and max_ hyperparameters" id="idm139656375186528" />regularize the model and thus
              reduce the risk of overfitting.</p>

            <p>The <code>DecisionTreeClassifier</code> <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.tree.DecisionTreeClassifier" id="idm139656375184240" />class has a few other
              parameters that similarly restrict the shape of the Decision Tree: <code>min_samples_split</code> (the
              minimum number of samples a node must have before it can be split), <code>min_samples_leaf</code> (the
              minimum number of samples a leaf node must have), <code>min_weight_fraction_leaf</code> (same as
              <code>min_samples_leaf</code> but expressed as a fraction of the total number of weighted instances),
              <code>max_leaf_nodes</code> (maximum number of leaf nodes), and <code>max_features</code> (maximum number
              of features that are evaluated for splitting at each node). Increasing <code>min_*</code> hyperparameters
              or reducing <code>max_*</code> hyperparameters will regularize the model.
            </p>
            <div data-type="note" epub:type="note">
              <h6>Note</h6>
              <p>Other algorithms work by first training the Decision Tree without restrictions, <a
                  data-type="indexterm" data-primary="pruning" id="idm139656375178688" />then <em>pruning</em>
                (deleting) unnecessary nodes. A node whose children are all leaf nodes is considered unnecessary if the
                purity improvement it provides is <a data-type="indexterm" data-primary="statistical significance"
                  id="idm139656375176928" />not <em>statistically significant</em>. Standard statistical tests, such as
                the <em>χ</em><sup>2</sup> <em>test</em>, <a data-type="indexterm" data-primary="χ 2 test"
                  data-see="chi square test" id="idm139656375174432" /><a data-type="indexterm"
                  data-primary="chi square test" id="idm139656375173424" />are used to estimate the probability that the
                improvement is purely the result of chance (which is called <a data-type="indexterm"
                  data-primary="hypothesis, null" id="idm139656375172496" />the <em>null hypothesis</em>). If this
                probability, called the <em>p-value</em>, <a data-type="indexterm" data-primary="p-value"
                  id="idm139656375170816" /><a data-type="indexterm" data-primary="null hypothesis"
                  id="idm139656375170080" />is higher than a given threshold (typically 5%, controlled by a
                hyperparameter), then the node is considered unnecessary and its children are deleted. The pruning
                continues until all unnecessary nodes have been pruned.</p>
            </div>

            <p><a data-type="xref" href="#min_samples_leaf_plot">Figure 6-3</a> shows two Decision Trees trained on the
              moons dataset (introduced in <a data-type="xref" href="ch05.xhtml#svm_chapter">Chapter 5</a>). On the
              left, the Decision Tree is trained with the default hyperparameters (i.e., no restrictions), and on the
              right the Decision Tree is trained with <code>min_samples_leaf=4</code>. It is quite obvious that the
              model on the left is overfitting, and the model on the right will probably generalize <a
                data-type="indexterm" data-primary="Decision Trees" data-secondary="regularization hyperparameters"
                data-startref="dt6rh" id="idm139656375166576" /><a data-type="indexterm" data-primary="regularization"
                data-secondary="Decision Trees" data-startref="r6dt" id="idm139656375165264" />better.</p>

            <figure>
              <div id="min_samples_leaf_plot" class="figure">
                <img src="mlst_0603.png" alt="mlst 0603" width="3228" height="1099" />
                <h6><span class="label">Figure 6-3. </span>Regularization using min_samples_leaf</h6>
              </div>
            </figure>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Regression">
          <div class="sect1" id="idm139656375161824">
            <h1>Regression</h1>

            <p>Decision Trees <a data-type="indexterm" data-primary="Decision Trees" data-secondary="regression tasks"
                id="dt6rt" /><a data-type="indexterm" data-primary="regression" data-secondary="Decision Trees"
                id="regr6dt" />are also capable of performing regression tasks. Let’s build a regression tree using <a
                data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.tree.DecisionTreeRegressor"
                id="idm139656375157536" />Scikit-Learn’s <code>DecisionTreeRegressor</code> class, training it on a
              noisy quadratic dataset with <code>max_depth=2</code>:</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.tree</code> <code class="kn">import</code> <code class="n">DecisionTreeRegressor</code>

<code class="n">tree_reg</code> <code class="o">=</code> <code class="n">DecisionTreeRegressor</code><code class="p">(</code><code class="n">max_depth</code><code class="o">=</code><code class="mi">2</code><code class="p">)</code>
<code class="n">tree_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

            <p>The resulting tree is represented on <a data-type="xref" href="#regression_tree">Figure 6-4</a>.</p>

            <figure>
              <div id="regression_tree" class="figure">
                <img src="mlst_0604.png" alt="mlst 0604" width="624" height="350" />
                <h6><span class="label">Figure 6-4. </span>A Decision Tree for regression</h6>
              </div>
            </figure>

            <p>This tree looks very similar to the classification tree you built earlier. The main difference is that
              instead of predicting a class in each node, it predicts a value. For example, suppose you want to make a
              prediction for a new instance with <em>x</em><sub>1</sub> = 0.6. You traverse the tree starting at the
              root, and you eventually reach the leaf node that predicts <code>value=0.1106</code>. This prediction is
              simply the average target value of the 110 training instances associated to this leaf node. This
              prediction results in a Mean Squared Error (MSE) equal to 0.0151 over these 110 instances.</p>

            <p>This model’s predictions are represented on the left of <a data-type="xref"
                href="#tree_regression_plot">Figure 6-5</a>. If you set <code>max_depth=3</code>, you get the
              predictions represented on the right. Notice how the predicted value for each region is always the average
              target value of the instances in that region. The algorithm splits each region in a way that makes most
              training instances as close as possible to that predicted value.</p>

            <figure>
              <div id="tree_regression_plot" class="figure">
                <img src="mlst_0605.png" alt="mlst 0605" width="3204" height="1096" />
                <h6><span class="label">Figure 6-5. </span>Predictions of two Decision Tree regression models</h6>
              </div>
            </figure>

            <p>The <a data-type="indexterm" data-primary="CART (Classification and Regression Tree) algorithm"
                id="idm139656375128528" /><a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="CART algorithm" id="idm139656375127824" />CART algorithm works mostly the same way as
              earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now
              tries to split the training set in a way that minimizes the MSE. <a data-type="xref"
                href="#regression_cart_cost_function">Equation 6-4</a> shows the cost function that the algorithm tries
              to minimize.</p>
            <div data-type="equation" id="regression_cart_cost_function">
              <h5><span class="label">Equation 6-4. </span>CART cost function for regression</h5>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow>
                  <mi>J</mi>
                  <mrow>
                    <mo>(</mo>
                    <mi>k</mi>
                    <mo>,</mo>
                    <msub>
                      <mi>t</mi>
                      <mi>k</mi>
                    </msub>
                    <mo>)</mo>
                  </mrow>
                  <mo>=</mo>
                  <mstyle scriptlevel="0" displaystyle="true">
                    <mfrac>
                      <msub>
                        <mi>m</mi>
                        <mtext>left</mtext>
                      </msub>
                      <mi>m</mi>
                    </mfrac>
                  </mstyle>
                  <msub>
                    <mtext>MSE</mtext>
                    <mtext>left</mtext>
                  </msub>
                  <mo>+</mo>
                  <mstyle scriptlevel="0" displaystyle="true">
                    <mfrac>
                      <msub>
                        <mi>m</mi>
                        <mtext>right</mtext>
                      </msub>
                      <mi>m</mi>
                    </mfrac>
                  </mstyle>
                  <msub>
                    <mtext>MSE</mtext>
                    <mtext>right</mtext>
                  </msub>
                  <mspace width="1.em" />
                  <mtext>where</mtext>
                  <mspace width="4.pt" />
                  <mfenced separators="" open="{" close="">
                    <mtable>
                      <mtr>
                        <mtd columnalign="left">
                          <mrow>
                            <msub>
                              <mtext>MSE</mtext>
                              <mtext>node</mtext>
                            </msub>
                            <mo>=</mo>
                            <munder>
                              <mo>∑</mo>
                              <mstyle scriptlevel="1" displaystyle="false">
                                <mrow>
                                  <mi>i</mi>
                                  <mo>∈</mo>
                                  <mtext>node</mtext>
                                </mrow>
                              </mstyle>
                            </munder>
                            <msup>
                              <mrow>
                                <mo>(</mo>
                                <msub>
                                  <mover accent="true">
                                    <mi>y</mi>
                                    <mo>^</mo>
                                  </mover>
                                  <mtext>node</mtext>
                                </msub>
                                <mo>-</mo>
                                <msup>
                                  <mi>y</mi>
                                  <mrow>
                                    <mo>(</mo>
                                    <mi>i</mi>
                                    <mo>)</mo>
                                  </mrow>
                                </msup>
                                <mo>)</mo>
                              </mrow>
                              <mn>2</mn>
                            </msup>
                          </mrow>
                        </mtd>
                      </mtr>
                      <mtr>
                        <mtd columnalign="left">
                          <mrow>
                            <msub>
                              <mover accent="true">
                                <mi>y</mi>
                                <mo>^</mo>
                              </mover>
                              <mtext>node</mtext>
                            </msub>
                            <mo>=</mo>
                            <mstyle scriptlevel="0" displaystyle="true">
                              <mfrac>
                                <mn>1</mn>
                                <msub>
                                  <mi>m</mi>
                                  <mtext>node</mtext>
                                </msub>
                              </mfrac>
                            </mstyle>
                            <munder>
                              <mo>∑</mo>
                              <mstyle scriptlevel="1" displaystyle="false">
                                <mrow>
                                  <mi>i</mi>
                                  <mo>∈</mo>
                                  <mtext>node</mtext>
                                </mrow>
                              </mstyle>
                            </munder>
                            <msup>
                              <mi>y</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                          </mrow>
                        </mtd>
                      </mtr>
                    </mtable>
                  </mfenced>
                </mrow>
              </math>
            </div>

            <p>Just like for classification tasks, Decision Trees are prone to overfitting <a data-type="indexterm"
                data-primary="overfitting" id="idm139656375083392" />when dealing with regression tasks. Without any
              regularization (i.e., using the default hyperparameters), you get the predictions on the left of <a
                data-type="xref" href="#tree_regression_regularization_plot">Figure 6-6</a>. It is obviously overfitting
              the training set very badly. Just setting <code>min_samples_leaf=10</code> results in a much more
              reasonable model, represented on the <a data-type="indexterm" data-primary="Decision Trees"
                data-secondary="regression tasks" data-startref="dt6rt" id="idm139656375081248" /><a
                data-type="indexterm" data-primary="regression" data-secondary="Decision Trees" data-startref="regr6dt"
                id="idm139656375080000" />right of <a data-type="xref"
                href="#tree_regression_regularization_plot">Figure 6-6</a>.</p>

            <figure>
              <div id="tree_regression_regularization_plot" class="figure">
                <img src="mlst_0606.png" alt="mlst 0606" width="3204" height="1096" />
                <h6><span class="label">Figure 6-6. </span>Regularizing a Decision Tree regressor</h6>
              </div>
            </figure>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Instability">
          <div class="sect1" id="idm139656375161232">
            <h1>Instability</h1>

            <p>Hopefully <a data-type="indexterm" data-primary="Decision Trees" data-secondary="instability with"
                id="dt6iw" />by now you are convinced that Decision Trees have a lot going for them: they are simple to
              understand and interpret, easy to use, versatile, and powerful. However they do have a few limitations.
              First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are
              perpendicular to an axis), which makes them sensitive to training set rotation. For example, <a
                data-type="xref" href="#sensitivity_to_rotation_plot">Figure 6-7</a> shows a simple linearly separable
              dataset: on the left, a Decision Tree can split it easily, while on the right, after the dataset is
              rotated by 45°, the decision boundary looks unnecessarily convoluted. Although both Decision Trees fit the
              training set perfectly, it is very likely that the model on the right will not generalize well. One way to
              limit this problem is to use PCA (see <a data-type="xref"
                href="ch08.xhtml#dimensionality_chapter">Chapter 8</a>), which often results in a better orientation of
              the training data.</p>

            <figure>
              <div id="sensitivity_to_rotation_plot" class="figure">
                <img src="mlst_0607.png" alt="mlst 0607" width="3196" height="1084" />
                <h6><span class="label">Figure 6-7. </span>Sensitivity to training set rotation</h6>
              </div>
            </figure>

            <p>More generally, the main issue with Decision Trees is that they are very sensitive to small variations in
              the training data. For example, if you just remove the widest Iris-Versicolor from the iris training set
              (the one with petals 4.8 cm long and 1.8 cm wide) and train a new Decision Tree, you may get the model
              represented in <a data-type="xref" href="#decision_tree_instability_plot">Figure 6-8</a>. As you can see,
              it looks very different from the previous Decision Tree (<a data-type="xref"
                href="#decision_tree_decision_boundaries_plot">Figure 6-2</a>). Actually, since the training algorithm
              used by Scikit-Learn is stochastic<sup><a data-type="noteref" id="idm139656375066032-marker"
                  href="ch06.xhtml#idm139656375066032">6</a></sup> you may <span class="keep-together">get very</span>
              different models even on the same training data (unless you set the <span
                class="keep-together"><code>random_state</code></span> hyperparameter).</p>

            <figure>
              <div id="decision_tree_instability_plot" class="figure">
                <img src="mlst_0608.png" alt="mlst 0608" width="2298" height="1101" />
                <h6><span class="label">Figure 6-8. </span>Sensitivity to training set details</h6>
              </div>
            </figure>

            <p>Random Forests <a data-type="indexterm" data-primary="Random Forests" id="idm139656375061536" />can limit
              this instability by averaging predictions over many trees, as we will see in the next <a
                data-type="indexterm" data-primary="Decision Trees" data-secondary="instability with"
                data-startref="dt6iw" id="idm139656375060560" />chapter.</p>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Exercises">
          <div class="sect1" id="idm139656375059088">
            <h1>Exercises</h1>
            <ol>
              <li>
                <p>What is the approximate depth of a Decision Tree trained (without restrictions) on a training set
                  with 1 million instances?</p>
              </li>
              <li>
                <p>Is a node’s Gini impurity generally lower or greater than its parent’s? Is it <em>generally</em>
                  lower/greater, or <em>always</em> lower/greater?</p>
              </li>
              <li>
                <p>If a Decision Tree is overfitting the training set, is it a good idea to try decreasing
                  <code>max_depth</code>?
                </p>
              </li>
              <li>
                <p>If a Decision Tree is underfitting the training set, is it a good idea to try scaling the input
                  features?</p>
              </li>
              <li>
                <p>If it takes one hour to train a Decision Tree on a training set containing 1 million instances,
                  roughly how much time will it take to train another Decision Tree on a training set containing 10
                  million instances?</p>
              </li>
              <li>
                <p>If your training set contains 100,000 instances, will setting <code>presort=True</code> speed up
                  training?</p>
              </li>
              <li>
                <p>Train and fine-tune a Decision Tree for the moons dataset.</p>
                <ol>
                  <li>
                    <p>Generate a moons dataset <a data-type="indexterm" data-primary="Scikit-Learn"
                        data-secondary="sklearn.datasets.make_moons()" id="idm139656375048208" />using
                      <code>make_moons(n_samples=10000, noise=0.4)</code>.
                    </p>
                  </li>
                  <li>
                    <p>Split it into a training set and a <a data-type="indexterm" data-primary="Scikit-Learn"
                        data-secondary="sklearn.model_selection.train_test_split()" id="idm139656375018128" />test set
                      using <code>train_test_split()</code>.</p>
                  </li>
                  <li>
                    <p>Use grid search with cross-validation (with the help of the <code>GridSearchCV</code> class) to
                      find good hyperparameter values for a <code>DecisionTreeClassifier</code>. <a
                        data-type="indexterm" data-primary="Scikit-Learn"
                        data-secondary="sklearn.model_selection.GridSearchCV" id="idm139656375014576" /> <a
                        data-type="indexterm" data-primary="Scikit-Learn"
                        data-secondary="sklearn.tree.DecisionTreeClassifier" id="idm139656375013472" />Hint: try various
                      values for <code>max_leaf_nodes</code>.</p>
                  </li>
                  <li>
                    <p>Train it on the full training set using these hyperparameters, and measure your model’s
                      performance on the test set. You should get roughly 85% to 87% accuracy.</p>
                  </li>

                </ol>
              </li>
              <li>
                <p>Grow a forest.</p>
                <ol>
                  <li>
                    <p>Continuing the previous exercise, generate 1,000 subsets of the training set, each containing 100
                      instances selected randomly. Hint: you can use Scikit-Learn’s <code>ShuffleSplit</code> class for
                      this.</p>
                  </li>
                  <li>
                    <p>Train one Decision Tree on each subset, using the best hyperparameter values found above.
                      Evaluate these 1,000 Decision Trees on the test set. Since they were trained on smaller sets,
                      these Decision Trees will likely perform worse than the first Decision Tree, achieving only about
                      80% accuracy.</p>
                  </li>
                  <li>
                    <p>Now comes the magic. For each test set instance, generate the predictions of the 1,000 Decision
                      Trees, and keep only the most frequent prediction (you can use SciPy’s <code>mode()</code>
                      function for this). This gives you <em>majority-vote predictions</em> over the test set.</p>
                  </li>
                  <li>
                    <p>Evaluate these predictions on the test set: you should obtain a slightly higher accuracy than
                      your first model (about 0.5 to 1.5% higher). Congratulations, you have trained a Random Forest
                      classifier!</p>
                  </li>

                </ol>
              </li>

            </ol>

            <p>Solutions to these exercises are <a data-type="indexterm" data-primary="Decision Trees"
                data-startref="dt6" id="idm139656375003232" />available in Appendix A.</p>
          </div>
        </section>







        <div data-type="footnotes">
          <p data-type="footnote" id="idm139656375394320"><sup><a
                href="ch06.xhtml#idm139656375394320-marker">1</a></sup> Graphviz is an open source graph visualization
            software package, available at <a href="http://www.graphviz.org/"><em
                class="hyperlink">http://www.graphviz.org/</em></a>.</p>
          <p data-type="footnote" id="idm139656375275168"><sup><a
                href="ch06.xhtml#idm139656375275168-marker">2</a></sup> P is the set of problems that can be solved in
            polynomial time. NP is the set of problems whose solutions can be verified in polynomial time. An NP-Hard
            problem is a problem to which any NP problem can be reduced in polynomial time. An NP-Complete problem is
            both NP and NP-Hard. A major open mathematical question is whether or not P = NP. If P ≠ NP (which seems
            likely), then no polynomial algorithm will ever be found for any NP-Complete problem (except perhaps on a
            quantum computer).</p>
          <p data-type="footnote" id="idm139656375267648"><sup><a
                href="ch06.xhtml#idm139656375267648-marker">3</a></sup> <em>log</em><sub>2</sub> is the binary
            logarithm. It is equal to <em>log</em><sub>2</sub>(<em>m</em>) = <em>log</em>(<em>m</em>) / <em>log</em>(2).
          </p>
          <p data-type="footnote" id="idm139656375249424"><sup><a
                href="ch06.xhtml#idm139656375249424-marker">4</a></sup> A reduction of entropy is often called an
            <em>information gain</em>.
          </p>
          <p data-type="footnote" id="idm139656375198448"><sup><a
                href="ch06.xhtml#idm139656375198448-marker">5</a></sup> See Sebastian Raschka’s <a
              href="https://homl.info/19">interesting analysis for more details</a>.</p>
          <p data-type="footnote" id="idm139656375066032"><sup><a
                href="ch06.xhtml#idm139656375066032-marker">6</a></sup> It randomly selects the set of features to
            evaluate at each node.</p>
        </div>
      </div>
    </section>
  </div>



</body>

</html>