<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd"
  xmlns:epub="http://www.idpf.org/2007/ops">

<head>
  <link href="Style00.css" rel="stylesheet" type="text/css" />
  <link href="Style01.css" rel="stylesheet" type="text/css" />
  <link href="Style02.css" rel="stylesheet" type="text/css" />
  <link href="Style03.css" rel="stylesheet" type="text/css" />
  <style type="text/css" title="ibis-book">
    @charset "utf-8";

    #sbo-rt-content html,
    #sbo-rt-content div,
    #sbo-rt-content div,
    #sbo-rt-content span,
    #sbo-rt-content applet,
    #sbo-rt-content object,
    #sbo-rt-content iframe,
    #sbo-rt-content h1,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5,
    #sbo-rt-content h6,
    #sbo-rt-content p,
    #sbo-rt-content blockquote,
    #sbo-rt-content pre,
    #sbo-rt-content a,
    #sbo-rt-content abbr,
    #sbo-rt-content acronym,
    #sbo-rt-content address,
    #sbo-rt-content big,
    #sbo-rt-content cite,
    #sbo-rt-content code,
    #sbo-rt-content del,
    #sbo-rt-content dfn,
    #sbo-rt-content em,
    #sbo-rt-content img,
    #sbo-rt-content ins,
    #sbo-rt-content kbd,
    #sbo-rt-content q,
    #sbo-rt-content s,
    #sbo-rt-content samp,
    #sbo-rt-content small,
    #sbo-rt-content strike,
    #sbo-rt-content strong,
    #sbo-rt-content sub,
    #sbo-rt-content sup,
    #sbo-rt-content tt,
    #sbo-rt-content var,
    #sbo-rt-content b,
    #sbo-rt-content u,
    #sbo-rt-content i,
    #sbo-rt-content center,
    #sbo-rt-content dl,
    #sbo-rt-content dt,
    #sbo-rt-content dd,
    #sbo-rt-content ol,
    #sbo-rt-content ul,
    #sbo-rt-content li,
    #sbo-rt-content fieldset,
    #sbo-rt-content form,
    #sbo-rt-content label,
    #sbo-rt-content legend,
    #sbo-rt-content table,
    #sbo-rt-content caption,
    #sbo-rt-content tdiv,
    #sbo-rt-content tfoot,
    #sbo-rt-content thead,
    #sbo-rt-content tr,
    #sbo-rt-content th,
    #sbo-rt-content td,
    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content canvas,
    #sbo-rt-content details,
    #sbo-rt-content embed,
    #sbo-rt-content figure,
    #sbo-rt-content figcaption,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content output,
    #sbo-rt-content ruby,
    #sbo-rt-content section,
    #sbo-rt-content summary,
    #sbo-rt-content time,
    #sbo-rt-content mark,
    #sbo-rt-content audio,
    #sbo-rt-content video {
      margin: 0;
      padding: 0;
      border: 0;
      font-size: 100%;
      font: inherit;
      vertical-align: baseline
    }

    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content details,
    #sbo-rt-content figcaption,
    #sbo-rt-content figure,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content section {
      display: block
    }

    #sbo-rt-content div {
      line-height: 1
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style: none
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content q {
      quotes: none
    }

    #sbo-rt-content blockquote:before,
    #sbo-rt-content blockquote:after,
    #sbo-rt-content q:before,
    #sbo-rt-content q:after {
      content: none
    }

    #sbo-rt-content table {
      border-collapse: collapse;
      border-spacing: 0
    }

    @page {
      margin: 5px !important
    }

    #sbo-rt-content p {
      margin: 10px 0 0;
      line-height: 125%;
      text-align: left
    }

    #sbo-rt-content p.byline {
      text-align: left;
      margin: -33px auto 35px;
      font-style: italic;
      font-weight: bold
    }

    #sbo-rt-content div.preface p+p.byline {
      margin: 1em 0 0 !important
    }

    #sbo-rt-content div.preface p.byline+p.byline {
      margin: 0 !important
    }

    #sbo-rt-content div.sect1&gt;

    p.byline {
      margin: -.25em 0 1em
    }

    #sbo-rt-content div.sect1&gt;

    p.byline+p.byline {
      margin-top: -1em
    }

    #sbo-rt-content em {
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content em strong,
    #sbo-rt-content strong em {
      font-weight: bold;
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content strong,
    #sbo-rt-content span.bold {
      font-weight: bold
    }

    #sbo-rt-content em.replaceable {
      font-style: italic
    }

    #sbo-rt-content strong.userinput {
      font-weight: bold;
      font-style: normal
    }

    #sbo-rt-content span.bolditalic {
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content a.ulink,
    #sbo-rt-content a.xref,
    #sbo-rt-content a.email,
    #sbo-rt-content a.link,
    #sbo-rt-content a {
      text-decoration: none;
      color: #8e0012
    }

    #sbo-rt-content span.lineannotation {
      font-style: italic;
      color: #a62a2a;
      font-family: serif
    }

    #sbo-rt-content span.underline {
      text-decoration: underline
    }

    #sbo-rt-content span.strikethrough {
      text-decoration: line-through
    }

    #sbo-rt-content span.smallcaps {
      font-variant: small-caps
    }

    #sbo-rt-content span.cursor {
      background: #000;
      color: #fff
    }

    #sbo-rt-content span.smaller {
      font-size: 75%
    }

    #sbo-rt-content .boxedtext,
    #sbo-rt-content .keycap {
      border-style: solid;
      border-width: 1px;
      border-color: #000;
      padding: 1px
    }

    #sbo-rt-content span.gray50 {
      color: #7F7F7F;
    }

    #sbo-rt-content h1,
    #sbo-rt-content div.toc-title,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5 {
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      font-weight: bold;
      text-align: left;
      page-break-after: avoid !important;
      font-family: sans-serif, "DejaVuSans"
    }

    #sbo-rt-content div.toc-title {
      font-size: 1.5em;
      margin-top: 20px !important;
      margin-bottom: 30px !important
    }

    #sbo-rt-content section[data-type="sect1"] h1 {
      font-size: 1.3em;
      color: #8e0012;
      margin: 40px 0 8px 0
    }

    #sbo-rt-content section[data-type="sect2"] h2 {
      font-size: 1.1em;
      margin: 30px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect3"] h3 {
      font-size: 1em;
      color: #555;
      margin: 20px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect4"] h4 {
      font-size: 1em;
      font-weight: normal;
      font-style: italic;
      margin: 15px 0 6px 0 !important
    }

    #sbo-rt-content section[data-type="chapter"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="preface"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="appendix"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="glossary"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="bibliography"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="index"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      line-height: 1;
      margin-bottom: 50px;
      color: #000;
      padding-bottom: 10px;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content span.label,
    #sbo-rt-content span.keep-together {
      font-size: inherit;
      font-weight: inherit
    }

    #sbo-rt-content div[data-type="part"] h1 {
      font-size: 2em;
      text-align: center;
      margin-top: 0 !important;
      margin-bottom: 50px;
      padding: 50px 0 10px 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content img.width-ninety {
      width: 90%
    }

    #sbo-rt-content img {
      max-width: 95%;
      margin: 0 auto;
      padding: 0
    }

    #sbo-rt-content div.figure {
      background-color: transparent;
      text-align: center !important;
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content figure {
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.figure h6,
    #sbo-rt-content figure h6,
    #sbo-rt-content figure figcaption {
      font-size: .9rem !important;
      text-align: center;
      font-weight: normal !important;
      font-style: italic;
      font-family: serif !important;
      text-transform: none !important;
      letter-spacing: normal !important;
      color: #000 !important;
      padding-top: 10px !important;
      page-break-before: avoid
    }

    #sbo-rt-content div.informalfigure {
      text-align: center !important;
      padding: 5px 0 !important
    }

    #sbo-rt-content div.sidebar {
      margin: 15px 0 10px 0 !important;
      border: 1px solid #DCDCDC;
      background-color: #F7F7F7;
      padding: 15px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content aside[data-type="sidebar"] {
      margin: 15px 0 10px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar-title,
    #sbo-rt-content aside[data-type="sidebar"] h5 {
      font-weight: bold;
      font-size: 1em;
      font-family: sans-serif;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar ol,
    #sbo-rt-content div.sidebar ul,
    #sbo-rt-content aside[data-type="sidebar"] ol,
    #sbo-rt-content aside[data-type="sidebar"] ul {
      margin-left: 1.25em !important
    }

    #sbo-rt-content div.sidebar div.figure p.title,
    #sbo-rt-content aside[data-type="sidebar"] figcaption,
    #sbo-rt-content div.sidebar div.informalfigure div.caption {
      font-size: 90%;
      text-align: center;
      font-weight: normal;
      font-style: italic;
      font-family: serif !important;
      color: #000;
      padding: 5px !important;
      page-break-before: avoid;
      page-break-after: avoid
    }

    #sbo-rt-content div.sidebar div.tip,
    #sbo-rt-content div.sidebar div[data-type="tip"],
    #sbo-rt-content div.sidebar div.note,
    #sbo-rt-content div.sidebar div[data-type="note"],
    #sbo-rt-content div.sidebar div.warning,
    #sbo-rt-content div.sidebar div[data-type="warning"],
    #sbo-rt-content div.sidebar div[data-type="caution"],
    #sbo-rt-content div.sidebar div[data-type="important"] {
      margin: 20px auto 20px auto !important;
      font-size: 90%;
      width: 85%
    }

    #sbo-rt-content aside[data-type="sidebar"] p.byline {
      font-size: 90%;
      font-weight: bold;
      font-style: italic;
      text-align: center;
      text-indent: 0;
      margin: 5px auto 6px;
      page-break-after: avoid
    }

    #sbo-rt-content pre {
      white-space: pre-wrap;
      font-family: "Ubuntu Mono", monospace;
      margin: 25px 0 25px 20px;
      font-size: 85%;
      display: block;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content div.note pre.programlisting,
    #sbo-rt-content div.tip pre.programlisting,
    #sbo-rt-content div.warning pre.programlisting,
    #sbo-rt-content div.caution pre.programlisting,
    #sbo-rt-content div.important pre.programlisting {
      margin-bottom: 0
    }

    #sbo-rt-content code {
      font-family: "Ubuntu Mono", monospace;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content code strong em,
    #sbo-rt-content code em strong,
    #sbo-rt-content pre em strong,
    #sbo-rt-content pre strong em,
    #sbo-rt-content strong code em code,
    #sbo-rt-content em code strong code,
    #sbo-rt-content span.bolditalic code {
      font-weight: bold;
      font-style: italic;
      font-family: "Ubuntu Mono BoldItal", monospace
    }

    #sbo-rt-content code em,
    #sbo-rt-content em code,
    #sbo-rt-content pre em,
    #sbo-rt-content em.replaceable {
      font-family: "Ubuntu Mono Ital", monospace;
      font-style: italic
    }

    #sbo-rt-content code strong,
    #sbo-rt-content strong code,
    #sbo-rt-content pre strong,
    #sbo-rt-content strong.userinput {
      font-family: "Ubuntu Mono Bold", monospace;
      font-weight: bold
    }

    #sbo-rt-content div[data-type="example"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div[data-type="example"] h1,
    #sbo-rt-content div[data-type="example"] h2,
    #sbo-rt-content div[data-type="example"] h3,
    #sbo-rt-content div[data-type="example"] h4,
    #sbo-rt-content div[data-type="example"] h5,
    #sbo-rt-content div[data-type="example"] h6 {
      font-style: italic;
      font-weight: normal;
      text-align: left !important;
      text-transform: none !important;
      font-family: serif !important;
      margin: 10px 0 5px 0 !important;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content li pre.example {
      padding: 10px 0 !important
    }

    #sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],
    #sbo-rt-content div[data-type="example"] pre[data-type="screen"] {
      margin: 0
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      margin: 50px 0 10px 0 !important;
      line-height: 1;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] h2,
    #sbo-rt-content section[data-type="titlepage"] p.subtitle,
    #sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"] {
      font-size: 1.3em;
      font-weight: normal;
      text-align: center;
      margin-top: .5em;
      color: #555
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h2[data-type="author"],
    #sbo-rt-content section[data-type="titlepage"] p.author {
      font-size: 1.3em;
      font-family: serif !important;
      font-weight: bold;
      margin: 50px 0 !important;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] p.edition {
      text-align: center;
      text-transform: uppercase;
      margin-top: 2em
    }

    #sbo-rt-content section[data-type="titlepage"] {
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"]:after {
      content: url(css_assets/titlepage_footer_ebook.png);
      margin: 0 auto;
      max-width: 80%
    }

    #sbo-rt-content div.book div.titlepage div.publishername {
      margin-top: 60%;
      margin-bottom: 20px;
      text-align: center;
      font-size: 1.25em
    }

    #sbo-rt-content div.book div.titlepage div.locations p {
      margin: 0;
      text-align: center
    }

    #sbo-rt-content div.book div.titlepage div.locations p.cities {
      font-size: 80%;
      text-align: center;
      margin-top: 5px
    }

    #sbo-rt-content section.preface[title="Dedication"]&gt;

    div.titlepage h2.title {
      text-align: center;
      text-transform: uppercase;
      font-size: 1.5em;
      margin-top: 50px;
      margin-bottom: 50px
    }

    #sbo-rt-content ul.stafflist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.stafflist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content ul.printings li {
      list-style-type: none
    }

    #sbo-rt-content section.preface[title="Dedication"] p {
      font-style: italic;
      text-align: center
    }

    #sbo-rt-content div.colophon h1.title {
      font-size: 1.3em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon h2.subtitle {
      margin: 0 !important;
      color: #000;
      font-family: serif !important;
      font-size: 1em;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.author h3.author {
      font-size: 1.1em;
      font-family: serif !important;
      margin: 10px 0 0 !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h4,
    #sbo-rt-content div.colophon div.editor h3.editor {
      color: #000;
      font-size: .8em;
      margin: 15px 0 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h3.editor {
      font-size: .8em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.publisher {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon div.publisher p,
    #sbo-rt-content div.colophon div.publisher span.publishername {
      margin: 0;
      font-size: .8em
    }

    #sbo-rt-content div.legalnotice p,
    #sbo-rt-content div.timestamp p {
      font-size: .8em
    }

    #sbo-rt-content div.timestamp p {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon[title="About the Author"] h1.title,
    #sbo-rt-content div.colophon[title="Colophon"] h1.title {
      font-size: 1.5em;
      margin: 0 !important;
      font-family: sans-serif !important
    }

    #sbo-rt-content section.chapter div.titlepage div.author {
      margin: 10px 0 10px 0
    }

    #sbo-rt-content section.chapter div.titlepage div.author div.affiliation {
      font-style: italic
    }

    #sbo-rt-content div.attribution {
      margin: 5px 0 0 50px !important
    }

    #sbo-rt-content h3.author span.orgname {
      display: none
    }

    #sbo-rt-content div.epigraph {
      margin: 10px 0 10px 20px !important;
      page-break-inside: avoid;
      font-size: 90%
    }

    #sbo-rt-content div.epigraph p {
      font-style: italic
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content div.blockquote {
      margin: 10px !important;
      page-break-inside: avoid;
      font-size: 95%
    }

    #sbo-rt-content blockquote p,
    #sbo-rt-content div.blockquote p {
      font-style: italic;
      margin: .75em 0 0 !important
    }

    #sbo-rt-content blockquote div.attribution,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      margin: 5px 0 10px 30px !important;
      text-align: right;
      width: 80%
    }

    #sbo-rt-content blockquote div.attribution p,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      font-style: normal;
      margin-top: 5px
    }

    #sbo-rt-content blockquote div.attribution p:before,
    #sbo-rt-content blockquote p[data-type="attribution"]:before {
      font-style: normal;
      content: "—";
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none
    }

    #sbo-rt-content p.right {
      text-align: right;
      margin: 0
    }

    #sbo-rt-content div[data-type="footnotes"] {
      border-top: 1px solid black;
      margin-top: 2em
    }

    #sbo-rt-content sub,
    #sbo-rt-content sup {
      font-size: 75%;
      line-height: 0;
      position: relative
    }

    #sbo-rt-content sup {
      top: -.5em
    }

    #sbo-rt-content sub {
      bottom: -.25em
    }

    #sbo-rt-content p[data-type="footnote"] {
      font-size: 90% !important;
      line-height: 1.2em !important;
      margin-left: 2.5em !important;
      text-indent: -2.3em !important
    }

    #sbo-rt-content p[data-type="footnote"] sup {
      display: inline-block !important;
      position: static !important;
      width: 2em !important;
      text-align: right !important;
      font-size: 100% !important;
      padding-right: .5em !important
    }

    #sbo-rt-content p[data-type="footnote"] a[href$="-marker"] {
      font-family: sans-serif !important;
      font-size: 90% !important;
      color: #8e0012 !important
    }

    #sbo-rt-content a[data-type="noteref"] {
      font-family: sans-serif !important;
      color: #8e0012;
      margin-left: 0;
      padding-left: 0
    }

    #sbo-rt-content div.refentry p.refname {
      font-size: 1em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold;
      margin-bottom: 5px;
      overflow: auto;
      width: 100%
    }

    #sbo-rt-content div.refentry {
      width: 100%;
      display: block;
      margin-top: 2em
    }

    #sbo-rt-content div.refsynopsisdiv {
      display: block;
      clear: both
    }

    #sbo-rt-content div.refentry header {
      page-break-inside: avoid !important;
      display: block;
      break-inside: avoid !important;
      padding-top: 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content div.refsect1 h6 {
      font-size: .9em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold
    }

    #sbo-rt-content div.refsect1 {
      margin-top: 3em
    }

    #sbo-rt-content dt {
      padding-top: 10px !important;
      padding-bottom: 0 !important
    }

    #sbo-rt-content dd {
      margin-left: 1.5em !important;
      margin-bottom: .25em
    }

    #sbo-rt-content dd ol,
    #sbo-rt-content dd ul {
      padding-left: 1em
    }

    #sbo-rt-content dd li {
      margin-top: 0;
      margin-bottom: 0
    }

    #sbo-rt-content dd,
    #sbo-rt-content li {
      text-align: left
    }

    #sbo-rt-content ul,
    #sbo-rt-content ul&gt;
    li,
    #sbo-rt-content ol ul,
    #sbo-rt-content ol ul&gt;
    li,
    #sbo-rt-content ul ol ul,
    #sbo-rt-content ul ol ul&gt;

    li {
      list-style-type: disc
    }

    #sbo-rt-content ul ul,
    #sbo-rt-content ul ul&gt;

    li {
      list-style-type: square
    }

    #sbo-rt-content ul ul ul,
    #sbo-rt-content ul ul ul&gt;

    li {
      list-style-type: circle
    }

    #sbo-rt-content ol,
    #sbo-rt-content ol&gt;
    li,
    #sbo-rt-content ol ul ol,
    #sbo-rt-content ol ul ol&gt;
    li,
    #sbo-rt-content ul ol,
    #sbo-rt-content ul ol&gt;

    li {
      list-style-type: decimal
    }

    #sbo-rt-content ol ol,
    #sbo-rt-content ol ol&gt;

    li {
      list-style-type: lower-alpha
    }

    #sbo-rt-content ol ol ol,
    #sbo-rt-content ol ol ol&gt;

    li {
      list-style-type: lower-roman
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style-position: outside;
      margin: 15px 0 15px 1.25em;
      padding-left: 2.25em
    }

    #sbo-rt-content ol li,
    #sbo-rt-content ul li {
      margin: .5em 0 .65em;
      line-height: 125%
    }

    #sbo-rt-content div.orderedlistalpha {
      list-style-type: upper-alpha
    }

    #sbo-rt-content table.simplelist,
    #sbo-rt-content ul.simplelist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.simplelist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content table.simplelist td {
      border: none
    }

    #sbo-rt-content table.simplelist tr {
      border-bottom: none
    }

    #sbo-rt-content table.simplelist tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content dl.calloutlist p:first-child {
      margin-top: -25px !important
    }

    #sbo-rt-content dl.calloutlist dd {
      padding-left: 0;
      margin-top: -25px
    }

    #sbo-rt-content dl.calloutlist img,
    #sbo-rt-content a.co img {
      padding: 0
    }

    #sbo-rt-content div.toc ol {
      margin-top: 8px !important;
      margin-bottom: 8px !important;
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol ol {
      margin-left: 30px !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol li {
      list-style-type: none
    }

    #sbo-rt-content div.toc a {
      color: #8e0012
    }

    #sbo-rt-content div.toc ol a {
      font-size: 1em;
      font-weight: bold
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;

    ol a {
      font-weight: bold;
      font-size: 1em
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;
    ol&gt;
    li&gt;

    ol a {
      text-decoration: none;
      font-weight: normal;
      font-size: 1em
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"],
    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      margin: 30px !important;
      font-size: 90%;
      padding: 10px 8px 20px 8px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.tip ol,
    #sbo-rt-content div.tip ul,
    #sbo-rt-content div[data-type="tip"] ol,
    #sbo-rt-content div[data-type="tip"] ul,
    #sbo-rt-content div.note ol,
    #sbo-rt-content div.note ul,
    #sbo-rt-content div[data-type="note"] ol,
    #sbo-rt-content div[data-type="note"] ul,
    #sbo-rt-content div.warning ol,
    #sbo-rt-content div.warning ul,
    #sbo-rt-content div[data-type="warning"] ol,
    #sbo-rt-content div[data-type="warning"] ul,
    #sbo-rt-content div[data-type="caution"] ol,
    #sbo-rt-content div[data-type="caution"] ul,
    #sbo-rt-content div[data-type="important"] ol,
    #sbo-rt-content div[data-type="important"] ul {
      margin-left: 1.5em !important
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"] {
      border: 1px solid #BEBEBE;
      background-color: transparent
    }

    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      border: 1px solid #BC8F8F
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="note"] h1,
    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1,
    #sbo-rt-content div[data-type="important"] h6 {
      font-weight: bold;
      font-size: 110%;
      font-family: sans-serif !important;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px !important
    }

    #sbo-rt-content div[data-type="tip"] figure h6,
    #sbo-rt-content div[data-type="note"] figure h6,
    #sbo-rt-content div[data-type="warning"] figure h6,
    #sbo-rt-content div[data-type="caution"] figure h6,
    #sbo-rt-content div[data-type="important"] figure h6 {
      font-family: serif !important
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div[data-type="note"] h1 {
      color: #737373
    }

    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="important"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1 {
      color: #C67171
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,
    #sbo-rt-content div.safarienabled {
      background-color: transparent;
      margin: 8px 0 0 !important;
      border: 0 solid #BEBEBE;
      font-size: 100%;
      padding: 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,
    #sbo-rt-content div.safarienabled h6 {
      display: none
    }

    #sbo-rt-content div.table,
    #sbo-rt-content table {
      margin: 15px 0 30px 0 !important;
      max-width: 95%;
      border: none !important;
      background: none;
      display: table !important
    }

    #sbo-rt-content div.table,
    #sbo-rt-content div.informaltable,
    #sbo-rt-content table {
      page-break-inside: avoid
    }

    #sbo-rt-content tr,
    #sbo-rt-content tr td {
      border-bottom: 1px solid #c3c3c3
    }

    #sbo-rt-content thead td,
    #sbo-rt-content thead th {
      border-bottom: #9d9d9d 1px solid !important;
      border-top: #9d9d9d 1px solid !important
    }

    #sbo-rt-content tr:nth-of-type(even) {
      background-color: #f1f6fc
    }

    #sbo-rt-content thead {
      font-family: sans-serif;
      font-weight: bold
    }

    #sbo-rt-content td,
    #sbo-rt-content th {
      display: table-cell;
      padding: .3em;
      text-align: left;
      vertical-align: middle;
      font-size: 80%
    }

    #sbo-rt-content div.informaltable table {
      margin: 10px auto !important
    }

    #sbo-rt-content div.informaltable table tr {
      border-bottom: none
    }

    #sbo-rt-content div.informaltable table tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content div.informaltable td,
    #sbo-rt-content div.informaltable th {
      border: #9d9d9d 1px solid
    }

    #sbo-rt-content div.table-title,
    #sbo-rt-content table caption {
      font-weight: normal;
      font-style: italic;
      font-family: serif;
      font-size: 1em;
      margin: 10px 0 10px 0 !important;
      padding: 0;
      page-break-after: avoid;
      text-align: left !important
    }

    #sbo-rt-content table code {
      font-size: smaller
    }

    #sbo-rt-content table.border tbody&gt;
    tr:last-child&gt;

    td {
      border-bottom: transparent
    }

    #sbo-rt-content div.equation,
    #sbo-rt-content div[data-type="equation"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div.equation-title,
    #sbo-rt-content div[data-type="equation"] h5 {
      font-style: italic;
      font-weight: normal;
      font-family: serif !important;
      font-size: 90%;
      margin: 20px 0 10px 0 !important;
      page-break-after: avoid
    }

    #sbo-rt-content div.equation-contents {
      margin-left: 20px
    }

    #sbo-rt-content div[data-type="equation"] math {
      font-size: calc(.35em + 1vw)
    }

    #sbo-rt-content span.inlinemediaobject {
      height: .85em;
      display: inline-block;
      margin-bottom: .2em
    }

    #sbo-rt-content span.inlinemediaobject img {
      margin: 0;
      height: .85em
    }

    #sbo-rt-content div.informalequation {
      margin: 20px 0 20px 20px;
      width: 75%
    }

    #sbo-rt-content div.informalequation img {
      width: 75%
    }

    #sbo-rt-content div.index {
      text-indent: 0
    }

    #sbo-rt-content div.index h3 {
      padding: .25em;
      margin-top: 1em !important;
      background-color: #F0F0F0
    }

    #sbo-rt-content div.index li {
      line-height: 130%;
      list-style-type: none
    }

    #sbo-rt-content div.index a.indexterm {
      color: #8e0012 !important
    }

    #sbo-rt-content div.index ul {
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.index ul ul {
      margin-left: 1em !important;
      margin-top: 0 !important
    }

    #sbo-rt-content code.boolean,
    #sbo-rt-content .navy {
      color: rgb(0, 0, 128);
    }

    #sbo-rt-content code.character,
    #sbo-rt-content .olive {
      color: rgb(128, 128, 0);
    }

    #sbo-rt-content code.comment,
    #sbo-rt-content .blue {
      color: rgb(0, 0, 255);
    }

    #sbo-rt-content code.conditional,
    #sbo-rt-content .limegreen {
      color: rgb(50, 205, 50);
    }

    #sbo-rt-content code.constant,
    #sbo-rt-content .darkorange {
      color: rgb(255, 140, 0);
    }

    #sbo-rt-content code.debug,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.define,
    #sbo-rt-content .darkgoldenrod,
    #sbo-rt-content .gold {
      color: rgb(184, 134, 11);
    }

    #sbo-rt-content code.delimiter,
    #sbo-rt-content .dimgray {
      color: rgb(105, 105, 105);
    }

    #sbo-rt-content code.error,
    #sbo-rt-content .red {
      color: rgb(255, 0, 0);
    }

    #sbo-rt-content code.exception,
    #sbo-rt-content .salmon {
      color: rgb(250, 128, 11);
    }

    #sbo-rt-content code.float,
    #sbo-rt-content .steelblue {
      color: rgb(70, 130, 180);
    }

    #sbo-rt-content pre code.function,
    #sbo-rt-content .green {
      color: rgb(0, 128, 0);
    }

    #sbo-rt-content code.identifier,
    #sbo-rt-content .royalblue {
      color: rgb(65, 105, 225);
    }

    #sbo-rt-content code.ignore,
    #sbo-rt-content .gray {
      color: rgb(128, 128, 128);
    }

    #sbo-rt-content code.include,
    #sbo-rt-content .purple {
      color: rgb(128, 0, 128);
    }

    #sbo-rt-content code.keyword,
    #sbo-rt-content .sienna {
      color: rgb(160, 82, 45);
    }

    #sbo-rt-content code.label,
    #sbo-rt-content .deeppink {
      color: rgb(255, 20, 147);
    }

    #sbo-rt-content code.macro,
    #sbo-rt-content .orangered {
      color: rgb(255, 69, 0);
    }

    #sbo-rt-content code.number,
    #sbo-rt-content .brown {
      color: rgb(165, 42, 42);
    }

    #sbo-rt-content code.operator,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.preCondit,
    #sbo-rt-content .teal {
      color: rgb(0, 128, 128);
    }

    #sbo-rt-content code.preProc,
    #sbo-rt-content .fuschia {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.repeat,
    #sbo-rt-content .indigo {
      color: rgb(75, 0, 130);
    }

    #sbo-rt-content code.special,
    #sbo-rt-content .saddlebrown {
      color: rgb(139, 69, 19);
    }

    #sbo-rt-content code.specialchar,
    #sbo-rt-content .magenta {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.specialcomment,
    #sbo-rt-content .seagreen {
      color: rgb(46, 139, 87);
    }

    #sbo-rt-content code.statement,
    #sbo-rt-content .forestgreen {
      color: rgb(34, 139, 34);
    }

    #sbo-rt-content code.storageclass,
    #sbo-rt-content .plum {
      color: rgb(221, 160, 221);
    }

    #sbo-rt-content code.string,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.structure,
    #sbo-rt-content .chocolate {
      color: rgb(210, 106, 30);
    }

    #sbo-rt-content code.tag,
    #sbo-rt-content .darkcyan {
      color: rgb(0, 139, 139);
    }

    #sbo-rt-content code.todo,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.type,
    #sbo-rt-content .mediumslateblue {
      color: rgb(123, 104, 238);
    }

    #sbo-rt-content code.typedef,
    #sbo-rt-content .darkgreen {
      color: rgb(0, 100, 0);
    }

    #sbo-rt-content code.underlined {
      text-decoration: underline;
    }

    #sbo-rt-content pre code.hll {
      background-color: #ffc
    }

    #sbo-rt-content pre code.c {
      color: #09F;
      font-style: italic
    }

    #sbo-rt-content pre code.err {
      color: #A00
    }

    #sbo-rt-content pre code.k {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.o {
      color: #555
    }

    #sbo-rt-content pre code.cm {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cp {
      color: #099
    }

    #sbo-rt-content pre code.c1 {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cs {
      color: #35586C;
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content pre code.gd {
      background-color: #FCC
    }

    #sbo-rt-content pre code.ge {
      font-style: italic
    }

    #sbo-rt-content pre code.gr {
      color: #F00
    }

    #sbo-rt-content pre code.gh {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gi {
      background-color: #CFC
    }

    #sbo-rt-content pre code.go {
      color: #000
    }

    #sbo-rt-content pre code.gp {
      color: #009;
      font-weight: bold
    }

    #sbo-rt-content pre code.gs {
      font-weight: bold
    }

    #sbo-rt-content pre code.gu {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gt {
      color: #9C6
    }

    #sbo-rt-content pre code.kc {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kd {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kn {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kp {
      color: #069
    }

    #sbo-rt-content pre code.kr {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kt {
      color: #078;
      font-weight: bold
    }

    #sbo-rt-content pre code.m {
      color: #F60
    }

    #sbo-rt-content pre code.s {
      color: #C30
    }

    #sbo-rt-content pre code.na {
      color: #309
    }

    #sbo-rt-content pre code.nb {
      color: #366
    }

    #sbo-rt-content pre code.nc {
      color: #0A8;
      font-weight: bold
    }

    #sbo-rt-content pre code.no {
      color: #360
    }

    #sbo-rt-content pre code.nd {
      color: #99F
    }

    #sbo-rt-content pre code.ni {
      color: #999;
      font-weight: bold
    }

    #sbo-rt-content pre code.ne {
      color: #C00;
      font-weight: bold
    }

    #sbo-rt-content pre code.nf {
      color: #C0F
    }

    #sbo-rt-content pre code.nl {
      color: #99F
    }

    #sbo-rt-content pre code.nn {
      color: #0CF;
      font-weight: bold
    }

    #sbo-rt-content pre code.nt {
      color: #309;
      font-weight: bold
    }

    #sbo-rt-content pre code.nv {
      color: #033
    }

    #sbo-rt-content pre code.ow {
      color: #000;
      font-weight: bold
    }

    #sbo-rt-content pre code.w {
      color: #bbb
    }

    #sbo-rt-content pre code.mf {
      color: #F60
    }

    #sbo-rt-content pre code.mh {
      color: #F60
    }

    #sbo-rt-content pre code.mi {
      color: #F60
    }

    #sbo-rt-content pre code.mo {
      color: #F60
    }

    #sbo-rt-content pre code.sb {
      color: #C30
    }

    #sbo-rt-content pre code.sc {
      color: #C30
    }

    #sbo-rt-content pre code.sd {
      color: #C30;
      font-style: italic
    }

    #sbo-rt-content pre code.s2 {
      color: #C30
    }

    #sbo-rt-content pre code.se {
      color: #C30;
      font-weight: bold
    }

    #sbo-rt-content pre code.sh {
      color: #C30
    }

    #sbo-rt-content pre code.si {
      color: #A00
    }

    #sbo-rt-content pre code.sx {
      color: #C30
    }

    #sbo-rt-content pre code.sr {
      color: #3AA
    }

    #sbo-rt-content pre code.s1 {
      color: #C30
    }

    #sbo-rt-content pre code.ss {
      color: #A60
    }

    #sbo-rt-content pre code.bp {
      color: #366
    }

    #sbo-rt-content pre code.vc {
      color: #033
    }

    #sbo-rt-content pre code.vg {
      color: #033
    }

    #sbo-rt-content pre code.vi {
      color: #033
    }

    #sbo-rt-content pre code.il {
      color: #F60
    }

    #sbo-rt-content pre code.g {
      color: #050
    }

    #sbo-rt-content pre code.l {
      color: #C60
    }

    #sbo-rt-content pre code.l {
      color: #F90
    }

    #sbo-rt-content pre code.n {
      color: #008
    }

    #sbo-rt-content pre code.nx {
      color: #008
    }

    #sbo-rt-content pre code.py {
      color: #96F
    }

    #sbo-rt-content pre code.p {
      color: #000
    }

    #sbo-rt-content pre code.x {
      color: #F06
    }

    #sbo-rt-content div.blockquote_sampler_toc {
      width: 95%;
      margin: 5px 5px 5px 10px !important
    }

    #sbo-rt-content div {
      font-family: serif;
      text-align: left
    }

    #sbo-rt-content .gray-background,
    #sbo-rt-content .reverse-video {
      background: #2E2E2E;
      color: #FFF
    }

    #sbo-rt-content .light-gray-background {
      background: #A0A0A0
    }

    #sbo-rt-content .preserve-whitespace {
      white-space: pre-wrap
    }

    #sbo-rt-content span.gray {
      color: #4C4C4C
    }

    #sbo-rt-content .width-10 {
      width: 10vw !important
    }

    #sbo-rt-content .width-20 {
      width: 20vw !important
    }

    #sbo-rt-content .width-30 {
      width: 30vw !important
    }

    #sbo-rt-content .width-40 {
      width: 40vw !important
    }

    #sbo-rt-content .width-50 {
      width: 50vw !important
    }

    #sbo-rt-content .width-60 {
      width: 60vw !important
    }

    #sbo-rt-content .width-70 {
      width: 70vw !important
    }

    #sbo-rt-content .width-80 {
      width: 80vw !important
    }

    #sbo-rt-content .width-90 {
      width: 90vw !important
    }

    #sbo-rt-content .width-full,
    #sbo-rt-content .width-100 {
      width: 100vw !important
    }

    #sbo-rt-content div[data-type="equation"].fifty-percent img {
      width: 50%
    }
  </style>
  <style type="text/css" id="font-styles">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-size: &lt;
      %=font_size %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="font-family">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-family: &lt;
      %=font_family %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="column-width">
    #sbo-rt-content {
      max-width: &lt;
      %=column_width %&gt;
      % !important;
      margin: 0 auto !important;
    }
  </style>

  <style type="text/css">
    body {
      background-color: #fbfbfb !important;
      margin: 1em;
    }

    #sbo-rt-content * {
      text-indent: 0pt !important;
    }

    #sbo-rt-content .bq {
      margin-right: 1em !important;
    }

    #sbo-rt-content * {
      word-wrap: break-word !important;
      word-break: break-word !important;
    }

    #sbo-rt-content table,
    #sbo-rt-content pre {
      overflow-x: unset !important;
      overflow: unset !important;
      overflow-y: unset !important;
      white-space: pre-wrap !important;
    }
  </style>
</head>

<body>
  <div id="sbo-rt-content">
    <section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 4. Training Models">
      <div class="chapter" id="linear_models_chapter">
        <h1><span class="label">Chapter 4. </span>Training Models</h1>


        <p>So <a data-type="indexterm" data-primary="training models" id="tm4" /><a data-type="indexterm"
            data-primary="training models" data-secondary="overview" id="tm4o" />far we have treated Machine Learning
          models and their training algorithms mostly like black boxes. If you went through some of the exercises in the
          previous chapters, you may have been surprised by how much you can get done without knowing anything about
          what’s under the hood: you optimized a regression system, you improved a digit image classifier, and you even
          built a spam classifier from scratch—all this without knowing how they actually work. Indeed, in many
          situations you don’t really need to know the implementation details.</p>

        <p>However, having a good understanding of how things work can help you quickly home in on the appropriate
          model, the right training algorithm to use, and a good set of hyperparameters for your task. Understanding
          what’s under the hood will also help you debug issues and perform error analysis more efficiently. Lastly,
          most of the topics discussed in this chapter will be essential in understanding, building, and training neural
          networks (discussed in another part of this book).</p>

        <p>In this chapter, we will start by looking at the <a data-type="indexterm" data-primary="training models"
            data-secondary="Linear Regression" id="idm139656380662752" /><a data-type="indexterm"
            data-primary="Linear Regression" id="lr4" />Linear Regression model, one of the simplest models there is. We
          will discuss two very different ways to train it:</p>

        <ul>
          <li>
            <p>Using a direct “closed-form” <a data-type="indexterm" data-primary="closed-form equation"
                id="idm139656380659504" />equation that directly computes the model parameters that best fit the model
              to the training set (i.e., the model parameters that minimize the cost function over the training set).
            </p>
          </li>
          <li>
            <p>Using an iterative optimization approach, called <a data-type="indexterm"
                data-primary="Gradient Descent (GD)" id="idm139656380657584" />Gradient Descent (GD), that gradually
              tweaks the model parameters to minimize the cost function <a data-type="indexterm"
                data-primary="cost function" data-secondary="in Gradient Descent" data-secondary-sortas="Gradient"
                id="idm139656380656640" />over the training set, eventually converging to the same set of parameters as
              the first method. We will look at a few variants of Gradient Descent that we will use again and again when
              we study neural networks in Part II: Batch GD, Mini-batch GD, and Stochastic GD.</p>
          </li>
        </ul>

        <p>Next we will look at <a data-type="indexterm" data-primary="training models"
            data-secondary="Polynomial Regression" id="idm139656380654368" /><a data-type="indexterm"
            data-primary="Polynomial Regression" id="idm139656380653360" />Polynomial Regression, a more complex model
          that can fit nonlinear datasets. Since this model has more parameters than Linear Regression, it is more prone
          to overfitting the training data, so we will look at how to detect whether or not this is the case, using
          learning curves, and then we will look at several regularization techniques that can reduce the risk of
          overfitting the training set.</p>

        <p>Finally, we will look at two more models that are commonly used for classification tasks: Logistic Regression
          and Softmax Regression.</p>
        <div data-type="warning" epub:type="warning">
          <h6>Warning</h6>
          <p>There will be quite a few math equations in this chapter, using basic notions of linear algebra and
            calculus. To understand these equations, you will need to know what vectors and matrices are, how to
            transpose them, multiply them, and inverse them, and what partial derivatives are. If you are unfamiliar
            with these concepts, please go through the linear algebra and calculus introductory tutorials available as
            Jupyter notebooks in the online supplemental material. For those who are truly allergic to mathematics, you
            should still go through this chapter and simply skip the equations; hopefully, the text will be sufficient
            to help you understand most of the <a data-type="indexterm" data-primary="training models"
              data-secondary="overview" data-startref="tm4o" id="idm139656380649936" />concepts.</p>
        </div>






        <section data-type="sect1" data-pdf-bookmark="Linear Regression">
          <div class="sect1" id="idm139656380648304">
            <h1>Linear Regression</h1>

            <p>In <a data-type="xref" href="ch01.xhtml#landscape_chapter">Chapter 1</a>, <a data-type="indexterm"
                data-primary="linear models" data-secondary="regression" data-see="Linear Regression"
                id="idm139656380645488" /><a data-type="indexterm" data-primary="training models"
                data-secondary="Linear Regression" id="tm4lr" />we looked at a simple regression model of life
              satisfaction: <em>life_satisfaction</em> = <em>θ</em><sub>0</sub> + <em>θ</em><sub>1</sub> ×
              <em>GDP_per_capita</em>.</p>

            <p>This model is just a linear function of the input feature <code>GDP_per_capita</code>.
              <em>θ</em><sub>0</sub> and <em>θ</em><sub>1</sub> are the model’s parameters.</p>

            <p>More generally, a linear model makes a prediction by simply computing a weighted sum of the input
              features, plus a constant called <a data-type="indexterm" data-primary="bias term"
                id="idm139656380637776" /><a data-type="indexterm" data-primary="intercept term"
                id="idm139656380637072" />the <em>bias term</em> (also called the <em>intercept term</em>), as shown in
              <a data-type="xref" href="#equation_four_one">Equation 4-1</a>.</p>
            <div data-type="equation" id="equation_four_one">
              <h5><span class="label">Equation 4-1. </span>Linear Regression model prediction</h5>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow>
                  <mover accent="true">
                    <mi>y</mi>
                    <mo>^</mo>
                  </mover>
                  <mo>=</mo>
                  <msub>
                    <mi>θ</mi>
                    <mn>0</mn>
                  </msub>
                  <mo>+</mo>
                  <msub>
                    <mi>θ</mi>
                    <mn>1</mn>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mn>1</mn>
                  </msub>
                  <mo>+</mo>
                  <msub>
                    <mi>θ</mi>
                    <mn>2</mn>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mn>2</mn>
                  </msub>
                  <mo>+</mo>
                  <mo>⋯</mo>
                  <mo>+</mo>
                  <msub>
                    <mi>θ</mi>
                    <mi>n</mi>
                  </msub>
                  <msub>
                    <mi>x</mi>
                    <mi>n</mi>
                  </msub>
                </mrow>
              </math>
            </div>

            <ul>
              <li>
                <p><em>ŷ</em> is the predicted value.</p>
              </li>
              <li>
                <p><em>n</em> is the number of features.</p>
              </li>
              <li>
                <p><em>x</em><sub>i</sub> is the i<sup>th</sup> feature value.</p>
              </li>
              <li>
                <p><em>θ</em><sub><em>j</em></sub> is the j<sup>th</sup> model parameter (including the bias term
                  <em>θ</em><sub>0</sub> and the feature weights <em>θ</em><sub>1</sub>, <em>θ</em><sub>2</sub>, ⋯,
                  <em>θ</em><sub><em>n</em></sub>).</p>
              </li>
            </ul>

            <p>This can be written much more concisely using a vectorized form, as shown in <a data-type="xref"
                href="#linear_regression_prediction_vectorized_equation">Equation 4-2</a>.</p>
            <div id="linear_regression_prediction_vectorized_equation" data-type="equation">
              <h5><span class="label">Equation 4-2. </span>Linear Regression model prediction (vectorized form)</h5>
              <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mover accent="true">
                  <mi>y</mi>
                  <mo>^</mo>
                </mover>
                <mo>=</mo>
                <msub>
                  <mi>h</mi>
                  <mi mathvariant="bold">θ</mi>
                </msub>
                <mo>(</mo>
                <mi mathvariant="bold">x</mi>
                <mo>)</mo>
                <mo>=</mo>
                <mi mathvariant="bold">θ</mi>
                <mo>·</mo>
                <mi mathvariant="bold">x</mi>
              </math>
            </div>

            <ul>
              <li>
                <p><strong>θ</strong> is the model’s <em>parameter vector</em>, <a data-type="indexterm"
                    data-primary="parameter vector" id="idm139656380551744" />containing the bias term
                  <em>θ</em><sub>0</sub> and the feature weights <em>θ</em><sub>1</sub> to <em>θ</em><sub>n</sub>.</p>
              </li>
              <li>
                <p><strong>x</strong> is the instance’s <em>feature vector</em>, <a data-type="indexterm"
                    data-primary="feature vector" id="idm139656380547024" />containing <em>x</em><sub>0</sub> to
                  <em>x</em><sub><em>n</em></sub>, with <em>x</em><sub>0</sub> always equal to 1.</p>
              </li>
              <li>
                <p><strong>θ</strong> · <strong>x</strong> is the dot product of the vectors <strong>θ</strong> and
                  <strong>x</strong>, which is of course equal to <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <msub>
                        <mi>θ</mi>
                        <mn>0</mn>
                      </msub>
                      <msub>
                        <mi>x</mi>
                        <mn>0</mn>
                      </msub>
                      <mo>+</mo>
                      <msub>
                        <mi>θ</mi>
                        <mn>1</mn>
                      </msub>
                      <msub>
                        <mi>x</mi>
                        <mn>1</mn>
                      </msub>
                      <mo>+</mo>
                      <msub>
                        <mi>θ</mi>
                        <mn>2</mn>
                      </msub>
                      <msub>
                        <mi>x</mi>
                        <mn>2</mn>
                      </msub>
                      <mo>+</mo>
                      <mo>⋯</mo>
                      <mo>+</mo>
                      <msub>
                        <mi>θ</mi>
                        <mi>n</mi>
                      </msub>
                      <msub>
                        <mi>x</mi>
                        <mi>n</mi>
                      </msub>
                    </mrow>
                  </math>.</p>
              </li>
              <li>
                <p><em>h</em><sub><strong>θ</strong></sub> is the <a data-type="indexterm"
                    data-primary="hypothesis function" id="idm139656380526880" />hypothesis function, using the model
                  parameters <strong>θ</strong>.</p>
              </li>
            </ul>
            <div data-type="note" epub:type="note">
              <h6>Note</h6>
              <p>In Machine Learning, vectors are often represented as <em>column vectors</em>, which are 2D arrays with
                a single column. If <strong>θ</strong> and <strong>x</strong> are column vectors, then the prediction
                is: <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <mover accent="true">
                    <mi>y</mi>
                    <mo>^</mo>
                  </mover>
                  <mo>=</mo>
                  <msup>
                    <mi mathvariant="bold">θ</mi>
                    <mi mathvariant="bold">T</mi>
                  </msup>
                  <mi mathvariant="bold">x</mi>
                </math>, where <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <msup>
                    <mi mathvariant="bold">θ</mi>
                    <mi mathvariant="bold">T</mi>
                  </msup>
                </math> is the <em>transpose</em> of <strong>θ</strong> (a row vector instead of a column vector) and
                <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <msup>
                    <mi mathvariant="bold">θ</mi>
                    <mi mathvariant="bold">T</mi>
                  </msup>
                  <mi mathvariant="bold">x</mi>
                </math> is the matrix multiplication of <math xmlns="http://www.w3.org/1998/Math/MathML">
                  <msup>
                    <mi mathvariant="bold">θ</mi>
                    <mi mathvariant="bold">T</mi>
                  </msup>
                </math> and <strong>x</strong>. It is of course the same prediction, except it is now represented as a
                single cell matrix rather than a scalar value. In this book we will use this notation to avoid switching
                between dot products and matrix multiplications.</p>
            </div>

            <p>Okay, that’s the Linear Regression model, so now how do we train it? Well, recall that training a model
              means setting its parameters so that the model best fits the training set. For this purpose, we first need
              a measure of how well (or poorly) the model fits the training data. In <a data-type="xref"
                href="ch02.xhtml#project_chapter">Chapter 2</a> we saw that the most common performance measure of a
              regression model is the <a data-type="indexterm" data-primary="Root Mean Square Error (RMSE)"
                id="idm139656380510384" />Root Mean Square Error (RMSE) (<a data-type="xref"
                href="ch02.xhtml#rmse_equation">Equation 2-1</a>). Therefore, to train a Linear Regression model, you
              need to find the value of <strong>θ</strong> that minimizes the RMSE. In practice, it is simpler to
              minimize the <a data-type="indexterm" data-primary="Mean Square Error (MSE)"
                id="idm139656380508432" />Mean Square Error (MSE) than the RMSE, and it leads to the same result
              (because the value that minimizes a function also minimizes its square root).<sup><a data-type="noteref"
                  id="idm139656380507440-marker" href="ch04.xhtml#idm139656380507440">1</a></sup></p>

            <p>The MSE of a Linear Regression hypothesis <em>h</em><sub><strong>θ</strong></sub> on a training set
              <strong>X</strong> is calculated using <a data-type="xref" href="#mse_cost_function">Equation 4-3</a>.</p>
            <div id="mse_cost_function" data-type="equation">
              <h5><span class="label">Equation 4-3. </span>MSE cost function for a Linear Regression model</h5><math
                xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow>
                  <mtext>MSE</mtext>
                  <mrow>
                    <mo>(</mo>
                    <mi mathvariant="bold">X</mi>
                    <mo>,</mo>
                    <msub>
                      <mi>h</mi>
                      <mi mathvariant="bold">θ</mi>
                    </msub>
                    <mo>)</mo>
                  </mrow>
                  <mo>=</mo>
                  <mstyle scriptlevel="0" displaystyle="true">
                    <mfrac>
                      <mn>1</mn>
                      <mi>m</mi>
                    </mfrac>
                  </mstyle>
                  <munderover>
                    <mo>∑</mo>
                    <mrow>
                      <mi>i</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mi>m</mi>
                  </munderover>
                  <msup>
                    <mrow>
                      <mo>(</mo>
                      <msup>
                        <mi mathvariant="bold">θ</mi>
                        <mi>T</mi>
                      </msup>
                      <msup>
                        <mi mathvariant="bold">x</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>i</mi>
                          <mo>)</mo>
                        </mrow>
                      </msup>
                      <mo>-</mo>
                      <msup>
                        <mi>y</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>i</mi>
                          <mo>)</mo>
                        </mrow>
                      </msup>
                      <mo>)</mo>
                    </mrow>
                    <mn>2</mn>
                  </msup>
                </mrow>
              </math>
            </div>

            <p>Most of these notations were presented in <a data-type="xref"
                href="ch02.xhtml#project_chapter">Chapter 2</a> (see <a data-type="xref"
                href="ch02.xhtml#notations">“Notations”</a>). The only difference is that we write
              <em>h</em><sub><strong>θ</strong></sub> instead of just <em>h</em> in order to make it clear that the
              model is parametrized by the vector <strong>θ</strong>. To simplify notations, we will just write
              MSE(<strong>θ</strong>) instead of MSE(<strong>X</strong>, <em>h</em><sub><strong>θ</strong></sub>).</p>








            <section data-type="sect2" data-pdf-bookmark="The Normal Equation">
              <div class="sect2" id="idm139656380479152">
                <h2>The Normal Equation</h2>

                <p>To <a data-type="indexterm" data-primary="Linear Regression" data-secondary="Normal Equation"
                    id="lr4ne" /><a data-type="indexterm" data-primary="Normal Equation" id="ne4" />find the value of
                  <strong>θ</strong> that minimizes the <a data-type="indexterm" data-primary="cost function"
                    data-secondary="in Linear Regression" data-secondary-sortas="Linear" id="idm139656380474176" />cost
                  function, there is a <em>closed-form solution</em>—in other words, a mathematical equation that gives
                  the result directly. This is called the <em>Normal Equation</em> (<a data-type="xref"
                    href="#equation_four_four">Equation 4-4</a>).<sup><a data-type="noteref"
                      id="idm139656380470992-marker" href="ch04.xhtml#idm139656380470992">2</a></sup></p>
                <div class="fifty-percent" id="equation_four_four" data-type="equation">
                  <h5><span class="label">Equation 4-4. </span>Normal Equation</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mover accent="true">
                        <mi mathvariant="bold">θ</mi>
                        <mo>^</mo>
                      </mover>
                      <mo>=</mo>
                      <msup>
                        <mrow>
                          <mo>(</mo>
                          <msup>
                            <mi mathvariant="bold">X</mi>
                            <mi>T</mi>
                          </msup>
                          <mi mathvariant="bold">X</mi>
                          <mo>)</mo>
                        </mrow>
                        <mrow>
                          <mo>-</mo>
                          <mn>1</mn>
                        </mrow>
                      </msup>
                      <mo> </mo>
                      <msup>
                        <mi mathvariant="bold">X</mi>
                        <mi>T</mi>
                      </msup>
                      <mo> </mo>
                      <mi mathvariant="bold">y</mi>
                    </mrow>
                  </math>
                </div>

                <ul>
                  <li>
                    <p><math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mover accent="true">
                          <mi mathvariant="bold">θ</mi>
                          <mo>^</mo>
                        </mover>
                      </math> is the value of <strong>θ</strong> that minimizes the cost function.</p>
                  </li>
                  <li>
                    <p><strong>y</strong> is the vector of target values containing <em>y</em><sup>(1)</sup> to
                      <em>y</em><sup>(<em>m</em>)</sup>.</p>
                  </li>
                </ul>

                <p>Let’s generate some linear-looking data to test this equation on (<a data-type="xref"
                    href="#generated_data_plot">Figure 4-1</a>):</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">import</code> <code class="nn">numpy</code> <code class="kn">as</code> <code class="nn">np</code>

<code class="n">X</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">y</code> <code class="o">=</code> <code class="mi">4</code> <code class="o">+</code> <code class="mi">3</code> <code class="o">*</code> <code class="n">X</code> <code class="o">+</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code></pre>

                <figure class="smallerseventyfive">
                  <div id="generated_data_plot" class="figure">
                    <img src="mlst_0401.png" alt="mlst 0401" width="1704" height="1084" />
                    <h6><span class="label">Figure 4-1. </span>Randomly generated linear dataset</h6>
                  </div>
                </figure>

                <p>Now let’s compute <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mover accent="true">
                      <mi mathvariant="bold">θ</mi>
                      <mo>^</mo>
                    </mover>
                  </math> using the Normal Equation. We will use the <code>inv()</code> function from NumPy’s Linear
                  Algebra module (<code>np.linalg</code>) to compute the inverse of a matrix, and the <code>dot()</code>
                  method for matrix multiplication:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">X_b</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">c_</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">((</code><code class="mi">100</code><code class="p">,</code> <code class="mi">1</code><code class="p">)),</code> <code class="n">X</code><code class="p">]</code>  <code class="c1"># add x0 = 1 to each instance</code>
<code class="n">theta_best</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">inv</code><code class="p">(</code><code class="n">X_b</code><code class="o">.</code><code class="n">T</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">X_b</code><code class="p">))</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">X_b</code><code class="o">.</code><code class="n">T</code><code class="p">)</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">y</code><code class="p">)</code></pre>

                <p>The actual function that we used to generate the data is <em>y</em> = 4 + 3<em>x</em><sub>1</sub> +
                  Gaussian noise. Let’s see what the equation found:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">theta_best</code>
<code class="go">array([[4.21509616],</code>
<code class="go">       [2.77011339]])</code></pre>

                <p>We would have hoped for <em>θ</em><sub>0</sub> = 4 and <em>θ</em><sub>1</sub> = 3 instead of
                  <em>θ</em><sub>0</sub> = 4.215 and <em>θ</em><sub>1</sub> = 2.770. Close enough, but the noise made it
                  impossible to recover the exact parameters of the original function.</p>

                <p>Now you can make predictions using <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mover accent="true">
                      <mi mathvariant="bold">θ</mi>
                      <mo>^</mo>
                    </mover>
                  </math>:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">array</code><code class="p">([[</code><code class="mi">0</code><code class="p">],</code> <code class="p">[</code><code class="mi">2</code><code class="p">]])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_new_b</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">c_</code><code class="p">[</code><code class="n">np</code><code class="o">.</code><code class="n">ones</code><code class="p">((</code><code class="mi">2</code><code class="p">,</code> <code class="mi">1</code><code class="p">)),</code> <code class="n">X_new</code><code class="p">]</code> <code class="c"># add x0 = 1 to each instance</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code> <code class="o">=</code> <code class="n">X_new_b</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">theta_best</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y_predict</code>
<code class="go">array([[4.21509616],</code>
<code class="go">       [9.75532293]])</code></pre>

                <p>Let’s plot this model’s predictions (<a data-type="xref"
                    href="#linear_model_predictions">Figure 4-2</a>):</p>

                <pre data-type="programlisting" data-code-language="python"><code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X_new</code><code class="p">,</code> <code class="n">y_predict</code><code class="p">,</code> <code class="s2">"r-"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="s2">"b."</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">axis</code><code class="p">([</code><code class="mi">0</code><code class="p">,</code> <code class="mi">2</code><code class="p">,</code> <code class="mi">0</code><code class="p">,</code> <code class="mi">15</code><code class="p">])</code>
<code class="n">plt</code><code class="o">.</code><code class="n">show</code><code class="p">()</code></pre>

                <figure class="smallerseventy">
                  <div id="linear_model_predictions" class="figure">
                    <img src="mlst_04in01.png" alt="mlst 04in01" width="1704" height="1084" />
                    <h6><span class="label">Figure 4-2. </span>Linear Regression model predictions</h6>
                  </div>
                </figure>

                <p>Performing linear regression <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.linear_model.LinearRegression" id="idm139656380086016" />using Scikit-Learn
                  is quite simple:<sup><a data-type="noteref" id="idm139656380084896-marker"
                      href="ch04.xhtml#idm139656380084896">3</a></sup></p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LinearRegression</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lin_reg</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lin_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lin_reg</code><code class="o">.</code><code class="n">intercept_</code><code class="p">,</code> <code class="n">lin_reg</code><code class="o">.</code><code class="n">coef_</code>
<code class="go">(array([4.21509616]), array([[2.77011339]]))</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lin_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="go">array([[4.21509616],</code>
<code class="go">       [9.75532293]])</code></pre>

                <p>The <code>LinearRegression</code> class is based on the <code>scipy.linalg.lstsq()</code> function
                  (the name stands for “least squares”), which you could call directly:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">theta_best_svd</code><code class="p">,</code> <code class="n">residuals</code><code class="p">,</code> <code class="n">rank</code><code class="p">,</code> <code class="n">s</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">lstsq</code><code class="p">(</code><code class="n">X_b</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">rcond</code><code class="o">=</code><code class="mf">1e-6</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">theta_best_svd</code>
<code class="go">array([[4.21509616],</code>
<code class="go">       [2.77011339]])</code></pre>

                <p>This function computes <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <mover accent="true">
                      <mi mathvariant="bold">θ</mi>
                      <mo>^</mo>
                    </mover>
                    <mo>=</mo>
                    <msup>
                      <mi mathvariant="bold">X</mi>
                      <mo>+</mo>
                    </msup>
                    <mi mathvariant="bold">y</mi>
                  </math>, where <math xmlns="http://www.w3.org/1998/Math/MathML"
                    alttext="bold upper X Superscript plus">
                    <msup>
                      <mi>𝐗</mi>
                      <mo>+</mo>
                    </msup>
                  </math> is the <em>pseudoinverse</em> of <strong>X</strong> (specifically the Moore-Penrose inverse).
                  You can use <code>np.linalg.pinv()</code> to compute the pseudoinverse directly:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">pinv</code><code class="p">(</code><code class="n">X_b</code><code class="p">)</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">y</code><code class="p">)</code>
<code class="go">array([[4.21509616],</code>
<code class="go">       [2.77011339]])</code></pre>

                <p>The pseudoinverse itself is computed using a standard matrix factorization technique <a
                    data-type="indexterm" data-primary="Singular Value Decomposition (SVD)"
                    id="idm139656379949936" />called <em>Singular Value Decomposition</em> (SVD) that can decompose the
                  training set matrix <strong>X</strong> into the matrix multiplication of three matrices
                  <strong>U</strong> <strong>Σ</strong> <strong>V</strong><sup><em>T</em></sup> (see
                  <code>numpy.linalg.svd()</code>). The pseudoinverse is computed as <math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <msup>
                      <mi mathvariant="bold">X</mi>
                      <mo>+</mo>
                    </msup>
                    <mo>=</mo>
                    <mi mathvariant="bold">V</mi>
                    <msup>
                      <mi mathvariant="bold">Σ</mi>
                      <mo>+</mo>
                    </msup>
                    <msup>
                      <mi mathvariant="bold">U</mi>
                      <mi mathvariant="normal">T</mi>
                    </msup>
                  </math>. To compute the matrix <math xmlns="http://www.w3.org/1998/Math/MathML">
                    <msup>
                      <mi mathvariant="bold">Σ</mi>
                      <mo>+</mo>
                    </msup>
                  </math>, the algorithm takes <strong>Σ</strong> and sets to zero all values smaller than a tiny
                  threshold value, then it replaces all the non-zero values with their inverse, and finally it
                  transposes the resulting matrix. This approach is more efficient than computing the Normal Equation,
                  plus it handles edge cases nicely: indeed, the Normal Equation may not work if the matrix
                  <strong>X</strong><sup>T</sup><strong>X</strong> is not invertible (i.e., singular), such as if
                  <em>m</em> &lt; <em>n</em> or if some features are redundant, but the pseudoinverse is always defined.
                </p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Computational Complexity">
              <div class="sect2" id="idm139656380478336">
                <h2>Computational Complexity</h2>

                <p>The <a data-type="indexterm" data-primary="Linear Regression"
                    data-secondary="computational complexity" id="idm139656379939840" /><a data-type="indexterm"
                    data-primary="computational complexity" id="idm139656379938480" />Normal Equation computes the
                  inverse of <strong>X</strong><sup><em>T</em></sup> <strong>X</strong>, which is an (<em>n</em> + 1) ×
                  (<em>n</em> + 1) matrix (where <em>n</em> is the number of features). The <em>computational
                    complexity</em> of inverting such a matrix is typically about <em>O</em>(<em>n</em><sup>2.4</sup>)
                  to <em>O</em>(<em>n</em><sup>3</sup>) (depending on the implementation). In other words, if you double
                  the number of features, you multiply the computation time by roughly 2<sup>2.4</sup> = 5.3 to
                  2<sup>3</sup> = 8.</p>

                <p>The SVD approach used by Scikit-Learn’s <code>LinearRegression</code> class is about
                  <em>O</em>(<em>n</em><sup>2</sup>). If you double the number of features, you multiply the computation
                  time by roughly 4.</p>
                <div data-type="warning" epub:type="warning">
                  <h6>Warning</h6>
                  <p>Both the Normal Equation and the SVD approach get very slow when the number of features grows large
                    (e.g., 100,000). On the positive side, both are linear with regards to the number of instances in
                    the training set (they are <em>O</em>(<em>m</em>)), so they handle large training sets efficiently,
                    provided they can fit in memory.</p>
                </div>

                <p>Also, once you have trained your Linear Regression model (using the Normal Equation or any other
                  algorithm), predictions are very fast: the computational complexity is linear with regards to both the
                  number of instances you want to make predictions on and the number of features. In other words, making
                  predictions on twice as many instances (or twice as many features) will just take roughly twice as
                  much <a data-type="indexterm" data-primary="Linear Regression" data-secondary="Normal Equation"
                    data-startref="lr4ne" id="idm139656379910400" /><a data-type="indexterm"
                    data-primary="Normal Equation" data-startref="ne4" id="idm139656379909152" />time.</p>

                <p>Now we will look at very different ways to train a Linear Regression model, better suited for cases
                  where there are a large number of features, or too many training instances to fit in memory.</p>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Gradient Descent">
          <div class="sect1" id="idm139656380647680">
            <h1>Gradient Descent</h1>

            <p><em>Gradient Descent</em> is a <a data-type="indexterm" data-primary="Linear Regression"
                data-secondary="Gradient Descent in" id="lr4gdi" /><a data-type="indexterm"
                data-primary="Gradient Descent (GD)" id="gd4" />very generic optimization algorithm <a
                data-type="indexterm" data-primary="Gradient Descent (GD)" data-secondary="defining"
                id="idm139656379902800" />capable of finding optimal solutions to a wide range of problems. The general
              idea of Gradient Descent is to tweak parameters iteratively in order to minimize a <a
                data-type="indexterm" data-primary="cost function" data-secondary="in Gradient Descent"
                data-secondary-sortas="Gradient" id="cf4igd" />cost function.</p>

            <p>Suppose you are lost in the mountains in a dense fog; you can only feel the slope of the ground below
              your feet. A good strategy to get to the bottom of the valley quickly is to go downhill in the direction
              of the steepest slope. This is exactly what Gradient Descent does: it measures the local gradient of the
              error function with regards to the <a data-type="indexterm" data-primary="parameter vector"
                id="idm139656379899152" />parameter vector <strong>θ</strong>, and it goes in the direction of
              descending gradient. Once the gradient is zero, you have reached a minimum!</p>

            <p>Concretely, you start by filling <strong>θ</strong> with random values (this is called <em>random
                initialization</em>), <a data-type="indexterm" data-primary="random initialization"
                id="idm139656379896304" />and then you improve it gradually, taking one baby step at a time, each step
              attempting to decrease the cost function (e.g., the MSE), until the algorithm <em>converges</em> to a
              minimum (see <a data-type="xref" href="#gradient_descent_diagram">Figure 4-3</a>).</p>

            <figure class="smallerseventyfive">
              <div id="gradient_descent_diagram" class="figure">
                <img src="mlst_0402.png" alt="mlst 0402" width="1635" height="1017" />
                <h6><span class="label">Figure 4-3. </span>Gradient Descent</h6>
              </div>
            </figure>

            <p>An important parameter in Gradient Descent is the size of the steps, determined by <a
                data-type="indexterm" data-primary="learning rate" id="idm139656379891520" /><a data-type="indexterm"
                data-primary="hyperparameters" id="idm139656379890816" />the <em>learning rate</em> hyperparameter. If
              the learning rate is too small, then the algorithm will have to go through many iterations to converge,
              which will take a long time (see <a data-type="xref" href="#small_learning_rate_diagram">Figure 4-4</a>).
            </p>

            <figure class="smallerseventyfive">
              <div id="small_learning_rate_diagram" class="figure">
                <img src="mlst_0403.png" alt="mlst 0403" width="1635" height="872" />
                <h6><span class="label">Figure 4-4. </span>Learning rate too small</h6>
              </div>
            </figure>

            <p>On the other hand, if the learning rate is too high, you might jump across the valley and end up on the
              other side, possibly even higher up than you were before. This might make the algorithm diverge, with
              larger and larger values, failing to find a good solution (see <a data-type="xref"
                href="#large_learning_rate_diagram">Figure 4-5</a>).</p>

            <figure class="smallerseventyfive">
              <div id="large_learning_rate_diagram" class="figure">
                <img src="mlst_0404.png" alt="mlst 0404" width="1635" height="880" />
                <h6><span class="label">Figure 4-5. </span>Learning rate too large</h6>
              </div>
            </figure>

            <p>Finally, not all cost functions look like nice regular bowls. There may be holes, ridges, plateaus, and
              all sorts of irregular terrains, making convergence to the minimum very difficult. <a data-type="xref"
                href="#gradient_descent_pitfalls_diagram">Figure 4-6</a> shows the two main challenges with Gradient
              Descent: if the random initialization starts the algorithm on the left, then it will converge to a
              <em>local minimum</em>, which <a data-type="indexterm" data-primary="Gradient Descent (GD)"
                data-secondary="local minimum versus global minimum" id="idm139656380169408" />is not as good as the
              <em>global minimum</em>. If it starts on the right, then it will take a very long time to cross the
              plateau, and if you stop too early you will never reach the global <a data-type="indexterm"
                data-primary="cost function" data-secondary="in Gradient Descent" data-secondary-sortas="Gradient"
                data-startref="cf4igd" id="idm139656380167680" />minimum.</p>

            <figure class="smallerseventyfive">
              <div id="gradient_descent_pitfalls_diagram" class="figure">
                <img src="mlst_0405.png" alt="mlst 0405" width="1635" height="936" />
                <h6><span class="label">Figure 4-6. </span>Gradient Descent pitfalls</h6>
              </div>
            </figure>

            <p>Fortunately, the MSE cost function for a <a data-type="indexterm" data-primary="cost function"
                data-secondary="in Linear Regression" data-secondary-sortas="Linear" id="idm139656380163312" />Linear
              Regression model happens to be a <em>convex function</em>, <a data-type="indexterm"
                data-primary="convex function" id="idm139656380161488" />which means that if you pick any two points on
              the curve, the line segment joining them never crosses the curve. This implies that there are no local
              minima, just one global minimum. It is also a continuous function with a slope that never changes
              abruptly.<sup><a data-type="noteref" id="idm139656380160352-marker"
                  href="ch04.xhtml#idm139656380160352">4</a></sup> <a data-type="indexterm"
                data-primary="Lipschitz continuous" id="idm139656380159216" />These two facts have a great consequence:
              Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough
              and if the learning rate is not too high).</p>

            <p>In fact, the cost function has the shape of a bowl, but it can be an elongated bowl if the features have
              very different scales. <a data-type="xref" href="#elongated_bowl_diagram">Figure 4-7</a> shows Gradient
              Descent on a training set where features 1 and 2 have the same scale (on the left), and on a training set
              where feature 1 has much smaller values than feature 2 (on the right).<sup><a data-type="noteref"
                  id="idm139656380156704-marker" href="ch04.xhtml#idm139656380156704">5</a></sup></p>

            <figure class="smallerseventyfive">
              <div id="elongated_bowl_diagram" class="figure">
                <img src="mlst_0406.png" alt="mlst 0406" width="2804" height="1022" />
                <h6><span class="label">Figure 4-7. </span>Gradient Descent with and without feature scaling</h6>
              </div>
            </figure>

            <p>As you can see, on the left the Gradient Descent algorithm goes straight toward the minimum, thereby
              reaching it quickly, whereas on the right it first goes in a direction almost orthogonal to the direction
              of the global minimum, and it ends with a long march down an almost flat valley. It will eventually reach
              the minimum, but it will take a long time.</p>
            <div data-type="warning" epub:type="warning">
              <h6>Warning</h6>
              <p>When using Gradient Descent, you should ensure that all features have a similar scale (e.g., using
                Scikit-Learn’s <code>StandardScaler</code> class), or <a data-type="indexterm"
                  data-primary="Scikit-Learn" data-secondary="StandardScaler" id="idm139656380150032" /><a
                  data-type="indexterm" data-primary="Scikit-Learn"
                  data-secondary="sklearn.preprocessing.StandardScaler" id="idm139656380149024" />else it will take much
                longer to converge.</p>
            </div>

            <p>This diagram also illustrates the fact that training a model means searching for a combination of model
              parameters <a data-type="indexterm" data-primary="model parameters" id="idm139656380147184" />that
              minimizes a cost function (over the training set). It is a search in the model’s <em>parameter space</em>:
              the more <a data-type="indexterm" data-primary="parameter space" id="idm139656380145856" />parameters a
              model has, the more dimensions this space has, and the harder the search is: searching for a needle in a
              300-dimensional haystack is much trickier than in three dimensions. Fortunately, since the cost function
              is convex in the case of Linear Regression, the needle is simply at the bottom of the bowl.</p>








            <section data-type="sect2" data-pdf-bookmark="Batch Gradient Descent">
              <div class="sect2" id="idm139656380144528">
                <h2>Batch Gradient Descent</h2>

                <p>To <a data-type="indexterm" data-primary="Gradient Descent (GD)" data-secondary="Batch GD"
                    id="gd4bgd" /><a data-type="indexterm" data-primary="Batch Gradient Descent" id="bgd4" />implement
                  Gradient Descent, you need to compute the gradient of the cost function <a data-type="indexterm"
                    data-primary="cost function" data-secondary="in Gradient Descent" data-secondary-sortas="Gradient"
                    id="idm139656380140160" />with regards to each model parameter <em>θ</em><sub><em>j</em></sub>. In
                  other words, you need to calculate how much the cost function will change if you change
                  <em>θ</em><sub><em>j</em></sub> just a little bit. This is called <a data-type="indexterm"
                    data-primary="partial derivative" id="idm139656380137040" />a <em>partial derivative</em>. It is
                  like asking “what is the slope of the mountain under my feet if I face east?” and then asking the same
                  question facing north (and so on for all other dimensions, if you can imagine a universe with more
                  than three dimensions). <a data-type="xref" href="#mse_partial_derivatives">Equation 4-5</a> computes
                  the partial derivative of the cost function with regards to parameter <em>θ</em><sub><em>j</em></sub>,
                  noted <math xmlns="http://www.w3.org/1998/Math/MathML"
                    alttext="StartFraction normal partial-differential Over normal partial-differential theta Subscript j Baseline EndFraction">
                    <mfrac>
                      <mi>∂</mi>
                      <mrow>
                        <mi>∂</mi>
                        <msub>
                          <mi>θ</mi>
                          <mi>j</mi>
                        </msub>
                      </mrow>
                    </mfrac>
                  </math> MSE(<strong>θ</strong>).</p>
                <div id="mse_partial_derivatives" data-type="equation">
                  <h5><span class="label">Equation 4-5. </span>Partial derivatives of the cost function</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mi>∂</mi>
                          <mrow>
                            <mi>∂</mi>
                            <msub>
                              <mi>θ</mi>
                              <mi>j</mi>
                            </msub>
                          </mrow>
                        </mfrac>
                      </mstyle>
                      <mtext>MSE</mtext>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">θ</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mn>2</mn>
                          <mi>m</mi>
                        </mfrac>
                      </mstyle>
                      <munderover>
                        <mo>∑</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>m</mi>
                      </munderover>
                      <mrow>
                        <mo>(</mo>
                        <msup>
                          <mi mathvariant="bold">θ</mi>
                          <mi>T</mi>
                        </msup>
                        <msup>
                          <mi mathvariant="bold">x</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>i</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                        <mo>-</mo>
                        <msup>
                          <mi>y</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>i</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                        <mo>)</mo>
                      </mrow>
                      <mspace width="0.166667em" />
                      <msubsup>
                        <mi>x</mi>
                        <mi>j</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>i</mi>
                          <mo>)</mo>
                        </mrow>
                      </msubsup>
                    </mrow>
                  </math>
                </div>

                <p>Instead of computing these partial derivatives individually, you can use <a data-type="xref"
                    href="#mse_gradient_vector">Equation 4-6</a> to compute them all in one go. The gradient vector,
                  noted ∇<sub><strong>θ</strong></sub>MSE(<strong>θ</strong>), contains all the partial derivatives of
                  the cost function (one for each model parameter).</p>
                <div class="pagebreak-before less_space" id="mse_gradient_vector" data-type="equation">
                  <h5><span class="label">Equation 4-6. </span>Gradient vector of the cost function</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <msub>
                        <mi>∇</mi>
                        <mi mathvariant="bold">θ</mi>
                      </msub>
                      <mspace width="0.166667em" />
                      <mtext>MSE</mtext>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">θ</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <mfenced open="(" close=")">
                        <mtable>
                          <mtr>
                            <mtd>
                              <mrow>
                                <mfrac>
                                  <mi>∂</mi>
                                  <mrow>
                                    <mi>∂</mi>
                                    <msub>
                                      <mi>θ</mi>
                                      <mn>0</mn>
                                    </msub>
                                  </mrow>
                                </mfrac>
                                <mtext>MSE</mtext>
                                <mrow>
                                  <mo>(</mo>
                                  <mi mathvariant="bold">θ</mi>
                                  <mo>)</mo>
                                </mrow>
                              </mrow>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mrow>
                                <mfrac>
                                  <mi>∂</mi>
                                  <mrow>
                                    <mi>∂</mi>
                                    <msub>
                                      <mi>θ</mi>
                                      <mn>1</mn>
                                    </msub>
                                  </mrow>
                                </mfrac>
                                <mtext>MSE</mtext>
                                <mrow>
                                  <mo>(</mo>
                                  <mi mathvariant="bold">θ</mi>
                                  <mo>)</mo>
                                </mrow>
                              </mrow>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mo>⋮</mo>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mrow>
                                <mfrac>
                                  <mi>∂</mi>
                                  <mrow>
                                    <mi>∂</mi>
                                    <msub>
                                      <mi>θ</mi>
                                      <mi>n</mi>
                                    </msub>
                                  </mrow>
                                </mfrac>
                                <mtext>MSE</mtext>
                                <mrow>
                                  <mo>(</mo>
                                  <mi mathvariant="bold">θ</mi>
                                  <mo>)</mo>
                                </mrow>
                              </mrow>
                            </mtd>
                          </mtr>
                        </mtable>
                      </mfenced>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mn>2</mn>
                          <mi>m</mi>
                        </mfrac>
                      </mstyle>
                      <msup>
                        <mi mathvariant="bold">X</mi>
                        <mi>T</mi>
                      </msup>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">X</mi>
                        <mi mathvariant="bold">θ</mi>
                        <mo>-</mo>
                        <mi mathvariant="bold">y</mi>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </math>
                </div>
                <div data-type="warning" epub:type="warning">
                  <h6>Warning</h6>
                  <p>Notice that this formula involves calculations over the full training set <strong>X</strong>, at
                    each Gradient Descent step! This is why the algorithm is called <em>Batch Gradient Descent</em>: it
                    uses the whole batch of training data at every step. As a result it is terribly slow on very large
                    training sets (but we will see much faster Gradient Descent algorithms shortly). However, Gradient
                    Descent scales well with the number of features; training a Linear Regression model when there are
                    hundreds of thousands of features is much faster using Gradient Descent than using the Normal
                    Equation or SVD decomposition.</p>
                </div>

                <p>Once you have the gradient vector, which points uphill, just go in the opposite direction to go
                  downhill. This means subtracting ∇<sub><strong>θ</strong></sub>MSE(<strong>θ</strong>) from
                  <strong>θ</strong>. This is where the <a data-type="indexterm" data-primary="learning rate"
                    id="lrate4" />learning rate <em>η</em> comes into play:<sup><a data-type="noteref"
                      id="idm139656379801232-marker" href="ch04.xhtml#idm139656379801232">6</a></sup> multiply the
                  gradient vector by <em>η</em> to determine the size of the downhill step (<a data-type="xref"
                    href="#gradient_descent_step">Equation 4-7</a>).</p>
                <div id="gradient_descent_step" data-type="equation">
                  <h5><span class="label">Equation 4-7. </span>Gradient Descent step</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <msup>
                      <mi mathvariant="bold">θ</mi>
                      <mrow>
                        <mo>(</mo>
                        <mtext>next step</mtext>
                        <mo>)</mo>
                      </mrow>
                    </msup>
                    <mo>=</mo>
                    <mi mathvariant="bold">θ</mi>
                    <mo>-</mo>
                    <mi>η</mi>
                    <msub>
                      <mo>∇</mo>
                      <mi mathvariant="bold">θ</mi>
                    </msub>
                    <mo> </mo>
                    <mtext>MSE</mtext>
                    <mo>(</mo>
                    <mi mathvariant="bold">θ</mi>
                    <mo>)</mo>
                  </math>
                </div>

                <p>Let’s look at a quick implementation of this algorithm:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">eta</code> <code class="o">=</code> <code class="mf">0.1</code>  <code class="c1"># learning rate</code>
<code class="n">n_iterations</code> <code class="o">=</code> <code class="mi">1000</code>
<code class="n">m</code> <code class="o">=</code> <code class="mi">100</code>

<code class="n">theta</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># random initialization</code>

<code class="k">for</code> <code class="n">iteration</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_iterations</code><code class="p">):</code>
    <code class="n">gradients</code> <code class="o">=</code> <code class="mi">2</code><code class="o">/</code><code class="n">m</code> <code class="o">*</code> <code class="n">X_b</code><code class="o">.</code><code class="n">T</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">X_b</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">theta</code><code class="p">)</code> <code class="o">-</code> <code class="n">y</code><code class="p">)</code>
    <code class="n">theta</code> <code class="o">=</code> <code class="n">theta</code> <code class="o">-</code> <code class="n">eta</code> <code class="o">*</code> <code class="n">gradients</code></pre>

                <p class="pagebreak-before">That wasn’t too hard! Let’s <a data-type="indexterm"
                    data-primary="random initialization" id="idm139656379788656" />look at the resulting
                  <code>theta</code>:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">theta</code>
<code class="go">array([[4.21509616],</code>
<code class="go">       [2.77011339]])</code></pre>

                <p>Hey, that’s exactly what the Normal Equation found! Gradient Descent worked perfectly. But what if
                  you had used a different learning rate <code>eta</code>? <a data-type="xref"
                    href="#gradient_descent_plot">Figure 4-8</a> shows the first 10 steps of Gradient Descent using
                  three different learning rates (the dashed line represents the starting point).</p>

                <figure>
                  <div id="gradient_descent_plot" class="figure">
                    <img src="mlst_0407.png" alt="mlst 0407" width="2904" height="1090" />
                    <h6><span class="label">Figure 4-8. </span>Gradient Descent with various learning rates</h6>
                  </div>
                </figure>

                <p>On the left, the learning rate is too low: the algorithm will eventually reach the solution, but it
                  will take a long time. In the middle, the learning rate looks pretty good: in just a few iterations,
                  it has already converged to the solution. On the right, the learning rate is too high: the algorithm
                  diverges, jumping all over the place and actually getting further and further away from the solution
                  at every step.</p>

                <p>To find a good learning rate, you can use grid search (see <a data-type="xref"
                    href="ch02.xhtml#project_chapter">Chapter 2</a>). However, you may want to limit the number of
                  iterations so that grid search can eliminate models that take too long to converge.</p>

                <p>You may wonder how to set the number of iterations. If it is too low, you will still be far away from
                  the optimal solution when the algorithm stops, but if it is too high, you will waste time while the
                  model parameters <a data-type="indexterm" data-primary="model parameters" id="idm139656379718960" />do
                  not change anymore. A simple solution is to set a very large number of iterations but to interrupt the
                  algorithm when the gradient vector becomes tiny—that is, when its norm becomes smaller than a tiny
                  number <em>ϵ</em> (called the <em>tolerance</em>)—because this happens when Gradient Descent has
                  (almost) reached the minimum.</p>
                <aside data-type="sidebar" epub:type="sidebar" class="pagebreak-before less_space">
                  <div class="sidebar" id="idm139656379716944">
                    <h5>Convergence Rate</h5>
                    <p>When the <a data-type="indexterm" data-primary="convergence rate" id="idm139656379715152" />cost
                      function is convex and its slope does not change abruptly (as is the case for the MSE cost
                      function), <a data-type="indexterm" data-primary="cost function"
                        data-secondary="in Gradient Descent" data-secondary-sortas="Gradient" id="cf4igdxx" />Batch
                      Gradient Descent with a fixed learning rate will eventually converge to the optimal solution, but
                      you may have to wait a while: it can take O(1/<em>ϵ</em>) iterations to reach the optimum within a
                      range of <em>ϵ</em> depending on the shape of the cost function. If you divide the tolerance by 10
                      to have a more precise solution, then the algorithm may have to run about 10 times <a
                        data-type="indexterm" data-primary="Gradient Descent (GD)" data-secondary="Batch GD"
                        data-startref="gd4bgd" id="idm139656379711408" /><a data-type="indexterm"
                        data-primary="Batch Gradient Descent" data-startref="bgd4" id="idm139656379710160" />longer.</p>
                  </div>
                </aside>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Stochastic Gradient Descent">
              <div class="sect2" id="idm139656380143936">
                <h2>Stochastic Gradient Descent</h2>

                <p>The <a data-type="indexterm" data-primary="Gradient Descent (GD)" data-secondary="Stochastic GD"
                    id="gd4sgd" /><a data-type="indexterm" data-primary="Stochastic Gradient Descent (SGD)"
                    id="sgd4" />main problem with Batch Gradient Descent is the fact that it uses the whole training set
                  to compute the gradients at every step, which makes it very slow when the training set is large. At
                  the opposite extreme, <em>Stochastic Gradient Descent</em> just picks a random instance in the
                  training set at every step and computes the gradients based only on that single instance. Obviously
                  this makes the algorithm much faster since it has very little data to manipulate at every iteration.
                  It also makes it possible to train on huge training sets, since only one instance needs to be in
                  memory at each iteration (SGD can be implemented as an out-of-core algorithm.<sup><a
                      data-type="noteref" id="idm139656379704064-marker"
                      href="ch04.xhtml#idm139656379704064">7</a></sup>)</p>

                <p>On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular
                  than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost
                  function will bounce up and down, decreasing only on average. Over time it will end up very close to
                  the minimum, but once it gets there it will continue to bounce around, never settling down (see <a
                    data-type="xref" href="#sgd_random_walk_diagram">Figure 4-9</a>). So once the algorithm stops, the
                  final parameter values are good, but not optimal.</p>

                <figure class="smallerfifty">
                  <div id="sgd_random_walk_diagram" class="figure">
                    <img src="mlst_0408.png" alt="mlst 0408" width="1349" height="1011" />
                    <h6><span class="label">Figure 4-9. </span>Stochastic Gradient Descent</h6>
                  </div>
                </figure>

                <p>When the cost function is very irregular (as in <a data-type="xref"
                    href="#gradient_descent_pitfalls_diagram">Figure 4-6</a>), this can actually help the algorithm jump
                  out of local minima, so Stochastic Gradient Descent has a better chance of finding the global minimum
                  than Batch Gradient Descent does.</p>

                <p>Therefore randomness is good to escape from local optima, but bad because it means that the algorithm
                  can never settle at the minimum. One solution to this dilemma is to gradually reduce the <a
                    data-type="indexterm" data-primary="learning rate" data-startref="lrate4"
                    id="idm139656379696304" />learning rate. The steps start out large (which helps make quick progress
                  and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global
                  minimum. This process is akin to<a data-type="indexterm" data-primary="simulated annealing"
                    id="idm139656379694976" /> <em>simulated annealing</em>, an algorithm inspired from the process of
                  annealing in metallurgy where molten metal is slowly cooled down. The function that determines the
                  learning rate at each iteration is called the <em>learning schedule</em>. <a data-type="indexterm"
                    data-primary="learning rate scheduling" id="idm139656379693136" />If the learning rate is reduced
                  too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. If
                  the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up
                  with a suboptimal solution if you halt training too early.</p>

                <p>This code implements Stochastic Gradient Descent using a <a data-type="indexterm"
                    data-primary="random initialization" id="idm139656379691744" />simple learning schedule:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">n_epochs</code> <code class="o">=</code> <code class="mi">50</code>
<code class="n">t0</code><code class="p">,</code> <code class="n">t1</code> <code class="o">=</code> <code class="mi">5</code><code class="p">,</code> <code class="mi">50</code>  <code class="c1"># learning schedule hyperparameters</code>

<code class="k">def</code> <code class="nf">learning_schedule</code><code class="p">(</code><code class="n">t</code><code class="p">):</code>
    <code class="k">return</code> <code class="n">t0</code> <code class="o">/</code> <code class="p">(</code><code class="n">t</code> <code class="o">+</code> <code class="n">t1</code><code class="p">)</code>

<code class="n">theta</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="mi">2</code><code class="p">,</code><code class="mi">1</code><code class="p">)</code>  <code class="c1"># random initialization</code>

<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">n_epochs</code><code class="p">):</code>
    <code class="k">for</code> <code class="n">i</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="n">m</code><code class="p">):</code>
        <code class="n">random_index</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randint</code><code class="p">(</code><code class="n">m</code><code class="p">)</code>
        <code class="n">xi</code> <code class="o">=</code> <code class="n">X_b</code><code class="p">[</code><code class="n">random_index</code><code class="p">:</code><code class="n">random_index</code><code class="o">+</code><code class="mi">1</code><code class="p">]</code>
        <code class="n">yi</code> <code class="o">=</code> <code class="n">y</code><code class="p">[</code><code class="n">random_index</code><code class="p">:</code><code class="n">random_index</code><code class="o">+</code><code class="mi">1</code><code class="p">]</code>
        <code class="n">gradients</code> <code class="o">=</code> <code class="mi">2</code> <code class="o">*</code> <code class="n">xi</code><code class="o">.</code><code class="n">T</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">xi</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">theta</code><code class="p">)</code> <code class="o">-</code> <code class="n">yi</code><code class="p">)</code>
        <code class="n">eta</code> <code class="o">=</code> <code class="n">learning_schedule</code><code class="p">(</code><code class="n">epoch</code> <code class="o">*</code> <code class="n">m</code> <code class="o">+</code> <code class="n">i</code><code class="p">)</code>
        <code class="n">theta</code> <code class="o">=</code> <code class="n">theta</code> <code class="o">-</code> <code class="n">eta</code> <code class="o">*</code> <code class="n">gradients</code></pre>

                <p>By convention we iterate by rounds of <em>m</em> iterations; each round is called an <em>epoch</em>.
                  <a data-type="indexterm" data-primary="epochs" id="idm139656379687888" />While the Batch Gradient
                  Descent code iterated 1,000 times through the whole training set, this code goes through the training
                  set only 50 times and reaches a fairly good solution:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">theta</code>
<code class="go">array([[4.21076011],</code>
<code class="go">       [2.74856079]])</code></pre>

                <p><a data-type="xref" href="#sgd_plot">Figure 4-10</a> shows the first 20 steps of training (notice how
                  irregular the steps are).</p>

                <figure class="smallerseventyfive">
                  <div id="sgd_plot" class="figure">
                    <img src="mlst_0409.png" alt="mlst 0409" width="1704" height="1084" />
                    <h6><span class="label">Figure 4-10. </span>Stochastic Gradient Descent first 20 steps</h6>
                  </div>
                </figure>

                <p>Note that since instances are picked randomly, some instances may be picked several times per epoch
                  while others may not be picked at all. If you want to be sure that the algorithm goes through every
                  instance at each epoch, another approach is to shuffle the training set, then go through it instance
                  by instance, then shuffle it again, and so on. However, this generally converges more slowly.</p>

                <p>To perform <a data-type="indexterm" data-primary="Linear Regression"
                    data-secondary="using Stochastic Gradient Descent (SGD)" id="idm139656379436672" />Linear Regression
                  using SGD with <a data-type="indexterm" data-primary="Scikit-Learn" data-secondary="SGDRegressor"
                    id="idm139656379435568" /><a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.linear_model.SGDRegressor" id="skllmsgdrch4" />Scikit-Learn, you can use the
                  <code>SGDRegressor</code> class, which defaults to optimizing the squared error cost function. The
                  following code runs for maximum 1000 epochs (<code>max_iter=1000</code>) or until the loss drops by
                  less than 1e-3 during one epoch (<code>tol=1e-3</code>), starting with a learning rate of 0.1
                  (<code>eta0=0.1</code>), using the default learning schedule (different from the preceding one), and
                  it does not use any <a data-type="indexterm" data-primary="cost function"
                    data-secondary="in Gradient Descent" data-secondary-sortas="Gradient" data-startref="cf4igdxx"
                    id="idm139656379431376" />regularization (<code>penalty=None</code>; more details on this shortly):
                </p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">SGDRegressor</code>
<code class="n">sgd_reg</code> <code class="o">=</code> <code class="n">SGDRegressor</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">1000</code><code class="p">,</code> <code class="n">tol</code><code class="o">=</code><code class="mf">1e-3</code><code class="p">,</code> <code class="n">penalty</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">eta0</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
<code class="n">sgd_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="o">.</code><code class="n">ravel</code><code class="p">())</code></pre>

                <p>Once again, you find a solution quite close to the one returned by the <a data-type="indexterm"
                    data-primary="Gradient Descent (GD)" data-secondary="Stochastic GD" data-startref="gd4sgd"
                    id="idm139656379427824" /><a data-type="indexterm" data-primary="Stochastic Gradient Descent (SGD)"
                    data-startref="sgd4" id="idm139656379304720" />Normal Equation:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">sgd_reg</code><code class="o">.</code><code class="n">intercept_</code><code class="p">,</code> <code class="n">sgd_reg</code><code class="o">.</code><code class="n">coef_</code>
<code class="go">(array([4.24365286]), array([2.8250878]))</code></pre>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Mini-batch Gradient Descent">
              <div class="sect2" id="idm139656379708624">
                <h2>Mini-batch Gradient Descent</h2>

                <p>The <a data-type="indexterm" data-primary="Mini-batch Gradient Descent" id="mbgd4" /><a
                    data-type="indexterm" data-primary="Gradient Descent (GD)" data-secondary="Mini-batch GD"
                    id="gd4mbgd" /><a data-type="indexterm" data-primary="Gradient Descent (GD)"
                    data-secondary="algorithm comparisons" id="gd4ac" />last Gradient Descent algorithm we will look at
                  is called <em>Mini-batch Gradient Descent</em>. It is quite simple to understand once you know Batch
                  and Stochastic Gradient Descent: at each step, instead of computing the gradients based on the full
                  training set (as in Batch GD) or based on just one instance (as in Stochastic GD), Mini-batch GD
                  computes the gradients on small random sets of instances called <em>mini-batches</em>. The main
                  advantage of Mini-batch GD over Stochastic GD is that you can get a performance boost from hardware
                  optimization of matrix operations, especially when using GPUs.</p>

                <p>The algorithm’s progress in parameter space is less erratic than with SGD, especially with fairly
                  large mini-batches. As a result, Mini-batch GD will end up walking around a bit closer to the minimum
                  than SGD. But, on the other hand, it may be harder for it to escape from local minima (in the case of
                  problems that suffer from local minima, unlike Linear Regression as we saw earlier). <a
                    data-type="xref" href="#gradient_descent_paths_plot">Figure 4-11</a> shows the paths taken by the
                  three Gradient Descent algorithms in parameter space during training. They all end up near the
                  minimum, but Batch GD’s path actually stops at the minimum, while both Stochastic GD and Mini-batch GD
                  continue to walk around. However, don’t forget that Batch GD takes a lot of time to take each step,
                  and Stochastic GD and Mini-batch GD would also reach the minimum if you used a good learning schedule.
                </p>

                <figure class="smallerseventyfive">
                  <div id="gradient_descent_paths_plot" class="figure">
                    <img src="mlst_0410.png" alt="mlst 0410" width="2000" height="1078" />
                    <h6><span class="label">Figure 4-11. </span>Gradient Descent paths in parameter space</h6>
                  </div>
                </figure>

                <p>Let’s compare the algorithms we’ve discussed so far for Linear Regression<sup><a data-type="noteref"
                      id="idm139656379289824-marker" href="ch04.xhtml#idm139656379289824">8</a></sup> (recall that
                  <em>m</em> is the number of training instances and <em>n</em> is the number of features); see <a
                    data-type="xref" href="#linear_regression_algorithm_comparison">Table 4-1</a>.</p>
                <table id="linear_regression_algorithm_comparison">
                  <caption><span class="label">Table 4-1. </span>Comparison of algorithms for Linear Regression
                  </caption>
                  <thead>
                    <tr>
                      <th>Algorithm</th>
                      <th>Large <em>m</em></th>
                      <th>Out-of-core support</th>
                      <th>Large <em>n</em></th>
                      <th>Hyperparams</th>
                      <th>Scaling required</th>
                      <th>Scikit-Learn</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>
                        <p>Normal Equation</p>
                      </td>
                      <td>
                        <p>Fast</p>
                      </td>
                      <td>
                        <p>No</p>
                      </td>
                      <td>
                        <p>Slow</p>
                      </td>
                      <td>
                        <p>0</p>
                      </td>
                      <td>
                        <p>No</p>
                      </td>
                      <td>
                        <p>n/a</p>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <p>SVD</p>
                      </td>
                      <td>
                        <p>Fast</p>
                      </td>
                      <td>
                        <p>No</p>
                      </td>
                      <td>
                        <p>Slow</p>
                      </td>
                      <td>
                        <p>0</p>
                      </td>
                      <td>
                        <p>No</p>
                      </td>
                      <td>
                        <p><code>LinearRegression</code></p>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <p>Batch GD</p>
                      </td>
                      <td>
                        <p>Slow</p>
                      </td>
                      <td>
                        <p>No</p>
                      </td>
                      <td>
                        <p>Fast</p>
                      </td>
                      <td>
                        <p>2</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                      <td>
                        <p><code>SGDRegressor</code></p>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <p>Stochastic GD</p>
                      </td>
                      <td>
                        <p>Fast</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                      <td>
                        <p>Fast</p>
                      </td>
                      <td>
                        <p>≥2</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                      <td>
                        <p><code>SGDRegressor</code></p>
                      </td>
                    </tr>
                    <tr>
                      <td>
                        <p>Mini-batch GD</p>
                      </td>
                      <td>
                        <p>Fast</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                      <td>
                        <p>Fast</p>
                      </td>
                      <td>
                        <p>≥2</p>
                      </td>
                      <td>
                        <p>Yes</p>
                      </td>
                      <td>
                        <p><code>SGDRegressor</code></p>
                      </td>
                    </tr>
                  </tbody>
                </table>
                <div data-type="note" epub:type="note">
                  <h6>Note</h6>
                  <p>There is almost no difference after training: all these algorithms end up with very similar models
                    and make predictions in exactly <a data-type="indexterm" data-primary="Linear Regression"
                      data-startref="lr4" id="idm139656379511744" /><a data-type="indexterm"
                      data-primary="training models" data-secondary="Linear Regression" data-startref="tm4lr"
                      id="idm139656379510768" />the same <a data-type="indexterm" data-primary="Linear Regression"
                      data-secondary="Gradient Descent in" data-startref="lr4gdi" id="idm139656379509424" /><a
                      data-type="indexterm" data-primary="Gradient Descent (GD)" data-startref="gd4"
                      id="idm139656379508176" /><a data-type="indexterm" data-primary="Mini-batch Gradient Descent"
                      data-startref="mbgd4" id="idm139656379507232" /><a data-type="indexterm"
                      data-primary="Gradient Descent (GD)" data-secondary="Mini-batch GD" data-startref="gd4mbgd"
                      id="idm139656379506272" /><a data-type="indexterm" data-primary="Gradient Descent (GD)"
                      data-secondary="algorithm comparisons" data-startref="gd4ac" id="idm139656379505056" />way.</p>
                </div>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Polynomial Regression">
          <div class="sect1" id="polynomial_regression">
            <h1>Polynomial Regression</h1>

            <p>What <a data-type="indexterm" data-primary="training models" data-secondary="Polynomial Regression"
                id="tm4pr" /><a data-type="indexterm" data-primary="Polynomial Regression" id="pr4" />if your data is
              actually more complex than a simple straight line? Surprisingly, you can actually use a linear model to
              fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a
              linear model on this extended set of features. This technique is called <em>Polynomial Regression</em>.
            </p>

            <p>Let’s look at an example. First, let’s generate some nonlinear data, based on a simple <em>quadratic
                equation</em><sup><a data-type="noteref" id="idm139656379498048-marker"
                  href="ch04.xhtml#idm139656379498048">9</a></sup> (plus some noise; see <a data-type="xref"
                href="#quadratic_data_plot">Figure 4-12</a>):</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="n">m</code> <code class="o">=</code> <code class="mi">100</code>
<code class="n">X</code> <code class="o">=</code> <code class="mi">6</code> <code class="o">*</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">rand</code><code class="p">(</code><code class="n">m</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code> <code class="o">-</code> <code class="mi">3</code>
<code class="n">y</code> <code class="o">=</code> <code class="mf">0.5</code> <code class="o">*</code> <code class="n">X</code><code class="o">**</code><code class="mi">2</code> <code class="o">+</code> <code class="n">X</code> <code class="o">+</code> <code class="mi">2</code> <code class="o">+</code> <code class="n">np</code><code class="o">.</code><code class="n">random</code><code class="o">.</code><code class="n">randn</code><code class="p">(</code><code class="n">m</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code></pre>

            <figure class="smallerseventyfive">
              <div id="quadratic_data_plot" class="figure">
                <img src="mlst_04in02.png" alt="mlst 04in02" width="1704" height="1095" />
                <h6><span class="label">Figure 4-12. </span>Generated nonlinear and noisy dataset</h6>
              </div>
            </figure>

            <p>Clearly, a straight line will never fit this data properly. So let’s use Scikit-Learn’s
              <code>PolynomialFeatures</code> class <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.preprocessing.PolynomialFeatures" id="sklppfcha4" />to transform our training
              data, adding the square (2<sup>nd</sup>-degree polynomial) of each feature in the training set as new
              features (in this case there is just one feature):</p>

            <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.preprocessing</code> <code class="kn">import</code> <code class="n">PolynomialFeatures</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">poly_features</code> <code class="o">=</code> <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">include_bias</code><code class="o">=</code><code class="bp">False</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_poly</code> <code class="o">=</code> <code class="n">poly_features</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="go">array([-0.75275929])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X_poly</code><code class="p">[</code><code class="mi">0</code><code class="p">]</code>
<code class="go">array([-0.75275929, 0.56664654])</code></pre>

            <p><code>X_poly</code> now contains the original feature of <code>X</code> plus the square of this feature.
              Now you can fit a <code>LinearRegression</code> <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.linear_model.LinearRegression" id="idm139656379072912" />model to this extended
              training data (<a data-type="xref" href="#quadratic_predictions_plot">Figure 4-13</a>):</p>

            <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">lin_reg</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lin_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_poly</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lin_reg</code><code class="o">.</code><code class="n">intercept_</code><code class="p">,</code> <code class="n">lin_reg</code><code class="o">.</code><code class="n">coef_</code>
<code class="go">(array([1.78134581]), array([[0.93366893, 0.56456263]]))</code></pre>

            <figure class="smallerseventyfive">
              <div id="quadratic_predictions_plot" class="figure">
                <img src="mlst_04in03.png" alt="mlst 04in03" width="1704" height="1095" />
                <h6><span class="label">Figure 4-13. </span>Polynomial Regression model predictions</h6>
              </div>
            </figure>

            <p>Not bad: the model estimates <math xmlns="http://www.w3.org/1998/Math/MathML"
                alttext="ModifyingAbove y With caret equals 0.56 x 1 squared plus 0.93 x 1 plus 1.78">
                <mrow>
                  <mover accent="true">
                    <mi>y</mi>
                    <mo>^</mo>
                  </mover>
                  <mo>=</mo>
                  <mn>0</mn>
                  <mo>.</mo>
                  <mn>56</mn>
                  <msubsup>
                    <mi>x</mi>
                    <mn>1</mn>
                    <mn>2</mn>
                  </msubsup>
                  <mo>+</mo>
                  <mn>0</mn>
                  <mo>.</mo>
                  <mn>93</mn>
                  <msub>
                    <mi>x</mi>
                    <mn>1</mn>
                  </msub>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo>.</mo>
                  <mn>78</mn>
                </mrow>
              </math> when in fact the original function was <math xmlns="http://www.w3.org/1998/Math/MathML"
                alttext="y equals 0.5 x 1 squared plus 1.0 x 1 plus 2.0 plus Gaussian noise">
                <mrow>
                  <mi>y</mi>
                  <mo>=</mo>
                  <mn>0</mn>
                  <mo>.</mo>
                  <mn>5</mn>
                  <msubsup>
                    <mi>x</mi>
                    <mn>1</mn>
                    <mn>2</mn>
                  </msubsup>
                  <mo>+</mo>
                  <mn>1</mn>
                  <mo>.</mo>
                  <mn>0</mn>
                  <msub>
                    <mi>x</mi>
                    <mn>1</mn>
                  </msub>
                  <mo>+</mo>
                  <mn>2</mn>
                  <mo>.</mo>
                  <mn>0</mn>
                  <mo>+</mo>
                  <mtext>Gaussian</mtext>
                  <mspace width="4.pt" />
                  <mtext>noise</mtext>
                </mrow>
              </math>.</p>

            <p>Note that when there are multiple features, Polynomial Regression is capable of finding relationships
              between features (which is something a plain Linear Regression model cannot do). This is made possible by
              the fact that <code>PolynomialFeatures</code> also adds all combinations of features up to the given
              degree. For example, if there were two features <em>a</em> and <em>b</em>, <code>PolynomialFeatures</code>
              <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.preprocessing.PolynomialFeatures" data-startref="sklppfcha4"
                id="idm139656378994016" />with <code>degree=3</code> would not only add the features
              <em>a</em><sup>2</sup>, <em>a</em><sup>3</sup>, <em>b</em><sup>2</sup>, and <em>b</em><sup>3</sup>, but
              also the combinations <em>ab</em>, <em>a</em><sup>2</sup><em>b</em>, and <em>ab</em><sup>2</sup>.</p>
            <div data-type="warning" epub:type="warning">
              <h6>Warning</h6>
              <p><code>PolynomialFeatures(degree=d)</code> transforms an array containing <em>n</em> features into an
                array containing <math xmlns="http://www.w3.org/1998/Math/MathML"
                  alttext="StartFraction left-parenthesis n plus d right-parenthesis factorial Over d factorial n factorial EndFraction">
                  <mstyle scriptlevel="0" displaystyle="true">
                    <mfrac>
                      <mrow>
                        <mo>(</mo>
                        <mi>n</mi>
                        <mo>+</mo>
                        <mi>d</mi>
                        <mo>)</mo>
                        <mo>!</mo>
                      </mrow>
                      <mrow>
                        <mi>d</mi>
                        <mo>!</mo>
                        <mspace width="0.166667em" />
                        <mi>n</mi>
                        <mo>!</mo>
                      </mrow>
                    </mfrac>
                  </mstyle>
                </math> features, where <em>n</em>! is the <em>factorial</em> of <em>n</em>, equal to 1 × 2 × 3 × ⋯ ×
                <em>n</em>. Beware of the combinatorial explosion of the number of <a data-type="indexterm"
                  data-primary="training models" data-secondary="Polynomial Regression" data-startref="tm4pr"
                  id="idm139656379183488" /><a data-type="indexterm" data-primary="Polynomial Regression"
                  data-startref="pr4" id="idm139656379182240" />features!</p>
            </div>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Learning Curves">
          <div class="sect1" id="idm139656379503328">
            <h1>Learning Curves</h1>

            <p>If you <a data-type="indexterm" data-primary="training models" data-secondary="learning curves in"
                id="tm4lci" />perform high-degree <a data-type="indexterm" data-primary="Linear Regression"
                data-secondary="learning curves in" id="lr4lci" /><a data-type="indexterm"
                data-primary="Polynomial Regression" data-secondary="learning curves in" id="pr4lci" />Polynomial
              Regression, you will likely fit the training data much better than with plain Linear Regression. For
              example, <a data-type="xref" href="#high_degree_polynomials_plot">Figure 4-14</a> applies a 300-degree
              polynomial model to the preceding training data, and compares the result with a pure linear model and a
              quadratic model (2<sup>nd</sup>-degree polynomial). Notice how the 300-degree polynomial model wiggles
              around to get as close as possible to the training instances.</p>

            <figure class="smallereighty">
              <div id="high_degree_polynomials_plot" class="figure">
                <img src="mlst_0411.png" alt="mlst 0411" width="1704" height="1095" />
                <h6><span class="label">Figure 4-14. </span>High-degree Polynomial Regression</h6>
              </div>
            </figure>

            <p>Of course, this high-degree Polynomial Regression model is severely overfitting the training data, while
              the linear model is underfitting it. The model that will generalize best in this case is the quadratic
              model. It makes sense since the data was generated using a quadratic model, but in general you won’t know
              what function generated the data, so how can you decide how complex your model should be? How can you tell
              that your model is overfitting or underfitting the data?</p>

            <p>In <a data-type="xref" href="ch02.xhtml#project_chapter">Chapter 2</a> you used cross-validation to get
              an estimate of a model’s generalization performance. If a model performs well on the training data but
              generalizes poorly according to the cross-validation metrics, then your model is overfitting. If it
              performs poorly on both, then it is underfitting. This is one way to tell when a model is too simple or
              too complex.</p>

            <p>Another way is to look at the <em>learning curves</em>: these are plots of the model’s performance on the
              training set and the validation set as a function of the training set size (or the training iteration). To
              generate the plots, simply train the model several times on different sized subsets of the training set.
              The following code defines a function that plots the learning curves of a model given some <a
                data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.metrics.mean_squared_error()"
                id="idm139656379167520" /><a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.model_selection.train_test_split()" id="idm139656379166480" />training data:</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">mean_squared_error</code>
<code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">train_test_split</code>

<code class="k">def</code> <code class="nf">plot_learning_curves</code><code class="p">(</code><code class="n">model</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">):</code>
    <code class="n">X_train</code><code class="p">,</code> <code class="n">X_val</code><code class="p">,</code> <code class="n">y_train</code><code class="p">,</code> <code class="n">y_val</code> <code class="o">=</code> <code class="n">train_test_split</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">,</code> <code class="n">test_size</code><code class="o">=</code><code class="mf">0.2</code><code class="p">)</code>
    <code class="n">train_errors</code><code class="p">,</code> <code class="n">val_errors</code> <code class="o">=</code> <code class="p">[],</code> <code class="p">[]</code>
    <code class="k">for</code> <code class="n">m</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1</code><code class="p">,</code> <code class="nb">len</code><code class="p">(</code><code class="n">X_train</code><code class="p">)):</code>
        <code class="n">model</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:</code><code class="n">m</code><code class="p">],</code> <code class="n">y_train</code><code class="p">[:</code><code class="n">m</code><code class="p">])</code>
        <code class="n">y_train_predict</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_train</code><code class="p">[:</code><code class="n">m</code><code class="p">])</code>
        <code class="n">y_val_predict</code> <code class="o">=</code> <code class="n">model</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_val</code><code class="p">)</code>
        <code class="n">train_errors</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">y_train</code><code class="p">[:</code><code class="n">m</code><code class="p">],</code> <code class="n">y_train_predict</code><code class="p">))</code>
        <code class="n">val_errors</code><code class="o">.</code><code class="n">append</code><code class="p">(</code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">y_val</code><code class="p">,</code> <code class="n">y_val_predict</code><code class="p">))</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">train_errors</code><code class="p">),</code> <code class="s2">"r-+"</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"train"</code><code class="p">)</code>
    <code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">sqrt</code><code class="p">(</code><code class="n">val_errors</code><code class="p">),</code> <code class="s2">"b-"</code><code class="p">,</code> <code class="n">linewidth</code><code class="o">=</code><code class="mi">3</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"val"</code><code class="p">)</code></pre>

            <p>Let’s look at the learning curves of the plain Linear Regression model <a data-type="indexterm"
                data-primary="Scikit-Learn" data-secondary="sklearn.linear_model.LinearRegression"
                id="sklmmlinregcha4" />(a straight line; <a data-type="xref"
                href="#underfitting_learning_curves_plot">Figure 4-15</a>):</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="n">lin_reg</code> <code class="o">=</code> <code class="n">LinearRegression</code><code class="p">()</code>
<code class="n">plot_learning_curves</code><code class="p">(</code><code class="n">lin_reg</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

            <figure class="smallerseventy">
              <div id="underfitting_learning_curves_plot" class="figure">
                <img src="mlst_04in04.png" alt="mlst 04in04" width="1701" height="1101" />
                <h6><span class="label">Figure 4-15. </span>Learning curves</h6>
              </div>
            </figure>

            <p>This deserves a bit of explanation. First, let’s look at the performance on the training data: when there
              are just one or two instances in the training set, the model can fit them perfectly, which is why the
              curve starts at zero. But as new instances are added to the training set, it becomes impossible for the
              model to fit the training data perfectly, both because the data is noisy and because it is not linear at
              all. So the error on the training data goes up until it reaches a plateau, at which point adding new
              instances to the training set doesn’t make the average error much better or worse. Now let’s look at the
              performance of the model on the validation data. When the model is trained on very few training instances,
              it is incapable of generalizing properly, which is why the validation error is initially quite big. Then
              as the model is shown more training examples, it learns and thus the validation error slowly goes down.
              However, once again a straight line cannot do a good job modeling the data, so the error ends up at a
              plateau, very close to the other curve.</p>

            <p>These learning curves are typical of an underfitting model. Both curves have reached a plateau; they are
              close and fairly high.</p>
            <div data-type="tip">
              <h6>Tip</h6>
              <p>If your model is underfitting the training data, adding more training examples will not help. You need
                to use a more complex model or come up with better features.</p>
            </div>

            <p>Now let’s look at the learning curves of a 10<sup>th</sup>-degree polynomial model on the same data <a
                data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.pipeline.Pipeline"
                id="idm139656378762784" /><a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.preprocessing.PolynomialFeatures" id="idm139656378761840" /><a
                data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.linear_model.LinearRegression"
                data-startref="sklmmlinregcha4" id="idm139656378760800" />(<a data-type="xref"
                href="#learning_curves_plot">Figure 4-16</a>):</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code>

<code class="n">polynomial_regression</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
        <code class="p">(</code><code class="s2">"poly_features"</code><code class="p">,</code> <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">10</code><code class="p">,</code> <code class="n">include_bias</code><code class="o">=</code><code class="bp">False</code><code class="p">)),</code>
        <code class="p">(</code><code class="s2">"lin_reg"</code><code class="p">,</code> <code class="n">LinearRegression</code><code class="p">()),</code>
    <code class="p">])</code>

<code class="n">plot_learning_curves</code><code class="p">(</code><code class="n">polynomial_regression</code><code class="p">,</code> <code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

            <p>These learning curves look a bit like the previous ones, but there are two very important differences:
            </p>

            <ul>
              <li>
                <p>The error on the training data is much lower than with the Linear Regression model.</p>
              </li>
              <li>
                <p>There is a gap between the curves. This means that the model performs significantly better on the
                  training data than on the validation data, which is the hallmark of an overfitting model. However, if
                  you used a much larger training set, the two curves would continue to get closer.</p>
              </li>
            </ul>

            <figure class="smallerseventy">
              <div id="learning_curves_plot" class="figure">
                <img src="mlst_04in05.png" alt="mlst 04in05" width="1701" height="1101" />
                <h6><span class="label">Figure 4-16. </span>Learning curves for the polynomial model</h6>
              </div>
            </figure>
            <div data-type="tip">
              <h6>Tip</h6>
              <p>One way to improve an overfitting model is to feed it more training data until the validation error
                reaches the training error.</p>
            </div>
            <aside data-type="sidebar" epub:type="sidebar">
              <div class="sidebar" id="idm139656378719488">
                <h5>The Bias/Variance Tradeoff</h5>
                <p>An important <a data-type="indexterm" data-primary="bias/variance tradeoff"
                    id="idm139656378718192" /><a data-type="indexterm" data-primary="variance"
                    data-secondary="bias/variance tradeoff" id="idm139656378717456" />theoretical result of statistics
                  and Machine Learning is the fact that a model’s generalization error can be expressed as the sum of
                  three very different errors:</p>
                <dl>
                  <dt><em>Bias</em></dt>
                  <dd>
                    <p>This part of the generalization error is due to wrong assumptions, such as assuming that the data
                      is linear when it is actually quadratic. A high-bias model is most likely to underfit the training
                      data.<sup><a data-type="noteref" id="idm139656378714160-marker"
                          href="ch04.xhtml#idm139656378714160">10</a></sup></p>
                  </dd>
                  <dt><em>Variance</em></dt>
                  <dd>
                    <p>This part is due to the model’s excessive sensitivity to small variations in the training data. A
                      model with many degrees of freedom <a data-type="indexterm" data-primary="degrees of freedom"
                        id="idm139656378712048" />(such as a high-degree polynomial model) is likely to have high
                      variance, and thus to overfit the training data.</p>
                  </dd>
                  <dt><em>Irreducible error</em></dt>
                  <dd>
                    <p>This <a data-type="indexterm" data-primary="irreducible error" id="idm139656378709680" />part is
                      due to the noisiness of the data itself. The only way to reduce this part of the error is to clean
                      up the data (e.g., fix the data sources, such as broken sensors, or detect and remove outliers).
                    </p>
                  </dd>
                </dl>

                <p>Increasing a model’s complexity will typically increase its variance and reduce its bias. Conversely,
                  reducing a model’s complexity increases its bias and reduces its variance. <a data-type="indexterm"
                    data-primary="training models" data-secondary="learning curves in" data-startref="tm4lci"
                    id="idm139656378707888" /><a data-type="indexterm" data-primary="Linear Regression"
                    data-secondary="learning curves in" data-startref="lr4lci" id="idm139656378706640" /><a
                    data-type="indexterm" data-primary="Polynomial Regression" data-secondary="learning curves in"
                    data-startref="pr4lci" id="idm139656378705424" />This is why it is called a tradeoff.</p>
              </div>
            </aside>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Regularized Linear Models">
          <div class="sect1" id="idm139656379180256">
            <h1>Regularized Linear Models</h1>

            <p>As we <a data-type="indexterm" data-primary="Linear Regression" data-secondary="regularizing models"
                data-see="regularization" id="idm139656378702800" /><a data-type="indexterm"
                data-primary="regularization" id="r4" />saw in Chapters <a data-type="#xref"
                data-xrefstyle="select:labelnumber" href="ch01.xhtml#landscape_chapter">1</a> and <a data-type="xref"
                data-xrefstyle="select:labelnumber" href="ch02.xhtml#project_chapter">2</a>, a good way to reduce
              overfitting is to regularize the model (i.e., to constrain it): the fewer degrees of freedom it has, the
              harder it will be for it to overfit the data. For example, a simple way to regularize a polynomial model
              is to reduce the number of polynomial degrees.</p>

            <p>For a linear model, regularization is typically achieved by constraining the weights of the model. We
              will now look at Ridge Regression, Lasso Regression, and Elastic Net, which implement three different ways
              to constrain the weights.</p>








            <section data-type="sect2" data-pdf-bookmark="Ridge Regression">
              <div class="sect2" id="idm139656378862208">
                <h2>Ridge Regression</h2>

                <p><em>Ridge Regression</em> <a data-type="indexterm" data-primary="linear models"
                    data-secondary="Ridge Regression" id="lm4rr" /><a data-type="indexterm"
                    data-primary="regularization" data-secondary="Ridge Regression" id="r4rr" /><a data-type="indexterm"
                    data-primary="Ridge Regression" id="rr4" /><a data-type="indexterm"
                    data-primary="Tikhonov regularization" id="idm139656378856528" />(also called <em>Tikhonov
                    regularization</em>) is a regularized version of Linear Regression: a <em>regularization term</em>
                  equal to <math xmlns="http://www.w3.org/1998/Math/MathML"
                    alttext="alpha sigma-summation Underscript i equals 1 Overscript n Endscripts theta Subscript i Baseline Superscript 2">
                    <mrow>
                      <mi>α</mi>
                      <msubsup>
                        <mo>∑</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>n</mi>
                      </msubsup>
                      <msup>
                        <mrow>
                          <msub>
                            <mi>θ</mi>
                            <mi>i</mi>
                          </msub>
                        </mrow>
                        <mn>2</mn>
                      </msup>
                    </mrow>
                  </math> is added to the cost function. <a data-type="indexterm" data-primary="cost function"
                    data-secondary="in ridge regression" data-secondary-sortas="ridge" id="cf4irr" />This forces the
                  learning algorithm to not only fit the data but also keep the model weights as small as possible. Note
                  that the regularization term should only be added to the cost function during training. Once the model
                  is trained, you want to evaluate the model’s performance using the unregularized performance measure.
                </p>
                <div data-type="note" epub:type="note">
                  <h6>Note</h6>
                  <p>It is quite common for the cost function used during training to be different from the performance
                    measure used for testing. Apart from regularization, another reason why they might be different is
                    that a good training cost function should have optimization-friendly derivatives, while the
                    performance measure used for testing should be as close as possible to the final objective. A good
                    example of this is a classifier trained using a cost function such as the log loss (discussed in a
                    moment) but evaluated using precision/recall.</p>
                </div>

                <p>The hyperparameter <em>α</em> controls how much you want to regularize the model. If <em>α</em> = 0
                  then Ridge Regression is just Linear Regression. If <em>α</em> is very large, then all weights end up
                  very close to zero and the result is a flat line going through the data’s mean. <a data-type="xref"
                    href="#ridge_cost_function">Equation 4-8</a> presents the Ridge Regression cost function.<sup><a
                      data-type="noteref" id="idm139656378842368-marker"
                      href="ch04.xhtml#idm139656378842368">11</a></sup></p>
                <div id="ridge_cost_function" data-type="equation">
                  <h5><span class="label">Equation 4-8. </span>Ridge Regression cost function</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>J</mi>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>=</mo>
                    <mrow>
                      <mtext>MSE</mtext>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>+</mo>
                    <mi>α</mi>
                    <mfrac>
                      <mn>1</mn>
                      <mn>2</mn>
                    </mfrac>
                    <munderover>
                      <mo>∑</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                      </mrow>
                      <mi>n</mi>
                    </munderover>
                    <msup>
                      <msub>
                        <mi>θ</mi>
                        <mi>i</mi>
                      </msub>
                      <mn>2</mn>
                    </msup>
                  </math>
                </div>

                <p>Note that the bias term <em>θ</em><sub>0</sub> is not regularized (the sum starts at <em>i</em> = 1,
                  not 0). If we define <strong>w</strong> as the vector of feature weights (<em>θ</em><sub>1</sub> to
                  <em>θ</em><sub><em>n</em></sub>), then the regularization term is simply equal to ½(∥
                  <strong>w</strong> ∥<sub>2</sub>)<sup>2</sup>, where ∥ <strong>w</strong> ∥<sub>2</sub> represents the
                  ℓ<sub>2</sub> <a data-type="indexterm" data-primary="ℓ 2 norm" id="ell2normch4" />norm of the weight
                  vector.<sup><a data-type="noteref" id="idm139656378824048-marker"
                      href="ch04.xhtml#idm139656378824048">12</a></sup> For Gradient Descent, just add
                  <em>α</em><strong>w</strong> to the MSE gradient vector (<a data-type="xref"
                    href="#mse_gradient_vector">Equation 4-6</a>).</p>
                <div data-type="warning" epub:type="warning">
                  <h6>Warning</h6>
                  <p>It is important to scale the data (e.g., using a <code>StandardScaler</code>) <a
                      data-type="indexterm" data-primary="Scikit-Learn"
                      data-secondary="sklearn.preprocessing.StandardScaler" id="idm139656378819728" />before performing
                    Ridge Regression, as it is sensitive to the scale of the input features. This is true of most
                    regularized models.</p>
                </div>

                <p><a data-type="xref" href="#ridge_regression_plot">Figure 4-17</a> shows several Ridge models trained
                  on some linear data using different <em>α</em> value. On the left, plain Ridge models are used,
                  leading to linear predictions. On the right, the data is first expanded using <a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="sklearn.preprocessing.PolynomialFeatures"
                    id="idm139656378816496" /><code>PolynomialFeatures(degree=10)</code>, then it is scaled using a
                  <code>StandardScaler</code>, and finally the Ridge models are applied to the resulting features: this
                  is Polynomial Regression with Ridge regularization. Note how increasing <em>α</em> leads to flatter
                  (i.e., less extreme, more reasonable) predictions; this reduces the model’s variance but increases its
                  bias.</p>

                <p>As with Linear Regression, we can perform Ridge Regression either by computing a <a
                    data-type="indexterm" data-primary="closed-form equation" id="idm139656378813424" />closed-form
                  equation or by performing Gradient Descent. The pros and cons are the same. <a data-type="xref"
                    href="#ridge_regression_solution">Equation 4-9</a> shows the <a data-type="indexterm"
                    data-primary="identity matrix" id="idm139656378811712" />closed-form solution (where
                  <strong>A</strong> is the (<em>n</em> + 1) × (<em>n</em> + 1) <em>identity matrix</em><sup><a
                      data-type="noteref" id="idm139656378809280-marker"
                      href="ch04.xhtml#idm139656378809280">13</a></sup> except with a 0 in the top-left cell,
                  corresponding to the bias term).</p>

                <figure>
                  <div id="ridge_regression_plot" class="figure">
                    <img src="mlst_0412.png" alt="mlst 0412" width="2328" height="1095" />
                    <h6><span class="label">Figure 4-17. </span>Ridge Regression</h6>
                  </div>
                </figure>
                <div class="fifty-percent" id="ridge_regression_solution" data-type="equation">
                  <h5><span class="label">Equation 4-9. </span>Ridge Regression closed-form solution</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mover accent="true">
                        <mi mathvariant="bold">θ</mi>
                        <mo>^</mo>
                      </mover>
                      <mo>=</mo>
                      <msup>
                        <mrow>
                          <mo>(</mo>
                          <msup>
                            <mi mathvariant="bold">X</mi>
                            <mi>T</mi>
                          </msup>
                          <mi mathvariant="bold">X</mi>
                          <mo>+</mo>
                          <mi>α</mi>
                          <mi mathvariant="bold">A</mi>
                          <mo>)</mo>
                        </mrow>
                        <mrow>
                          <mo>-</mo>
                          <mn>1</mn>
                        </mrow>
                      </msup>
                      <mo> </mo>
                      <msup>
                        <mi mathvariant="bold">X</mi>
                        <mi>T</mi>
                      </msup>
                      <mo> </mo>
                      <mi mathvariant="bold">y</mi>
                    </mrow>
                  </math>
                </div>

                <p>Here is how to perform Ridge Regression with <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.linear_model.Ridge" id="idm139656378598384" /><a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="Ridge Regression with"
                    id="idm139656378597440" />Scikit-Learn using a closed-form solution (a variant of <a
                    data-type="xref" href="#ridge_regression_solution">Equation 4-9</a> using a matrix factorization
                  technique by André-Louis Cholesky):</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Ridge</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">ridge_reg</code> <code class="o">=</code> <code class="n">Ridge</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">solver</code><code class="o">=</code><code class="s">"cholesky"</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">ridge_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">ridge_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mf">1.5</code><code class="p">]])</code>
<code class="go">array([[1.55071465]])</code></pre>

                <p>And using <a data-type="indexterm" data-primary="Stochastic Gradient Descent (SGD) classifier"
                    id="idm139656378593744" />Stochastic Gradient Descent:<sup><a data-type="noteref"
                      id="idm139656378570704-marker" href="ch04.xhtml#idm139656378570704">14</a></sup></p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">sgd_reg</code> <code class="o">=</code> <code class="n">SGDRegressor</code><code class="p">(</code><code class="n">penalty</code><code class="o">=</code><code class="s">"l2"</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">sgd_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="o">.</code><code class="n">ravel</code><code class="p">())</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">sgd_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mf">1.5</code><code class="p">]])</code>
<code class="go">array([1.47012588])</code></pre>

                <p>The <code>penalty</code> hyperparameter sets the type of regularization term to use. Specifying
                  <code>"l2"</code> indicates that you want SGD to add a regularization term to the cost function <a
                    data-type="indexterm" data-primary="cost function" data-secondary="in ridge regression"
                    data-secondary-sortas="ridge" data-startref="cf4irr" id="idm139656378424336" />equal to half the
                  square of the ℓ<sub>2</sub> norm of the weight vector: this is simply <a data-type="indexterm"
                    data-primary="linear models" data-secondary="Ridge Regression" data-startref="lm4rr"
                    id="idm139656378422400" /><a data-type="indexterm" data-primary="regularization"
                    data-secondary="Ridge Regression" data-startref="r4rr" id="idm139656378421152" /><a
                    data-type="indexterm" data-primary="Ridge Regression" data-startref="rr4"
                    id="idm139656378419936" />Ridge Regression.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Lasso Regression">
              <div class="sect2" id="lasso_regression">
                <h2>Lasso Regression</h2>

                <p><em>Least Absolute Shrinkage and Selection Operator Regression</em> (simply called <em>Lasso
                    Regression</em>) is <a data-type="indexterm" data-primary="Lasso Regression" id="lasso4" /><a
                    data-type="indexterm" data-primary="regularization" data-secondary="Lasso Regression" id="r4lr" /><a
                    data-type="indexterm" data-primary="linear models" data-secondary="Lasso Regression"
                    id="lm4lr" />another regularized version of Linear Regression: just like Ridge Regression, it adds a
                  regularization term to the <a data-type="indexterm" data-primary="cost function"
                    data-secondary="in Lasso  Regression" data-secondary-sortas="Lasso" id="cf4ilr" />cost function, but
                  it uses the ℓ<sub>1</sub> <a data-type="indexterm" data-primary="ℓ 1 norm"
                    id="idm139656378410400" />norm of the weight vector instead of half the square of the ℓ<sub>2</sub>
                  <a data-type="indexterm" data-primary="ℓ 2 norm" data-startref="ell2normch4"
                    id="idm139656378409152" />norm (see <a data-type="xref" href="#lasso_cost_function">Equation
                    4-10</a>).</p>
                <div id="lasso_cost_function" data-type="equation">
                  <h5><span class="label">Equation 4-10. </span>Lasso Regression cost function</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>J</mi>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>=</mo>
                    <mrow>
                      <mtext>MSE</mtext>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>+</mo>
                    <mrow>
                      <mi>α</mi>
                      <munderover>
                        <mo>∑</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>n</mi>
                      </munderover>
                      <mfenced open="|" close="|">
                        <msub>
                          <mi>θ</mi>
                          <mi>i</mi>
                        </msub>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <p><a data-type="xref" href="#lasso_regression_plot">Figure 4-18</a> shows the same thing as <a
                    data-type="xref" href="#ridge_regression_plot">Figure 4-17</a> but replaces Ridge models with Lasso
                  models and uses smaller <em>α</em> values.</p>

                <figure>
                  <div id="lasso_regression_plot" class="figure">
                    <img src="mlst_0413.png" alt="mlst 0413" width="2328" height="1095" />
                    <h6><span class="label">Figure 4-18. </span>Lasso Regression</h6>
                  </div>
                </figure>

                <p>An important characteristic of Lasso Regression is that it tends to completely eliminate the weights
                  of the least important features (i.e., set them to zero). For example, the dashed line in the right
                  plot on <a data-type="xref" href="#lasso_regression_plot">Figure 4-18</a> (with <em>α</em> =
                  10<sup>-7</sup>) looks quadratic, almost linear: all the weights for the high-degree polynomial
                  features are equal to zero. In other words, Lasso Regression automatically performs <a
                    data-type="indexterm" data-primary="feature selection" id="idm139656378539120" />feature selection
                  and outputs <a data-type="indexterm" data-primary="sparse models" id="idm139656378538288" />a
                  <em>sparse model</em> (i.e., with few nonzero feature weights).</p>

                <p>You can get a sense of why this is the case by looking at <a data-type="xref"
                    href="#lasso_vs_ridge_plot">Figure 4-19</a>: on the top-left plot, the background contours
                  (ellipses) represent an unregularized MSE cost function (<em>α</em> = 0), and the white circles show
                  the <a data-type="indexterm" data-primary="Batch Gradient Descent" id="idm139656378534992" /><a
                    data-type="indexterm" data-primary="Gradient Descent (GD)" data-secondary="Batch GD"
                    id="idm139656378534288" />Batch Gradient Descent path with that cost function. The foreground
                  contours (diamonds) represent the ℓ<sub>1</sub> penalty, and the triangles show the BGD path for this
                  penalty only (<em>α</em> → ∞). Notice how the path first reaches <em>θ</em><sub>1</sub> = 0, then
                  rolls down a gutter until it reaches <em>θ</em><sub>2</sub> = 0. On the top-right plot, the contours
                  represent the same cost function plus an ℓ<sub>1</sub> penalty with <em>α</em> = 0.5. The global
                  minimum is on the <em>θ</em><sub>2</sub> = 0 axis. BGD first reaches <em>θ</em><sub>2</sub> = 0, then
                  rolls down the gutter until it reaches the global minimum. The two bottom plots show the same thing
                  but uses an ℓ<sub>2</sub> penalty instead. The regularized minimum is closer to <strong>θ</strong> =
                  <strong>0</strong> than the unregularized minimum, but the weights do not get fully eliminated.</p>

                <figure>
                  <div id="lasso_vs_ridge_plot" class="figure">
                    <img src="mlst_0414.png" alt="mlst 0414" width="3501" height="2284" />
                    <h6><span class="label">Figure 4-19. </span>Lasso versus Ridge regularization</h6>
                  </div>
                </figure>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>On the Lasso cost function, the BGD path tends to bounce across the gutter toward the end. This is
                    because the slope changes abruptly at <em>θ</em><sub>2</sub> = 0. You need to gradually reduce the
                    learning rate in order to actually converge to the global minimum.</p>
                </div>

                <p>The Lasso cost function is not differentiable at <em>θ</em><sub><em>i</em></sub> = 0 (for <em>i</em>
                  = 1, 2, ⋯, <em>n</em>), but Gradient Descent still works fine if you use <a data-type="indexterm"
                    data-primary="subgradient vector" id="idm139656378520608" />a <em>subgradient vector</em>
                  <strong>g</strong><sup><a data-type="noteref" id="idm139656378519040-marker"
                      href="ch04.xhtml#idm139656378519040">15</a></sup> instead when any <em>θ</em><sub><em>i</em></sub>
                  = 0. <a data-type="xref" href="#lasso_subgradient_vector">Equation 4-11</a> shows a subgradient vector
                  equation you can use for Gradient Descent with the Lasso <a data-type="indexterm"
                    data-primary="cost function" data-secondary="in Lasso  Regression" data-secondary-sortas="Lasso"
                    data-startref="cf4ilr" id="idm139656378516432" />cost function.</p>
                <div id="lasso_subgradient_vector" data-type="equation">
                  <h5><span class="label">Equation 4-11. </span>Lasso Regression subgradient vector</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>g</mi>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>,</mo>
                      <mi>J</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>=</mo>
                    <mrow>
                      <msub>
                        <mo>∇</mo>
                        <mi mathvariant="bold">θ</mi>
                      </msub>
                      <mo> </mo>
                      <mtext>MSE</mtext>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>+</mo>
                    <mrow>
                      <mi>α</mi>
                      <mfenced>
                        <mtable>
                          <mtr>
                            <mtd>
                              <mo>sign</mo>
                              <mo>(</mo>
                              <msub>
                                <mi>θ</mi>
                                <mn>1</mn>
                              </msub>
                              <mo>)</mo>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mo>sign</mo>
                              <mo>(</mo>
                              <msub>
                                <mi>θ</mi>
                                <mn>2</mn>
                              </msub>
                              <mo>)</mo>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mo>⋮</mo>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mo>sign</mo>
                              <mo>(</mo>
                              <msub>
                                <mi>θ</mi>
                                <mi>n</mi>
                              </msub>
                              <mo>)</mo>
                            </mtd>
                          </mtr>
                        </mtable>
                      </mfenced>
                    </mrow>
                    <mo>  </mo>
                    <mtext>where </mtext>
                    <mrow>
                      <mo>sign</mo>
                      <mo>(</mo>
                      <msub>
                        <mi>θ</mi>
                        <mi>i</mi>
                      </msub>
                      <mo>)</mo>
                    </mrow>
                    <mo>=</mo>
                    <mrow>
                      <mfenced open="{" close="">
                        <mtable>
                          <mtr>
                            <mtd>
                              <mo>-</mo>
                              <mn>1</mn>
                            </mtd>
                            <mtd>
                              <mtext>if </mtext>
                              <msub>
                                <mi>θ</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>&lt;</mo>
                              <mn>0</mn>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mn>0</mn>
                            </mtd>
                            <mtd>
                              <mtext>if </mtext>
                              <msub>
                                <mi>θ</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>=</mo>
                              <mn>0</mn>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mo>+</mo>
                              <mn>1</mn>
                            </mtd>
                            <mtd>
                              <mtext>if </mtext>
                              <msub>
                                <mi>θ</mi>
                                <mi>i</mi>
                              </msub>
                              <mo>&gt;</mo>
                              <mn>0</mn>
                            </mtd>
                          </mtr>
                        </mtable>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <p>Here is a small Scikit-Learn example using the <code>Lasso</code> class. Note that you could instead
                  use <a data-type="indexterm" data-primary="Lasso Regression" data-startref="lasso4"
                    id="idm139656378489024" /><a data-type="indexterm" data-primary="regularization"
                    data-secondary="Lasso Regression" data-startref="r4lr" id="idm139656378488176" /><a
                    data-type="indexterm" data-primary="linear models" data-secondary="Lasso Regression"
                    data-startref="lm4lr" id="idm139656378486960" /><a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.linear_model.Lasso" id="idm139656378485744" />an <a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="sklearn.linear_model.SGDRegressor"
                    id="skllmsgdrch4part2" /><code>SGDRegressor(penalty="l1")</code>.</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">Lasso</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lasso_reg</code> <code class="o">=</code> <code class="n">Lasso</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.1</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lasso_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">lasso_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mf">1.5</code><code class="p">]])</code>
<code class="go">array([1.53788174])</code></pre>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Elastic Net">
              <div class="sect2" id="idm139656378418400">
                <h2>Elastic Net</h2>

                <p>Elastic Net <a data-type="indexterm" data-primary="linear models" data-secondary="Elastic Net"
                    id="idm139656378345424" /><a data-type="indexterm" data-primary="Elastic Net"
                    id="idm139656378344416" /><a data-type="indexterm" data-primary="regularization"
                    data-secondary="Elastic Net" id="idm139656378343744" />is a middle ground between Ridge Regression
                  and Lasso Regression. The regularization term is a simple mix of both Ridge and Lasso’s regularization
                  terms, and you can control the mix ratio <em>r</em>. When <em>r</em> = 0, Elastic Net is equivalent to
                  Ridge Regression, and when <em>r</em> = 1, it is equivalent to <a data-type="indexterm"
                    data-primary="cost function" data-secondary="in Elastic Net" data-secondary-sortas="Elastic"
                    id="idm139656378341216" />Lasso Regression (see <a data-type="xref"
                    href="#elastic_net_cost_function">Equation 4-12</a>).</p>
                <div id="elastic_net_cost_function" data-type="equation">
                  <h5><span class="label">Equation 4-12. </span>Elastic Net cost function</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>J</mi>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>=</mo>
                    <mrow>
                      <mtext>MSE</mtext>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>+</mo>
                    <mrow>
                      <mi>r</mi>
                      <mi>α</mi>
                      <munderover>
                        <mo>∑</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>n</mi>
                      </munderover>
                      <mfenced open="|" close="|">
                        <msub>
                          <mi>θ</mi>
                          <mi>i</mi>
                        </msub>
                      </mfenced>
                    </mrow>
                    <mo>+</mo>
                    <mfrac>
                      <mrow>
                        <mn>1</mn>
                        <mo>-</mo>
                        <mi>r</mi>
                      </mrow>
                      <mn>2</mn>
                    </mfrac>
                    <mi>α</mi>
                    <munderover>
                      <mo>∑</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                      </mrow>
                      <mi>n</mi>
                    </munderover>
                    <msup>
                      <msub>
                        <mi>θ</mi>
                        <mi>i</mi>
                      </msub>
                      <mn>2</mn>
                    </msup>
                  </math>
                </div>

                <p>So when should you use <a data-type="indexterm" data-primary="Linear Regression"
                    id="idm139656378325280" /><a data-type="indexterm" data-primary="Ridge Regression"
                    id="idm139656378324576" /><a data-type="indexterm" data-primary="linear models"
                    data-secondary="Linear Regression" data-see="Linear Regression" id="idm139656378323904" /><a
                    data-type="indexterm" data-primary="linear models" data-secondary="Ridge Regression"
                    id="idm139656378322688" />plain Linear Regression (i.e., without any regularization), Ridge, Lasso,
                  or Elastic Net? It is almost always preferable to have at least a little bit of regularization, so
                  generally you should avoid plain Linear Regression. Ridge is a good default, but if you suspect that
                  only a few features are actually useful, you should prefer Lasso or Elastic Net since they tend to
                  reduce the useless features’ weights down to zero as we have discussed. In general, <a
                    data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.linear_model.ElasticNet"
                    id="idm139656378321136" />Elastic Net is preferred over Lasso since Lasso may behave erratically
                  when the number of features is greater than the number of training instances or when several features
                  are strongly correlated.</p>

                <p>Here is a short example using Scikit-Learn’s <code>ElasticNet</code> (<code>l1_ratio</code>
                  corresponds to the mix ratio <em>r</em>):</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">ElasticNet</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">elastic_net</code> <code class="o">=</code> <code class="n">ElasticNet</code><code class="p">(</code><code class="n">alpha</code><code class="o">=</code><code class="mf">0.1</code><code class="p">,</code> <code class="n">l1_ratio</code><code class="o">=</code><code class="mf">0.5</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">elastic_net</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">elastic_net</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mf">1.5</code><code class="p">]])</code>
<code class="go">array([1.54333232])</code></pre>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Early Stopping">
              <div class="sect2" id="idm139656378286944">
                <h2>Early Stopping</h2>

                <p>A <a data-type="indexterm" data-primary="early stopping" id="es4" /><a data-type="indexterm"
                    data-primary="linear models" data-secondary="early stopping" id="lm4es" /><a data-type="indexterm"
                    data-primary="regularization" data-secondary="early stopping" id="r4es" />very different way to
                  regularize iterative learning algorithms such as Gradient Descent is to stop training as soon as the
                  validation error reaches a minimum. This is called <em>early stopping</em>. <a data-type="xref"
                    href="#early_stopping_plot">Figure 4-20</a> shows a complex model (in this case a high-degree
                  Polynomial Regression model) being trained using Batch Gradient Descent. As the epochs go by, the
                  algorithm learns and its prediction error (RMSE) on the training set naturally goes down, and so does
                  its prediction error on the validation set. However, after a while the validation error stops
                  decreasing and actually starts to go back up. This indicates that the model has started to overfit the
                  training data. With early stopping you just stop training as soon as the validation error reaches the
                  minimum. It is such a simple and efficient regularization technique that Geoffrey Hinton called it a
                  “beautiful free lunch.”</p>

                <figure class="smallereighty">
                  <div id="early_stopping_plot" class="figure">
                    <img src="mlst_0415.png" alt="mlst 0415" width="1692" height="1090" />
                    <h6><span class="label">Figure 4-20. </span>Early stopping regularization</h6>
                  </div>
                </figure>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>With Stochastic and Mini-batch Gradient Descent, the curves are not so smooth, and it may be hard
                    to know whether you have reached the minimum or not. One solution is to stop only after the
                    validation error has been above the minimum for some time (when you are confident that the model
                    will not do any better), then roll back the <a data-type="indexterm" data-primary="model parameters"
                      id="idm139656378265568" />model parameters to the point where the validation error was at a
                    minimum.</p>
                </div>

                <p>Here is a basic implementation of early <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.base.clone()" id="idm139656378264224" /><a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="sklearn.linear_model.SGDRegressor"
                    data-startref="skllmsgdrch4part2" id="idm139656378263248" /><a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="sklearn.metrics.mean_squared_error()"
                    id="idm139656378262064" />stopping:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.base</code> <code class="kn">import</code> <code class="n">clone</code>

<code class="c1"># prepare the data</code>
<code class="n">poly_scaler</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
        <code class="p">(</code><code class="s2">"poly_features"</code><code class="p">,</code> <code class="n">PolynomialFeatures</code><code class="p">(</code><code class="n">degree</code><code class="o">=</code><code class="mi">90</code><code class="p">,</code> <code class="n">include_bias</code><code class="o">=</code><code class="bp">False</code><code class="p">)),</code>
        <code class="p">(</code><code class="s2">"std_scaler"</code><code class="p">,</code> <code class="n">StandardScaler</code><code class="p">())</code>
    <code class="p">])</code>
<code class="n">X_train_poly_scaled</code> <code class="o">=</code> <code class="n">poly_scaler</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">X_val_poly_scaled</code> <code class="o">=</code> <code class="n">poly_scaler</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_val</code><code class="p">)</code>

<code class="n">sgd_reg</code> <code class="o">=</code> <code class="n">SGDRegressor</code><code class="p">(</code><code class="n">max_iter</code><code class="o">=</code><code class="mi">1</code><code class="p">,</code> <code class="n">tol</code><code class="o">=-</code><code class="n">np</code><code class="o">.</code><code class="n">infty</code><code class="p">,</code> <code class="n">warm_start</code><code class="o">=</code><code class="bp">True</code><code class="p">,</code>
                       <code class="n">penalty</code><code class="o">=</code><code class="bp">None</code><code class="p">,</code> <code class="n">learning_rate</code><code class="o">=</code><code class="s2">"constant"</code><code class="p">,</code> <code class="n">eta0</code><code class="o">=</code><code class="mf">0.0005</code><code class="p">)</code>

<code class="n">minimum_val_error</code> <code class="o">=</code> <code class="nb">float</code><code class="p">(</code><code class="s2">"inf"</code><code class="p">)</code>
<code class="n">best_epoch</code> <code class="o">=</code> <code class="bp">None</code>
<code class="n">best_model</code> <code class="o">=</code> <code class="bp">None</code>
<code class="k">for</code> <code class="n">epoch</code> <code class="ow">in</code> <code class="nb">range</code><code class="p">(</code><code class="mi">1000</code><code class="p">):</code>
    <code class="n">sgd_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train_poly_scaled</code><code class="p">,</code> <code class="n">y_train</code><code class="p">)</code>  <code class="c1"># continues where it left off</code>
    <code class="n">y_val_predict</code> <code class="o">=</code> <code class="n">sgd_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">(</code><code class="n">X_val_poly_scaled</code><code class="p">)</code>
    <code class="n">val_error</code> <code class="o">=</code> <code class="n">mean_squared_error</code><code class="p">(</code><code class="n">y_val</code><code class="p">,</code> <code class="n">y_val_predict</code><code class="p">)</code>
    <code class="k">if</code> <code class="n">val_error</code> <code class="o">&lt;</code> <code class="n">minimum_val_error</code><code class="p">:</code>
        <code class="n">minimum_val_error</code> <code class="o">=</code> <code class="n">val_error</code>
        <code class="n">best_epoch</code> <code class="o">=</code> <code class="n">epoch</code>
        <code class="n">best_model</code> <code class="o">=</code> <code class="n">clone</code><code class="p">(</code><code class="n">sgd_reg</code><code class="p">)</code></pre>

                <p>Note that with <code>warm_start=True</code>, when the <code>fit()</code> method is called, it just
                  continues training where it left off instead of restarting <a data-type="indexterm"
                    data-primary="regularization" data-startref="r4" id="idm139656378258464" /><a data-type="indexterm"
                    data-primary="early stopping" data-startref="es4" id="idm139656378257520" /><a data-type="indexterm"
                    data-primary="linear models" data-secondary="early stopping" data-startref="lm4es"
                    id="idm139656378114768" /><a data-type="indexterm" data-primary="regularization"
                    data-secondary="early stopping" data-startref="r4es" id="idm139656378113552" />from scratch.</p>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Logistic Regression">
          <div class="sect1" id="idm139656378112080">
            <h1>Logistic Regression</h1>

            <p>As <a data-type="indexterm" data-primary="training models" data-secondary="Logistic Regression"
                id="tm4logr" /><a data-type="indexterm" data-primary="Logistic Regression" id="lreg4" />we discussed in
              <a data-type="xref" href="ch01.xhtml#landscape_chapter">Chapter 1</a>, some regression algorithms can be
              used for classification as well (and vice versa). <em>Logistic Regression</em> (also called <em>Logit
                Regression</em>) is commonly used to estimate the probability that an instance belongs to a particular
              class (e.g., what is the probability that this email is spam?). If the estimated probability is greater
              than 50%, then the model predicts that the instance belongs to that class (called the positive class,
              labeled “1”), or else it predicts that it does not (i.e., it belongs to the negative class, labeled “0”).
              This makes it a <a data-type="indexterm" data-primary="binary classifiers"
                id="idm139656378105824" />binary classifier.</p>








            <section data-type="sect2" data-pdf-bookmark="Estimating Probabilities">
              <div class="sect2" id="idm139656378104992">
                <h2>Estimating Probabilities</h2>

                <p>So <a data-type="indexterm" data-primary="Logistic Regression"
                    data-secondary="estimating probablities" id="lr4ep" /><a data-type="indexterm"
                    data-primary="probabilities, estimating" id="p4e" />how does it work? Just like a Linear Regression
                  model, a Logistic Regression model computes a weighted sum of the input features (plus a bias term),
                  but instead of outputting the result directly like the Linear Regression model does, it outputs the
                  <em>logistic</em> of this result (see <a data-type="xref"
                    href="#logisticregression_model_estimated_probability_vectorized_form">Equation 4-13</a>).</p>
                <div class="fifty-percent" id="logisticregression_model_estimated_probability_vectorized_form"
                  data-type="equation">
                  <h5><span class="label">Equation 4-13. </span>Logistic Regression model estimated probability
                    (vectorized form)</h5><math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mover accent="true">
                        <mi>p</mi>
                        <mo>^</mo>
                      </mover>
                      <mo>=</mo>
                      <msub>
                        <mi>h</mi>
                        <mi mathvariant="bold">θ</mi>
                      </msub>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">x</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <mi>σ</mi>
                      <mrow>
                        <mo>(</mo>
                        <msup>
                          <mi mathvariant="bold">x</mi>
                          <mi>T</mi>
                        </msup>
                        <mi mathvariant="bold">θ</mi>
                        <mo>)</mo>
                      </mrow>
                    </mrow>
                  </math>
                </div>

                <p>The logistic—noted <em>σ</em>(·)—is a <em>sigmoid function</em> (i.e., <em>S</em>-shaped) that
                  outputs <a data-type="indexterm" data-primary="logistic function" id="idm139656378085520" /><a
                    data-type="indexterm" data-primary="sigmoid function" id="idm139656378084784" />a number between 0
                  and 1. It is defined as shown in <a data-type="xref" href="#equation_four_fourteen">Equation 4-14</a>
                  and <a data-type="xref" href="#logistic_function_plot">Figure 4-21</a>.</p>
                <div data-type="equation" id="equation_four_fourteen">
                  <h5><span class="label">Equation 4-14. </span>Logistic function</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mi>σ</mi>
                      <mrow>
                        <mo>(</mo>
                        <mi>t</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mn>1</mn>
                          <mrow>
                            <mn>1</mn>
                            <mo>+</mo>
                            <mo form="prefix">exp</mo>
                            <mo>(</mo>
                            <mo>-</mo>
                            <mi>t</mi>
                            <mo>)</mo>
                          </mrow>
                        </mfrac>
                      </mstyle>
                    </mrow>
                  </math>
                </div>

                <figure>
                  <div id="logistic_function_plot" class="figure">
                    <img src="mlst_04in06.png" alt="mlst 04in06" width="2605" height="778" />
                    <h6><span class="label">Figure 4-21. </span>Logistic function</h6>
                  </div>
                </figure>

                <p>Once the Logistic Regression model has estimated the probability <math
                    xmlns="http://www.w3.org/1998/Math/MathML" alttext="ModifyingAbove p With caret">
                    <mover accent="true">
                      <mi>p</mi>
                      <mo>^</mo>
                    </mover>
                  </math> = <em>h</em><sub><strong>θ</strong></sub>(<strong>x</strong>) that an instance
                  <strong>x</strong> belongs to the positive class, it can make its prediction <em>ŷ</em> easily (see <a
                    data-type="xref" href="#equation_four_fifteen">Equation 4-15</a>).</p>
                <div data-type="equation" id="equation_four_fifteen">
                  <h5><span class="label">Equation 4-15. </span>Logistic Regression model prediction</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mover accent="true">
                        <mi>y</mi>
                        <mo>^</mo>
                      </mover>
                      <mo>=</mo>
                      <mfenced separators="" open="{" close="">
                        <mtable>
                          <mtr>
                            <mtd columnalign="left">
                              <mn>0</mn>
                            </mtd>
                            <mtd columnalign="left">
                              <mrow>
                                <mtext>if</mtext>
                                <mspace width="4.pt" />
                                <mover accent="true">
                                  <mi>p</mi>
                                  <mo>^</mo>
                                </mover>
                                <mo>&lt;</mo>
                                <mn>0</mn>
                                <mo>.</mo>
                                <mn>5</mn>
                              </mrow>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd columnalign="left">
                              <mn>1</mn>
                            </mtd>
                            <mtd columnalign="left">
                              <mrow>
                                <mtext>if</mtext>
                                <mspace width="4.pt" />
                                <mover accent="true">
                                  <mi>p</mi>
                                  <mo>^</mo>
                                </mover>
                                <mo>≥</mo>
                                <mn>0</mn>
                                <mo>.</mo>
                                <mn>5</mn>
                              </mrow>
                            </mtd>
                          </mtr>
                        </mtable>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <p>Notice that <em>σ</em>(<em>t</em>) &lt; 0.5 when <em>t</em> &lt; 0, and <em>σ</em>(<em>t</em>) ≥ 0.5
                  when <em>t</em> ≥ 0, so a Logistic Regression model predicts 1 if
                  <strong>x</strong><sup><em>T</em></sup> <strong>θ</strong> is positive, and 0 if it is <a
                    data-type="indexterm" data-primary="Logistic Regression" data-secondary="estimating probabilities"
                    data-startref="lr4ep" id="idm139656378040224" /><a data-type="indexterm"
                    data-primary="probabilities, estimating" data-startref="p4e" id="idm139656378038912" />negative.</p>
                <div data-type="note" epub:type="note">
                  <h6>Note</h6>
                  <p>The score <em>t</em> is often called the <em>logit</em>: this name comes from the fact that the
                    logit function, defined as logit(<em>p</em>) = log(<em>p</em> / (1 - <em>p</em>)), is the inverse of
                    the logistic function. Indeed, if you compute the logit of the estimated probability <em>p</em>, you
                    will find that the result is <em>t</em>. The logit is also called the <em>log-odds</em>, since it is
                    the log of the ratio between the estimated probability for the positive class and the estimated
                    probability for the negative class.</p>
                </div>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Training and Cost Function">
              <div class="sect2" id="idm139656378104368">
                <h2>Training and Cost Function</h2>

                <p>Good, <a data-type="indexterm" data-primary="Logistic Regression"
                    data-secondary="training and cost function" id="lr4tacf" /><a data-type="indexterm"
                    data-primary="training set" data-secondary="cost function of" id="ts4cfo" /><a data-type="indexterm"
                    data-primary="cost function" data-secondary="in Logistic Regression" id="cf4" />now you know how a
                  Logistic Regression model estimates probabilities and makes predictions. But how is it trained? The
                  objective of training is to set the <a data-type="indexterm" data-primary="parameter vector"
                    id="idm139656378027376" />parameter vector <strong>θ</strong> so that the model estimates high
                  probabilities for positive instances (<em>y</em> = 1) and low probabilities for negative instances
                  (<em>y</em> = 0). This idea is captured by the cost function shown in <a data-type="xref"
                    href="#cost_function_of_a_single_training_instance">Equation 4-16</a> for a single training instance
                  <strong>x</strong>.</p>
                <div id="cost_function_of_a_single_training_instance" data-type="equation">
                  <h5><span class="label">Equation 4-16. </span>Cost function of a single training instance</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>c</mi>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>=</mo>
                    <mrow>
                      <mfenced open="{" close="">
                        <mtable>
                          <mtr>
                            <mtd>
                              <mo>-</mo>
                              <mi>log</mi>
                              <mo>(</mo>
                              <mover accent="true">
                                <mi>p</mi>
                                <mo>^</mo>
                              </mover>
                              <mo>)</mo>
                            </mtd>
                            <mtd>
                              <mtext>if </mtext>
                              <mi>y</mi>
                              <mo>=</mo>
                              <mn>1</mn>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mo>-</mo>
                              <mi>log</mi>
                              <mo>(</mo>
                              <mn>1</mn>
                              <mo>-</mo>
                              <mover accent="true">
                                <mi>p</mi>
                                <mo>^</mo>
                              </mover>
                              <mo>)</mo>
                            </mtd>
                            <mtd>
                              <mtext>if </mtext>
                              <mi>y</mi>
                              <mo>=</mo>
                              <mn>0</mn>
                            </mtd>
                          </mtr>
                        </mtable>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <p>This cost function makes sense because – log(<em>t</em>) grows very large when <em>t</em> approaches
                  0, so the cost will be large if the model estimates a probability close to 0 for a positive instance,
                  and it will also be very large if the model estimates a probability close to 1 for a negative
                  instance. On the other hand, – log(<em>t</em>) is close to 0 when <em>t</em> is close to 1, so the
                  cost will be close to 0 if the estimated probability is close to 0 for a negative instance or close to
                  1 for a positive instance, which is precisely what we want.</p>

                <p>The cost function over the whole training set is simply the average cost over all training instances.
                  It can be written in a single expression (as you can verify easily), called <a data-type="indexterm"
                    data-primary="log loss" id="idm139656378008752" />the <em>log loss</em>, shown in <a
                    data-type="xref" href="#logistic_regression_cost_function">Equation 4-17</a>.</p>
                <div id="logistic_regression_cost_function" data-type="equation">
                  <h5><span class="label">Equation 4-17. </span>Logistic Regression cost function (log loss)</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>J</mi>
                      <mo>(</mo>
                      <mi mathvariant="bold">θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>=</mo>
                    <mrow>
                      <mo>-</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mi>m</mi>
                      </mfrac>
                      <munderover>
                        <mo>∑</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>m</mi>
                      </munderover>
                      <mfenced open="[" close="]">
                        <mrow>
                          <msup>
                            <mi>y</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi>i</mi>
                              <mo>)</mo>
                            </mrow>
                          </msup>
                          <mi>l</mi>
                          <mi>o</mi>
                          <mi>g</mi>
                          <mfenced>
                            <msup>
                              <mover accent="true">
                                <mi>p</mi>
                                <mo>^</mo>
                              </mover>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                          </mfenced>
                          <mo>+</mo>
                          <mo>(</mo>
                          <mn>1</mn>
                          <mo>-</mo>
                          <msup>
                            <mi>y</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi>i</mi>
                              <mo>)</mo>
                            </mrow>
                          </msup>
                          <mo>)</mo>
                          <mi>l</mi>
                          <mi>o</mi>
                          <mi>g</mi>
                          <mfenced>
                            <mrow>
                              <mn>1</mn>
                              <mo>-</mo>
                              <msup>
                                <mover accent="true">
                                  <mi>p</mi>
                                  <mo>^</mo>
                                </mover>
                                <mrow>
                                  <mo>(</mo>
                                  <mi>i</mi>
                                  <mo>)</mo>
                                </mrow>
                              </msup>
                            </mrow>
                          </mfenced>
                        </mrow>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <p>The bad news is that there is no known <a data-type="indexterm" data-primary="closed-form equation"
                    id="idm139656377987664" />closed-form equation to compute the value of <strong>θ</strong> that
                  minimizes this cost function (there is no equivalent of the Normal Equation). But the good news is
                  that this cost function is convex, so Gradient Descent (or any other optimization algorithm) is
                  guaranteed to find the global minimum (if the learning rate is not too large and you wait long
                  enough). The partial derivatives of the cost function with regards to the j<sup>th</sup> model
                  parameter <em>θ</em><sub><em>j</em></sub> is given by <a data-type="xref"
                    href="#logistic_cost_function_partial_derivatives">Equation 4-18</a>.</p>
                <div id="logistic_cost_function_partial_derivatives" data-type="equation">
                  <h5><span class="label">Equation 4-18. </span>Logistic cost function partial derivatives</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mi>∂</mi>
                          <mrow>
                            <mi>∂</mi>
                            <msub>
                              <mi>θ</mi>
                              <mi>j</mi>
                            </msub>
                          </mrow>
                        </mfrac>
                      </mstyle>
                      <mtext>J</mtext>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">θ</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mn>1</mn>
                          <mi>m</mi>
                        </mfrac>
                      </mstyle>
                      <munderover>
                        <mo>∑</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>m</mi>
                      </munderover>
                      <mfenced separators="" open="(" close=")">
                        <mi>σ</mi>
                        <mrow>
                          <mo>(</mo>
                          <msup>
                            <mi mathvariant="bold">θ</mi>
                            <mi>T</mi>
                          </msup>
                          <msup>
                            <mi mathvariant="bold">x</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi>i</mi>
                              <mo>)</mo>
                            </mrow>
                          </msup>
                          <mo>)</mo>
                        </mrow>
                        <mo>-</mo>
                        <msup>
                          <mi>y</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>i</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                      </mfenced>
                      <mspace width="0.166667em" />
                      <msubsup>
                        <mi>x</mi>
                        <mi>j</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>i</mi>
                          <mo>)</mo>
                        </mrow>
                      </msubsup>
                    </mrow>
                  </math>
                </div>

                <p>This equation looks very much like <a data-type="xref" href="#mse_partial_derivatives">Equation
                    4-5</a>: for each instance it computes the prediction error and multiplies it by the j<sup>th</sup>
                  feature value, and then it computes the average over all training instances. Once you have the
                  gradient vector containing all the partial derivatives you can use it in the Batch Gradient Descent
                  algorithm. That’s it: you now know how to train a Logistic Regression model. For <a
                    data-type="indexterm" data-primary="Stochastic Gradient Descent (SGD)" data-secondary="training"
                    id="idm139656377957072" />Stochastic GD you would of course just take one instance at a time, and
                  for <a data-type="indexterm" data-primary="Mini-batch Gradient Descent"
                    id="idm139656377955952" />Mini-batch GD you would use a <a data-type="indexterm"
                    data-primary="Logistic Regression" data-secondary="training and cost function"
                    data-startref="lr4tacf" id="idm139656377955136" /><a data-type="indexterm"
                    data-primary="training set" data-secondary="cost function of" data-startref="ts4cfo"
                    id="idm139656377953856" /><a data-type="indexterm" data-primary="cost function"
                    data-secondary="in Logistic Regression" data-startref="cf4" id="idm139656377952640" />mini-batch at
                  a time.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Decision Boundaries">
              <div class="sect2" id="idm139656377951296">
                <h2>Decision Boundaries</h2>

                <p>Let’s <a data-type="indexterm" data-primary="Logistic Regression"
                    data-secondary="decision boundaries" id="lr4db" /><a data-type="indexterm"
                    data-primary="decision boundaries" id="db4" />use the iris dataset to illustrate Logistic
                  Regression. This is a famous dataset that contains the sepal and petal length and width of 150 iris
                  flowers of three different species: Iris-Setosa, Iris-Versicolor, and Iris-Virginica (see <a
                    data-type="xref" href="#iris_dataset_diagram">Figure 4-22</a>).</p>

                <figure>
                  <div id="iris_dataset_diagram" class="figure">
                    <img src="mlst_0416.png" alt="mlst 0416" width="1762" height="979" />
                    <h6><span class="label">Figure 4-22. </span>Flowers of three iris plant species<sup><a
                          data-type="noteref" id="idm139656377944320-marker"
                          href="ch04.xhtml#idm139656377944320">16</a></sup></h6>
                  </div>
                </figure>

                <p>Let’s try to build a classifier to detect the Iris-Virginica type based only on the petal width
                  feature. <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.datasets.load_iris()" id="idm139656377941536" />First let’s load the data:
                </p>

                <pre data-type="programlisting"
                  data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn</code> <code class="kn">import</code> <code class="n">datasets</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">iris</code> <code class="o">=</code> <code class="n">datasets</code><code class="o">.</code><code class="n">load_iris</code><code class="p">()</code>
<code class="gp">&gt;&gt;&gt; </code><code class="nb">list</code><code class="p">(</code><code class="n">iris</code><code class="o">.</code><code class="n">keys</code><code class="p">())</code>
<code class="go">['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename']</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="p">[</code><code class="s">"data"</code><code class="p">][:,</code> <code class="mi">3</code><code class="p">:]</code>  <code class="c"># petal width</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">y</code> <code class="o">=</code> <code class="p">(</code><code class="n">iris</code><code class="p">[</code><code class="s">"target"</code><code class="p">]</code> <code class="o">==</code> <code class="mi">2</code><code class="p">)</code><code class="o">.</code><code class="n">astype</code><code class="p">(</code><code class="n">np</code><code class="o">.</code><code class="n">int</code><code class="p">)</code>  <code class="c"># 1 if Iris-Virginica, else 0</code></pre>

                <p>Now let’s train a <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.linear_model.LogisticRegression" id="idm139656377937840" />Logistic
                  Regression model:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>

<code class="n">log_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">()</code>
<code class="n">log_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

                <p>Let’s look at the model’s estimated probabilities for flowers with petal widths varying from 0 to 3
                  cm (<a data-type="xref" href="#logistic_regression_plot">Figure 4-23</a>)<sup><a data-type="noteref"
                      id="idm139656377775728-marker" href="ch04.xhtml#idm139656377775728">17</a></sup>:</p>

                <pre data-type="programlisting" data-code-language="python"><code class="n">X_new</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mi">0</code><code class="p">,</code> <code class="mi">3</code><code class="p">,</code> <code class="mi">1000</code><code class="p">)</code><code class="o">.</code><code class="n">reshape</code><code class="p">(</code><code class="o">-</code><code class="mi">1</code><code class="p">,</code> <code class="mi">1</code><code class="p">)</code>
<code class="n">y_proba</code> <code class="o">=</code> <code class="n">log_reg</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">(</code><code class="n">X_new</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X_new</code><code class="p">,</code> <code class="n">y_proba</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">],</code> <code class="s2">"g-"</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Iris-Virginica"</code><code class="p">)</code>
<code class="n">plt</code><code class="o">.</code><code class="n">plot</code><code class="p">(</code><code class="n">X_new</code><code class="p">,</code> <code class="n">y_proba</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">],</code> <code class="s2">"b--"</code><code class="p">,</code> <code class="n">label</code><code class="o">=</code><code class="s2">"Not Iris-Virginica"</code><code class="p">)</code>
<code class="c1"># + more Matplotlib code to make the image look pretty</code></pre>

                <figure>
                  <div id="logistic_regression_plot" class="figure">
                    <img src="mlst_04in07.png" alt="mlst 04in07" width="2301" height="791" />
                    <h6><span class="label">Figure 4-23. </span>Estimated probabilities and decision boundary</h6>
                  </div>
                </figure>

                <p>The petal width of Iris-Virginica flowers (represented by triangles) ranges from 1.4 cm to 2.5 cm,
                  while the other iris flowers (represented by squares) generally have a smaller petal width, ranging
                  from 0.1 cm to 1.8 cm. Notice that there is a bit of overlap. Above about 2 cm the classifier is
                  highly confident that the flower is an Iris-Virginica (it outputs a high probability to that class),
                  while below 1 cm it is highly confident that it is not an Iris-Virginica (high probability for the
                  “Not Iris-Virginica” class). In between these extremes, the classifier is unsure. However, if you ask
                  it to predict the class (using the <code>predict()</code> method rather than the
                  <code>predict_proba()</code> method), it will return whichever class is the most likely. Therefore,
                  there is a <em>decision boundary</em> at around 1.6 cm where both probabilities are equal to 50%: if
                  the petal width is higher than 1.6 cm, the classifier will predict that the flower is an
                  Iris-Virginica, or else it will predict that it is not (even if it is not very confident):</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">log_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mf">1.7</code><code class="p">],</code> <code class="p">[</code><code class="mf">1.5</code><code class="p">]])</code>
<code class="go">array([1, 0])</code></pre>

                <p><a data-type="xref" href="#logistic_regression_contour_plot">Figure 4-24</a> shows the same dataset
                  but this time displaying two features: petal width and length. Once trained, the Logistic Regression
                  classifier can estimate the probability that a new flower is an Iris-Virginica based on these two
                  features. The dashed line represents the points where the model estimates a 50% probability: this is
                  the model’s decision boundary. Note that it is a linear boundary.<sup><a data-type="noteref"
                      id="idm139656377539856-marker" href="ch04.xhtml#idm139656377539856">18</a></sup> Each parallel
                  line represents the points where the model outputs a specific probability, from 15% (bottom left) to
                  90% (top right). All the flowers beyond the top-right line have an over 90% chance of being
                  Iris-Virginica according to the model.</p>

                <figure>
                  <div id="logistic_regression_contour_plot" class="figure">
                    <img src="mlst_0417.png" alt="mlst 0417" width="2908" height="1090" />
                    <h6><span class="label">Figure 4-24. </span>Linear decision boundary</h6>
                  </div>
                </figure>

                <p>Just like the other linear models, Logistic Regression models can be regularized using <a
                    data-type="indexterm" data-primary="ℓ 2 norm" id="idm139656377698336" /><a data-type="indexterm"
                    data-primary="ℓ 1 norm" id="idm139656377697632" />ℓ<sub>1</sub> or ℓ<sub>2</sub> penalties. <a
                    data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.linear_model.LogisticRegression" id="idm139656377695968" />Scitkit-Learn
                  actually adds an ℓ<sub>2</sub> penalty by default.</p>
                <div data-type="note" epub:type="note">
                  <h6>Note</h6>
                  <p>The hyperparameter controlling the regularization strength of a Scikit-Learn
                    <code>LogisticRegression</code> model is not <code>alpha</code> (as in other linear models), but its
                    inverse: <code>C</code>. The higher the value of <code>C</code>, the <em>less</em> the model is <a
                      data-type="indexterm" data-primary="Logistic Regression" data-secondary="decision boundaries"
                      data-startref="lr4db" id="idm139656377626800" /><a data-type="indexterm"
                      data-primary="decision boundaries" data-startref="db4" id="idm139656377625520" />regularized.</p>
                </div>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Softmax Regression">
              <div class="sect2" id="idm139656377950672">
                <h2>Softmax Regression</h2>

                <p>The <a data-type="indexterm" data-primary="Logistic Regression"
                    data-secondary="Softmax Regression model" id="lr4srm" /><a data-type="indexterm"
                    data-primary="Softmax Regression" id="sr4" /><a data-type="indexterm"
                    data-primary="Multinomial Logistic Regression" data-see="Softmax Regression"
                    id="idm139656377620464" />Logistic Regression model can be generalized to support multiple classes
                  directly, without having to train and combine multiple binary classifiers (as discussed in <a
                    data-type="xref" href="ch03.xhtml#classification_chapter">Chapter 3</a>). This is called <em>Softmax
                    Regression</em>, or <em>Multinomial Logistic Regression</em>.</p>

                <p>The idea is quite simple: when given an instance <strong>x</strong>, the Softmax Regression model
                  first computes a score <em>s</em><sub><em>k</em></sub>(<strong>x</strong>) for each class <em>k</em>,
                  then estimates the probability of each class by applying <a data-type="indexterm"
                    data-primary="softmax function" id="idm139656377614704" /><a data-type="indexterm"
                    data-primary="normalized exponential" id="idm139656377614000" />the <em>softmax function</em> (also
                  called the <em>normalized exponential</em>) to the scores. The equation to compute
                  <em>s</em><sub><em>k</em></sub>(<strong>x</strong>) should look familiar, as it is just like the
                  equation for Linear Regression prediction (see <a data-type="xref"
                    href="#softmax_score_for_class_k">Equation 4-19</a>).</p>
                <div class="fifty-percent" id="softmax_score_for_class_k" data-type="equation">
                  <h5><span class="label">Equation 4-19. </span>Softmax score for class k</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <msub>
                        <mi>s</mi>
                        <mi>k</mi>
                      </msub>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">x</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <msup>
                        <mi mathvariant="bold">x</mi>
                        <mi>T</mi>
                      </msup>
                      <msup>
                        <mi mathvariant="bold">θ</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>k</mi>
                          <mo>)</mo>
                        </mrow>
                      </msup>
                    </mrow>
                  </math>
                </div>

                <p>Note that each class has its own dedicated <a data-type="indexterm" data-primary="parameter vector"
                    id="idm139656377599632" />parameter vector <strong>θ</strong><sup><em>(k)</em></sup>. All these
                  vectors are typically stored as rows <a data-type="indexterm" data-primary="parameter matrix"
                    id="idm139656377597872" />in a <em>parameter matrix</em> <strong>Θ</strong>.</p>

                <p>Once you have computed the score of every class for the instance <strong>x</strong>, you can estimate
                  the probability <math xmlns="http://www.w3.org/1998/Math/MathML"
                    alttext="ModifyingAbove p With caret">
                    <mover accent="true">
                      <mi>p</mi>
                      <mo>^</mo>
                    </mover>
                  </math><sub><em>k</em></sub> that the instance belongs to class <em>k</em> by running the scores
                  through the softmax function (<a data-type="xref" href="#softmax_function">Equation 4-20</a>): it
                  computes the exponential of every score, then normalizes them (dividing by the sum of all the
                  exponentials). The scores are generally called logits or log-odds (although they are actually
                  unnormalized log-odds).</p>
                <div id="softmax_function" data-type="equation">
                  <h5><span class="label">Equation 4-20. </span>Softmax function</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <msub>
                        <mover accent="true">
                          <mi>p</mi>
                          <mo>^</mo>
                        </mover>
                        <mi>k</mi>
                      </msub>
                      <mo>=</mo>
                      <mi>σ</mi>
                      <msub>
                        <mfenced separators="" open="(" close=")">
                          <mi mathvariant="bold">s</mi>
                          <mo>(</mo>
                          <mi mathvariant="bold">x</mi>
                          <mo>)</mo>
                        </mfenced>
                        <mi>k</mi>
                      </msub>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mrow>
                            <mo form="prefix">exp</mo>
                            <mfenced separators="" open="(" close=")">
                              <msub>
                                <mi>s</mi>
                                <mi>k</mi>
                              </msub>
                              <mrow>
                                <mo>(</mo>
                                <mi mathvariant="bold">x</mi>
                                <mo>)</mo>
                              </mrow>
                            </mfenced>
                          </mrow>
                          <mrow>
                            <munderover>
                              <mo>∑</mo>
                              <mrow>
                                <mi>j</mi>
                                <mo>=</mo>
                                <mn>1</mn>
                              </mrow>
                              <mi>K</mi>
                            </munderover>
                            <mrow>
                              <mo form="prefix">exp</mo>
                              <mfenced separators="" open="(" close=")">
                                <msub>
                                  <mi>s</mi>
                                  <mi>j</mi>
                                </msub>
                                <mrow>
                                  <mo>(</mo>
                                  <mi mathvariant="bold">x</mi>
                                  <mo>)</mo>
                                </mrow>
                              </mfenced>
                            </mrow>
                          </mrow>
                        </mfrac>
                      </mstyle>
                    </mrow>
                  </math>
                </div>

                <ul>
                  <li>
                    <p><em>K</em> is the number of classes.</p>
                  </li>
                  <li>
                    <p><strong>s</strong>(<strong>x</strong>) is a vector containing the scores of each class for the
                      instance <strong>x</strong>.</p>
                  </li>
                  <li>
                    <p><em>σ</em>(<strong>s</strong>(<strong>x</strong>))<sub><em>k</em></sub> is the estimated
                      probability that the instance <strong>x</strong> belongs to class <em>k</em> given the scores of
                      each class for that instance.</p>
                  </li>
                </ul>

                <p>Just like the Logistic Regression classifier, the Softmax Regression classifier predicts the class
                  with the highest estimated probability (which is simply the class with the highest score), as shown in
                  <a data-type="xref" href="#softmax_regression_classifier_prediction">Equation 4-21</a>.</p>
                <div id="softmax_regression_classifier_prediction" data-type="equation">
                  <h5><span class="label">Equation 4-21. </span>Softmax Regression classifier prediction</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mover accent="true">
                        <mi>y</mi>
                        <mo>^</mo>
                      </mover>
                      <mo>=</mo>
                      <munder>
                        <mo form="prefix">argmax</mo>
                        <mi>k</mi>
                      </munder>
                      <mspace width="0.166667em" />
                      <mi>σ</mi>
                      <msub>
                        <mfenced separators="" open="(" close=")">
                          <mi mathvariant="bold">s</mi>
                          <mo>(</mo>
                          <mi mathvariant="bold">x</mi>
                          <mo>)</mo>
                        </mfenced>
                        <mi>k</mi>
                      </msub>
                      <mo>=</mo>
                      <munder>
                        <mo form="prefix">argmax</mo>
                        <mi>k</mi>
                      </munder>
                      <mspace width="0.166667em" />
                      <msub>
                        <mi>s</mi>
                        <mi>k</mi>
                      </msub>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">x</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <munder>
                        <mo form="prefix">argmax</mo>
                        <mi>k</mi>
                      </munder>
                      <mspace width="0.166667em" />
                      <mfenced separators="" open="(" close=")">
                        <msup>
                          <mrow>
                            <mo>(</mo>
                            <msup>
                              <mi mathvariant="bold">θ</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>k</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mo>)</mo>
                          </mrow>
                          <mi>T</mi>
                        </msup>
                        <mi mathvariant="bold">x</mi>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <ul>
                  <li>
                    <p>The <em>argmax</em> operator returns the value of a variable that maximizes a function. In this
                      equation, it returns the value of <em>k</em> that maximizes the estimated probability
                      <em>σ</em>(<strong>s</strong>(<strong>x</strong>))<sub><em>k</em></sub>.</p>
                  </li>
                </ul>
                <div data-type="tip">
                  <h6>Tip</h6>
                  <p>The Softmax Regression classifier predicts only one class at a time (i.e., it is multiclass, not
                    multioutput) so it should be used only with mutually exclusive classes such as different types of
                    plants. You cannot use it to recognize multiple people in one picture.</p>
                </div>

                <p>Now that you know how the model estimates probabilities and makes predictions, let’s take a look at
                  training. The objective is to have a model that estimates a high probability for the target class (and
                  consequently a low probability for the other classes). Minimizing the cost function shown in <a
                    data-type="xref" href="#cross_entropy_cost_function">Equation 4-22</a>, called <a
                    data-type="indexterm" data-primary="cross entropy" id="ce4" />the <em>cross entropy</em>, should
                  lead to this objective because it penalizes the model when it estimates a low probability for a target
                  class. Cross entropy is frequently used to measure how well a set of estimated class probabilities
                  match the target classes (we will use it again several times in the following chapters).</p>
                <div id="cross_entropy_cost_function" data-type="equation">
                  <h5><span class="label">Equation 4-22. </span>Cross entropy cost function</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML">
                    <mrow>
                      <mi>J</mi>
                      <mo>(</mo>
                      <mi mathvariant="bold">Θ</mi>
                      <mo>)</mo>
                    </mrow>
                    <mo>=</mo>
                    <mrow>
                      <mo>-</mo>
                      <mfrac>
                        <mn>1</mn>
                        <mi>m</mi>
                      </mfrac>
                      <munderover>
                        <mo>∑</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>m</mi>
                      </munderover>
                      <munderover>
                        <mo>∑</mo>
                        <mrow>
                          <mi>k</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>K</mi>
                      </munderover>
                      <msubsup>
                        <mi>y</mi>
                        <mi>k</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>i</mi>
                          <mo>)</mo>
                        </mrow>
                      </msubsup>
                      <mi>log</mi>
                      <mfenced>
                        <msubsup>
                          <mover accent="true">
                            <mi>p</mi>
                            <mo>^</mo>
                          </mover>
                          <mi>k</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>i</mi>
                            <mo>)</mo>
                          </mrow>
                        </msubsup>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <ul>
                  <li>
                    <p><math xmlns="http://www.w3.org/1998/Math/MathML"
                        alttext="y Subscript k Superscript left-parenthesis i right-parenthesis">
                        <msubsup>
                          <mi>y</mi>
                          <mi>k</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>i</mi>
                            <mo>)</mo>
                          </mrow>
                        </msubsup>
                      </math> is the target probability that the i<sup>th</sup> instance belongs to class <em>k</em>. In
                      general, it is either equal to 1 or 0, depending on whether the instance belongs to the class or
                      not.</p>
                  </li>
                </ul>

                <p>Notice that when there are just two classes (<em>K</em> = 2), this cost function is equivalent to the
                  Logistic Regression’s cost function (log loss; see <a data-type="xref"
                    href="#logistic_regression_cost_function">Equation 4-17</a>).</p>
                <aside data-type="sidebar" epub:type="sidebar">
                  <div class="sidebar" id="idm139656377456736">
                    <h5>Cross Entropy</h5>
                    <p>Cross entropy originated from information theory. Suppose you want to efficiently transmit
                      information about the weather every day. If there are eight options (sunny, rainy, etc.), you
                      could encode each option using 3 bits since 2<sup>3</sup> = 8. However, if you think it will be
                      sunny almost every day, it would be much more efficient to code “sunny” on just one bit (0) and
                      the other seven options on 4 bits (starting with a 1). Cross entropy measures the average number
                      of bits you actually send per option. If your assumption about the weather is perfect, cross
                      entropy will just be equal to the entropy of the weather itself (i.e., its intrinsic
                      unpredictability). But if your assumptions are wrong (e.g., if it rains often), cross entropy will
                      be greater by an amount <a data-type="indexterm" data-primary="Kullback–Leibler divergence"
                        id="idm139656377454192" />called the <em>Kullback–Leibler divergence</em>.</p>

                    <p>The cross entropy between two probability distributions <em>p</em> and <em>q</em> is defined as
                      <math xmlns="http://www.w3.org/1998/Math/MathML">
                        <mrow>
                          <mi>H</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>p</mi>
                            <mo>,</mo>
                            <mi>q</mi>
                            <mo>)</mo>
                          </mrow>
                          <mo>=</mo>
                          <mo>-</mo>
                          <munder>
                            <mo>∑</mo>
                            <mi>x</mi>
                          </munder>
                          <mi>p</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>x</mi>
                            <mo>)</mo>
                          </mrow>
                          <mo form="prefix">log</mo>
                          <mi>q</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>x</mi>
                            <mo>)</mo>
                          </mrow>
                        </mrow>
                      </math> (at least when the distributions are discrete). For more details, check out <a
                        href="https://homl.info/xentropy">this video</a>.</p>
                  </div>
                </aside>

                <p>The <a data-type="indexterm" data-primary="cross entropy" data-startref="ce4"
                    id="idm139656377438752" />gradient vector of this cost function with regards to
                  <strong>θ</strong><sup><em>(k)</em></sup> is given by <a data-type="xref"
                    href="#cross_entropy_gradient_vector_for_class_k">Equation 4-23</a>:</p>
                <div id="cross_entropy_gradient_vector_for_class_k" data-type="equation">
                  <h5><span class="label">Equation 4-23. </span>Cross entropy gradient vector for class k</h5><math
                    xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <msub>
                        <mi>∇</mi>
                        <msup>
                          <mi mathvariant="bold">θ</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>k</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                      </msub>
                      <mspace width="0.166667em" />
                      <mi>J</mi>
                      <mrow>
                        <mo>(</mo>
                        <mi mathvariant="bold">Θ</mi>
                        <mo>)</mo>
                      </mrow>
                      <mo>=</mo>
                      <mstyle scriptlevel="0" displaystyle="true">
                        <mfrac>
                          <mn>1</mn>
                          <mi>m</mi>
                        </mfrac>
                      </mstyle>
                      <munderover>
                        <mo>∑</mo>
                        <mrow>
                          <mi>i</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>m</mi>
                      </munderover>
                      <mrow>
                        <mfenced separators="" open="(" close=")">
                          <msubsup>
                            <mover accent="true">
                              <mi>p</mi>
                              <mo>^</mo>
                            </mover>
                            <mi>k</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi>i</mi>
                              <mo>)</mo>
                            </mrow>
                          </msubsup>
                          <mo>-</mo>
                          <msubsup>
                            <mi>y</mi>
                            <mi>k</mi>
                            <mrow>
                              <mo>(</mo>
                              <mi>i</mi>
                              <mo>)</mo>
                            </mrow>
                          </msubsup>
                        </mfenced>
                        <msup>
                          <mi mathvariant="bold">x</mi>
                          <mrow>
                            <mo>(</mo>
                            <mi>i</mi>
                            <mo>)</mo>
                          </mrow>
                        </msup>
                      </mrow>
                    </mrow>
                  </math>
                </div>

                <p>Now you can compute the gradient vector for every class, then use Gradient Descent (or any other
                  optimization algorithm) to find the parameter matrix <strong>Θ</strong> that minimizes the cost
                  function.</p>

                <p>Let’s use Softmax Regression to classify the iris flowers into all three classes. Scikit-Learn’s
                  <code>LogisticRegression</code> <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.linear_model.LogisticRegression" id="idm139656377410544" />uses <a
                    data-type="indexterm" data-primary="one-versus-all   (OvA)  strategy"
                    id="idm139656377409440" />one-versus-all by default when you train it on more than two classes, but
                  you can set the <code>multi_class</code> hyperparameter to <code>"multinomial"</code> to switch it to
                  Softmax Regression instead. You must also specify a solver that supports Softmax Regression, such as
                  the <code>"lbfgs"</code> solver (see Scikit-Learn’s documentation for more details). It also applies
                  ℓ<sub>2</sub> <a data-type="indexterm" data-primary="ℓ 2 norm"
                    id="idm139656377406720" />regularization by default, which you can control using the hyperparameter
                  <code>C</code>.</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">X</code> <code class="o">=</code> <code class="n">iris</code><code class="p">[</code><code class="s2">"data"</code><code class="p">][:,</code> <code class="p">(</code><code class="mi">2</code><code class="p">,</code> <code class="mi">3</code><code class="p">)]</code>  <code class="c1"># petal length, petal width</code>
<code class="n">y</code> <code class="o">=</code> <code class="n">iris</code><code class="p">[</code><code class="s2">"target"</code><code class="p">]</code>

<code class="n">softmax_reg</code> <code class="o">=</code> <code class="n">LogisticRegression</code><code class="p">(</code><code class="n">multi_class</code><code class="o">=</code><code class="s2">"multinomial"</code><code class="p">,</code><code class="n">solver</code><code class="o">=</code><code class="s2">"lbfgs"</code><code class="p">,</code> <code class="n">C</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
<code class="n">softmax_reg</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

                <p>So the next time you find an iris with 5 cm long and 2 cm wide petals, you can ask your model to tell
                  you what type of iris it is, and it will answer Iris-Virginica (class 2) with 94.2% probability (or
                  Iris-Versicolor with 5.8% probability):</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">softmax_reg</code><code class="o">.</code><code class="n">predict</code><code class="p">([[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">2</code><code class="p">]])</code>
<code class="go">array([2])</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">softmax_reg</code><code class="o">.</code><code class="n">predict_proba</code><code class="p">([[</code><code class="mi">5</code><code class="p">,</code> <code class="mi">2</code><code class="p">]])</code>
<code class="go">array([[6.38014896e-07, 5.74929995e-02, 9.42506362e-01]])</code></pre>

                <p><a data-type="xref" href="#softmax_regression_contour_plot">Figure 4-25</a> shows the resulting <a
                    data-type="indexterm" data-primary="decision boundaries" id="idm139656377353680" />decision
                  boundaries, represented by the background colors. Notice that the decision boundaries between any two
                  classes are linear. The figure also shows the probabilities for the Iris-Versicolor class, represented
                  by the curved lines (e.g., the line labeled with 0.450 represents the 45% probability boundary).
                  Notice that the model can predict a class that has an estimated probability below 50%. For example, at
                  the point where all decision boundaries meet, all classes have an equal estimated probability <a
                    data-type="indexterm" data-primary="training models" data-secondary="Logistic Regression"
                    data-startref="tm4logr" id="idm139656377352416" /><a data-type="indexterm"
                    data-primary="Logistic Regression" data-startref="lreg4" id="idm139656377222976" /><a
                    data-type="indexterm" data-primary="Logistic Regression" data-secondary="Softmax Regression model"
                    data-startref="lr4srm" id="idm139656377222032" /><a data-type="indexterm"
                    data-primary="Softmax Regression" data-startref="sr4" id="idm139656377220848" />of 33%.</p>

                <figure>
                  <div id="softmax_regression_contour_plot" class="figure">
                    <img src="mlst_0418.png" alt="mlst 0418" width="2906" height="1101" />
                    <h6><span class="label">Figure 4-25. </span>Softmax Regression decision boundaries</h6>
                  </div>
                </figure>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Exercises">
          <div class="sect1" id="idm139656377623536">
            <h1>Exercises</h1>
            <ol>
              <li>
                <p>What Linear Regression training algorithm can you use if you have a training set with millions of
                  features?</p>
              </li>
              <li>
                <p>Suppose the features in your training set have very different scales. What algorithms might suffer
                  from this, and how? What can you do about it?</p>
              </li>
              <li>
                <p>Can Gradient Descent get stuck in a local minimum when training a Logistic Regression model?</p>
              </li>
              <li>
                <p>Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?</p>
              </li>
              <li>
                <p>Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you
                  notice that the validation error consistently goes up, what is likely going on? How can you fix this?
                </p>
              </li>
              <li>
                <p>Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?
                </p>
              </li>
              <li>
                <p>Which Gradient Descent algorithm (among those we discussed) will reach the vicinity of the optimal
                  solution the fastest? Which will actually converge? How can you make the others converge as well?</p>
              </li>
              <li>
                <p>Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there
                  is a large gap between the training error and the validation error. What is happening? What are three
                  ways to solve this?</p>
              </li>
              <li>
                <p>Suppose you are using Ridge Regression and you notice that the training error and the validation
                  error are almost equal and fairly high. Would you say that the model suffers from high bias or high
                  variance? Should you increase the regularization hyperparameter <em>α</em> or reduce it?</p>
              </li>
              <li>
                <p>Why would you want to use:</p>

                <ul>
                  <li>
                    <p>Ridge Regression instead of plain Linear Regression (i.e., without any regularization)?</p>
                  </li>
                  <li>
                    <p>Lasso instead of Ridge Regression?</p>
                  </li>
                  <li>
                    <p>Elastic Net instead of Lasso?</p>
                  </li>
                </ul>
              </li>
              <li>
                <p>Suppose you want to classify pictures as outdoor/indoor and daytime/nighttime. Should you implement
                  two Logistic Regression classifiers or one Softmax Regression classifier?</p>
              </li>
              <li>
                <p>Implement Batch Gradient Descent with early stopping for Softmax Regression <a data-type="indexterm"
                    data-primary="training models" data-startref="tm4" id="idm139656377327808" />(without using
                  Scikit-Learn).</p>
              </li>

            </ol>

            <p>Solutions to these exercises are available in Appendix A.</p>
          </div>
        </section>







        <div data-type="footnotes">
          <p data-type="footnote" id="idm139656380507440"><sup><a
                href="ch04.xhtml#idm139656380507440-marker">1</a></sup> It is often the case that a learning algorithm
            will try to optimize a different function than the performance measure used to evaluate the final model.
            This is generally because that function is easier to compute, because it has useful differentiation
            properties that the performance measure lacks, or because we want to constrain the model during training, as
            we will see when we discuss regularization.</p>
          <p data-type="footnote" id="idm139656380470992"><sup><a
                href="ch04.xhtml#idm139656380470992-marker">2</a></sup> The demonstration that this returns the value of
            <strong>θ</strong> that minimizes the cost function is outside the scope of this book.</p>
          <p data-type="footnote" id="idm139656380084896"><sup><a
                href="ch04.xhtml#idm139656380084896-marker">3</a></sup> Note that Scikit-Learn separates the bias term
            (<code>intercept_</code>) from the feature weights (<code>coef_</code>).</p>
          <p data-type="footnote" id="idm139656380160352"><sup><a
                href="ch04.xhtml#idm139656380160352-marker">4</a></sup> Technically speaking, its derivative is
            <em>Lipschitz continuous</em>.</p>
          <p data-type="footnote" id="idm139656380156704"><sup><a
                href="ch04.xhtml#idm139656380156704-marker">5</a></sup> Since feature 1 is smaller, it takes a larger
            change in <em>θ</em><sub>1</sub> to affect the cost function, which is why the bowl is elongated along the
            <em>θ</em><sub>1</sub> axis.</p>
          <p data-type="footnote" id="idm139656379801232"><sup><a
                href="ch04.xhtml#idm139656379801232-marker">6</a></sup> Eta (<em>η</em>) is the 7<sup>th</sup> letter of
            the Greek alphabet.</p>
          <p data-type="footnote" id="idm139656379704064"><sup><a
                href="ch04.xhtml#idm139656379704064-marker">7</a></sup> Out-of-core algorithms are discussed in <a
              data-type="xref" href="ch01.xhtml#landscape_chapter">Chapter 1</a>.</p>
          <p data-type="footnote" id="idm139656379289824"><sup><a
                href="ch04.xhtml#idm139656379289824-marker">8</a></sup> While the Normal Equation can only perform
            Linear Regression, the Gradient Descent algorithms can be used to train many other models, <a
              data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.linear_model.LinearRegression"
              id="idm139656379289120" /><a data-type="indexterm" data-primary="Scikit-Learn"
              data-secondary="sklearn.linear_model.SGDRegressor" data-startref="skllmsgdrch4"
              id="idm139656379288128" />as we will see.</p>
          <p data-type="footnote" id="idm139656379498048"><sup><a
                href="ch04.xhtml#idm139656379498048-marker">9</a></sup> A quadratic equation is of the form <em>y</em> =
            <em>ax</em><sup>2</sup> + <em>bx</em> + <em>c</em>.</p>
          <p data-type="footnote" id="idm139656378714160"><sup><a
                href="ch04.xhtml#idm139656378714160-marker">10</a></sup> This notion of bias is not to be confused with
            the bias term of linear models.</p>
          <p data-type="footnote" id="idm139656378842368"><sup><a
                href="ch04.xhtml#idm139656378842368-marker">11</a></sup> It is common to use the notation
            <em>J</em>(<strong>θ</strong>) for cost functions that don’t have a short name; we will often use this
            notation throughout the rest of this book. The context will make it clear which cost function is being
            discussed.</p>
          <p data-type="footnote" id="idm139656378824048"><sup><a
                href="ch04.xhtml#idm139656378824048-marker">12</a></sup> Norms are discussed in <a data-type="xref"
              href="ch02.xhtml#project_chapter">Chapter 2</a>.</p>
          <p data-type="footnote" id="idm139656378809280"><sup><a
                href="ch04.xhtml#idm139656378809280-marker">13</a></sup> A square matrix full of 0s except for 1s on the
            main diagonal (top-left to bottom-right).</p>
          <p data-type="footnote" id="idm139656378570704"><sup><a
                href="ch04.xhtml#idm139656378570704-marker">14</a></sup> Alternatively you can use the
            <code>Ridge</code> class with the <code>"sag"</code> solver. Stochastic Average GD <a data-type="indexterm"
              data-primary="Scikit-Learn" data-secondary="sklearn.linear_model.SGDRegressor"
              id="idm139656378569408" />is a variant of SGD. For more details, see the presentation <a
              href="https://homl.info/12">“Minimizing Finite Sums with the Stochastic Average Gradient Algorithm”</a> by
            Mark Schmidt et al. from the University of British Columbia.</p>
          <p data-type="footnote" id="idm139656378519040"><sup><a
                href="ch04.xhtml#idm139656378519040-marker">15</a></sup> You can think of a subgradient vector at a
            nondifferentiable point as an intermediate vector between the gradient vectors around that point.</p>
          <p data-type="footnote" id="idm139656377944320"><sup><a
                href="ch04.xhtml#idm139656377944320-marker">16</a></sup> Photos reproduced from the corresponding
            Wikipedia pages. Iris-Virginica photo by Frank Mayfield (<a
              href="https://creativecommons.org/licenses/by-sa/2.0/">Creative Commons BY-SA 2.0</a>), Iris-Versicolor
            photo by D. Gordon E. Robertson (<a href="https://creativecommons.org/licenses/by-sa/3.0/">Creative Commons
              BY-SA 3.0</a>), and Iris-Setosa photo is public domain.</p>
          <p data-type="footnote" id="idm139656377775728"><sup><a
                href="ch04.xhtml#idm139656377775728-marker">17</a></sup> NumPy’s <code>reshape()</code> function allows
            one dimension to be –1, which means “unspecified”: the value is inferred from the length of the array and
            the remaining dimensions.</p>
          <p data-type="footnote" id="idm139656377539856"><sup><a
                href="ch04.xhtml#idm139656377539856-marker">18</a></sup> It is the the set of points <strong>x</strong>
            such that <em>θ</em><sub>0</sub> + <em>θ</em><sub>1</sub><em>x</em><sub>1</sub> +
            <em>θ</em><sub>2</sub><em>x</em><sub>2</sub> = 0, which defines a straight line.</p>
        </div>
      </div>
    </section>
  </div>



</body>

</html>