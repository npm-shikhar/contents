<!DOCTYPE html>
<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
  xsi:schemaLocation="http://www.w3.org/2002/06/xhtml2/ http://www.w3.org/MarkUp/SCHEMA/xhtml2.xsd"
  xmlns:epub="http://www.idpf.org/2007/ops">

<head>
  <link href="Style00.css" rel="stylesheet" type="text/css" />
  <link href="Style01.css" rel="stylesheet" type="text/css" />
  <link href="Style02.css" rel="stylesheet" type="text/css" />
  <link href="Style03.css" rel="stylesheet" type="text/css" />
  <style type="text/css" title="ibis-book">
    @charset "utf-8";

    #sbo-rt-content html,
    #sbo-rt-content div,
    #sbo-rt-content div,
    #sbo-rt-content span,
    #sbo-rt-content applet,
    #sbo-rt-content object,
    #sbo-rt-content iframe,
    #sbo-rt-content h1,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5,
    #sbo-rt-content h6,
    #sbo-rt-content p,
    #sbo-rt-content blockquote,
    #sbo-rt-content pre,
    #sbo-rt-content a,
    #sbo-rt-content abbr,
    #sbo-rt-content acronym,
    #sbo-rt-content address,
    #sbo-rt-content big,
    #sbo-rt-content cite,
    #sbo-rt-content code,
    #sbo-rt-content del,
    #sbo-rt-content dfn,
    #sbo-rt-content em,
    #sbo-rt-content img,
    #sbo-rt-content ins,
    #sbo-rt-content kbd,
    #sbo-rt-content q,
    #sbo-rt-content s,
    #sbo-rt-content samp,
    #sbo-rt-content small,
    #sbo-rt-content strike,
    #sbo-rt-content strong,
    #sbo-rt-content sub,
    #sbo-rt-content sup,
    #sbo-rt-content tt,
    #sbo-rt-content var,
    #sbo-rt-content b,
    #sbo-rt-content u,
    #sbo-rt-content i,
    #sbo-rt-content center,
    #sbo-rt-content dl,
    #sbo-rt-content dt,
    #sbo-rt-content dd,
    #sbo-rt-content ol,
    #sbo-rt-content ul,
    #sbo-rt-content li,
    #sbo-rt-content fieldset,
    #sbo-rt-content form,
    #sbo-rt-content label,
    #sbo-rt-content legend,
    #sbo-rt-content table,
    #sbo-rt-content caption,
    #sbo-rt-content tdiv,
    #sbo-rt-content tfoot,
    #sbo-rt-content thead,
    #sbo-rt-content tr,
    #sbo-rt-content th,
    #sbo-rt-content td,
    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content canvas,
    #sbo-rt-content details,
    #sbo-rt-content embed,
    #sbo-rt-content figure,
    #sbo-rt-content figcaption,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content output,
    #sbo-rt-content ruby,
    #sbo-rt-content section,
    #sbo-rt-content summary,
    #sbo-rt-content time,
    #sbo-rt-content mark,
    #sbo-rt-content audio,
    #sbo-rt-content video {
      margin: 0;
      padding: 0;
      border: 0;
      font-size: 100%;
      font: inherit;
      vertical-align: baseline
    }

    #sbo-rt-content article,
    #sbo-rt-content aside,
    #sbo-rt-content details,
    #sbo-rt-content figcaption,
    #sbo-rt-content figure,
    #sbo-rt-content footer,
    #sbo-rt-content header,
    #sbo-rt-content hgroup,
    #sbo-rt-content menu,
    #sbo-rt-content nav,
    #sbo-rt-content section {
      display: block
    }

    #sbo-rt-content div {
      line-height: 1
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style: none
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content q {
      quotes: none
    }

    #sbo-rt-content blockquote:before,
    #sbo-rt-content blockquote:after,
    #sbo-rt-content q:before,
    #sbo-rt-content q:after {
      content: none
    }

    #sbo-rt-content table {
      border-collapse: collapse;
      border-spacing: 0
    }

    @page {
      margin: 5px !important
    }

    #sbo-rt-content p {
      margin: 10px 0 0;
      line-height: 125%;
      text-align: left
    }

    #sbo-rt-content p.byline {
      text-align: left;
      margin: -33px auto 35px;
      font-style: italic;
      font-weight: bold
    }

    #sbo-rt-content div.preface p+p.byline {
      margin: 1em 0 0 !important
    }

    #sbo-rt-content div.preface p.byline+p.byline {
      margin: 0 !important
    }

    #sbo-rt-content div.sect1&gt;

    p.byline {
      margin: -.25em 0 1em
    }

    #sbo-rt-content div.sect1&gt;

    p.byline+p.byline {
      margin-top: -1em
    }

    #sbo-rt-content em {
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content em strong,
    #sbo-rt-content strong em {
      font-weight: bold;
      font-style: italic;
      font-family: inherit
    }

    #sbo-rt-content strong,
    #sbo-rt-content span.bold {
      font-weight: bold
    }

    #sbo-rt-content em.replaceable {
      font-style: italic
    }

    #sbo-rt-content strong.userinput {
      font-weight: bold;
      font-style: normal
    }

    #sbo-rt-content span.bolditalic {
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content a.ulink,
    #sbo-rt-content a.xref,
    #sbo-rt-content a.email,
    #sbo-rt-content a.link,
    #sbo-rt-content a {
      text-decoration: none;
      color: #8e0012
    }

    #sbo-rt-content span.lineannotation {
      font-style: italic;
      color: #a62a2a;
      font-family: serif
    }

    #sbo-rt-content span.underline {
      text-decoration: underline
    }

    #sbo-rt-content span.strikethrough {
      text-decoration: line-through
    }

    #sbo-rt-content span.smallcaps {
      font-variant: small-caps
    }

    #sbo-rt-content span.cursor {
      background: #000;
      color: #fff
    }

    #sbo-rt-content span.smaller {
      font-size: 75%
    }

    #sbo-rt-content .boxedtext,
    #sbo-rt-content .keycap {
      border-style: solid;
      border-width: 1px;
      border-color: #000;
      padding: 1px
    }

    #sbo-rt-content span.gray50 {
      color: #7F7F7F;
    }

    #sbo-rt-content h1,
    #sbo-rt-content div.toc-title,
    #sbo-rt-content h2,
    #sbo-rt-content h3,
    #sbo-rt-content h4,
    #sbo-rt-content h5 {
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      font-weight: bold;
      text-align: left;
      page-break-after: avoid !important;
      font-family: sans-serif, "DejaVuSans"
    }

    #sbo-rt-content div.toc-title {
      font-size: 1.5em;
      margin-top: 20px !important;
      margin-bottom: 30px !important
    }

    #sbo-rt-content section[data-type="sect1"] h1 {
      font-size: 1.3em;
      color: #8e0012;
      margin: 40px 0 8px 0
    }

    #sbo-rt-content section[data-type="sect2"] h2 {
      font-size: 1.1em;
      margin: 30px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect3"] h3 {
      font-size: 1em;
      color: #555;
      margin: 20px 0 8px 0 !important
    }

    #sbo-rt-content section[data-type="sect4"] h4 {
      font-size: 1em;
      font-weight: normal;
      font-style: italic;
      margin: 15px 0 6px 0 !important
    }

    #sbo-rt-content section[data-type="chapter"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="preface"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="appendix"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="glossary"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="bibliography"]&gt;
    div&gt;
    h1,
    #sbo-rt-content section[data-type="index"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      line-height: 1;
      margin-bottom: 50px;
      color: #000;
      padding-bottom: 10px;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content span.label,
    #sbo-rt-content span.keep-together {
      font-size: inherit;
      font-weight: inherit
    }

    #sbo-rt-content div[data-type="part"] h1 {
      font-size: 2em;
      text-align: center;
      margin-top: 0 !important;
      margin-bottom: 50px;
      padding: 50px 0 10px 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content img.width-ninety {
      width: 90%
    }

    #sbo-rt-content img {
      max-width: 95%;
      margin: 0 auto;
      padding: 0
    }

    #sbo-rt-content div.figure {
      background-color: transparent;
      text-align: center !important;
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content figure {
      margin: 15px 0 15px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.figure h6,
    #sbo-rt-content figure h6,
    #sbo-rt-content figure figcaption {
      font-size: .9rem !important;
      text-align: center;
      font-weight: normal !important;
      font-style: italic;
      font-family: serif !important;
      text-transform: none !important;
      letter-spacing: normal !important;
      color: #000 !important;
      padding-top: 10px !important;
      page-break-before: avoid
    }

    #sbo-rt-content div.informalfigure {
      text-align: center !important;
      padding: 5px 0 !important
    }

    #sbo-rt-content div.sidebar {
      margin: 15px 0 10px 0 !important;
      border: 1px solid #DCDCDC;
      background-color: #F7F7F7;
      padding: 15px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content aside[data-type="sidebar"] {
      margin: 15px 0 10px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar-title,
    #sbo-rt-content aside[data-type="sidebar"] h5 {
      font-weight: bold;
      font-size: 1em;
      font-family: sans-serif;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sidebar ol,
    #sbo-rt-content div.sidebar ul,
    #sbo-rt-content aside[data-type="sidebar"] ol,
    #sbo-rt-content aside[data-type="sidebar"] ul {
      margin-left: 1.25em !important
    }

    #sbo-rt-content div.sidebar div.figure p.title,
    #sbo-rt-content aside[data-type="sidebar"] figcaption,
    #sbo-rt-content div.sidebar div.informalfigure div.caption {
      font-size: 90%;
      text-align: center;
      font-weight: normal;
      font-style: italic;
      font-family: serif !important;
      color: #000;
      padding: 5px !important;
      page-break-before: avoid;
      page-break-after: avoid
    }

    #sbo-rt-content div.sidebar div.tip,
    #sbo-rt-content div.sidebar div[data-type="tip"],
    #sbo-rt-content div.sidebar div.note,
    #sbo-rt-content div.sidebar div[data-type="note"],
    #sbo-rt-content div.sidebar div.warning,
    #sbo-rt-content div.sidebar div[data-type="warning"],
    #sbo-rt-content div.sidebar div[data-type="caution"],
    #sbo-rt-content div.sidebar div[data-type="important"] {
      margin: 20px auto 20px auto !important;
      font-size: 90%;
      width: 85%
    }

    #sbo-rt-content aside[data-type="sidebar"] p.byline {
      font-size: 90%;
      font-weight: bold;
      font-style: italic;
      text-align: center;
      text-indent: 0;
      margin: 5px auto 6px;
      page-break-after: avoid
    }

    #sbo-rt-content pre {
      white-space: pre-wrap;
      font-family: "Ubuntu Mono", monospace;
      margin: 25px 0 25px 20px;
      font-size: 85%;
      display: block;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content div.note pre.programlisting,
    #sbo-rt-content div.tip pre.programlisting,
    #sbo-rt-content div.warning pre.programlisting,
    #sbo-rt-content div.caution pre.programlisting,
    #sbo-rt-content div.important pre.programlisting {
      margin-bottom: 0
    }

    #sbo-rt-content code {
      font-family: "Ubuntu Mono", monospace;
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none;
      overflow-wrap: break-word
    }

    #sbo-rt-content code strong em,
    #sbo-rt-content code em strong,
    #sbo-rt-content pre em strong,
    #sbo-rt-content pre strong em,
    #sbo-rt-content strong code em code,
    #sbo-rt-content em code strong code,
    #sbo-rt-content span.bolditalic code {
      font-weight: bold;
      font-style: italic;
      font-family: "Ubuntu Mono BoldItal", monospace
    }

    #sbo-rt-content code em,
    #sbo-rt-content em code,
    #sbo-rt-content pre em,
    #sbo-rt-content em.replaceable {
      font-family: "Ubuntu Mono Ital", monospace;
      font-style: italic
    }

    #sbo-rt-content code strong,
    #sbo-rt-content strong code,
    #sbo-rt-content pre strong,
    #sbo-rt-content strong.userinput {
      font-family: "Ubuntu Mono Bold", monospace;
      font-weight: bold
    }

    #sbo-rt-content div[data-type="example"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div[data-type="example"] h1,
    #sbo-rt-content div[data-type="example"] h2,
    #sbo-rt-content div[data-type="example"] h3,
    #sbo-rt-content div[data-type="example"] h4,
    #sbo-rt-content div[data-type="example"] h5,
    #sbo-rt-content div[data-type="example"] h6 {
      font-style: italic;
      font-weight: normal;
      text-align: left !important;
      text-transform: none !important;
      font-family: serif !important;
      margin: 10px 0 5px 0 !important;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content li pre.example {
      padding: 10px 0 !important
    }

    #sbo-rt-content div[data-type="example"] pre[data-type="programlisting"],
    #sbo-rt-content div[data-type="example"] pre[data-type="screen"] {
      margin: 0
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h1 {
      font-size: 2em;
      margin: 50px 0 10px 0 !important;
      line-height: 1;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] h2,
    #sbo-rt-content section[data-type="titlepage"] p.subtitle,
    #sbo-rt-content section[data-type="titlepage"] p[data-type="subtitle"] {
      font-size: 1.3em;
      font-weight: normal;
      text-align: center;
      margin-top: .5em;
      color: #555
    }

    #sbo-rt-content section[data-type="titlepage"]&gt;
    div&gt;

    h2[data-type="author"],
    #sbo-rt-content section[data-type="titlepage"] p.author {
      font-size: 1.3em;
      font-family: serif !important;
      font-weight: bold;
      margin: 50px 0 !important;
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"] p.edition {
      text-align: center;
      text-transform: uppercase;
      margin-top: 2em
    }

    #sbo-rt-content section[data-type="titlepage"] {
      text-align: center
    }

    #sbo-rt-content section[data-type="titlepage"]:after {
      content: url(css_assets/titlepage_footer_ebook.png);
      margin: 0 auto;
      max-width: 80%
    }

    #sbo-rt-content div.book div.titlepage div.publishername {
      margin-top: 60%;
      margin-bottom: 20px;
      text-align: center;
      font-size: 1.25em
    }

    #sbo-rt-content div.book div.titlepage div.locations p {
      margin: 0;
      text-align: center
    }

    #sbo-rt-content div.book div.titlepage div.locations p.cities {
      font-size: 80%;
      text-align: center;
      margin-top: 5px
    }

    #sbo-rt-content section.preface[title="Dedication"]&gt;

    div.titlepage h2.title {
      text-align: center;
      text-transform: uppercase;
      font-size: 1.5em;
      margin-top: 50px;
      margin-bottom: 50px
    }

    #sbo-rt-content ul.stafflist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.stafflist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content ul.printings li {
      list-style-type: none
    }

    #sbo-rt-content section.preface[title="Dedication"] p {
      font-style: italic;
      text-align: center
    }

    #sbo-rt-content div.colophon h1.title {
      font-size: 1.3em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon h2.subtitle {
      margin: 0 !important;
      color: #000;
      font-family: serif !important;
      font-size: 1em;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.author h3.author {
      font-size: 1.1em;
      font-family: serif !important;
      margin: 10px 0 0 !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h4,
    #sbo-rt-content div.colophon div.editor h3.editor {
      color: #000;
      font-size: .8em;
      margin: 15px 0 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.editor h3.editor {
      font-size: .8em;
      margin: 0 !important;
      font-family: serif !important;
      font-weight: normal
    }

    #sbo-rt-content div.colophon div.publisher {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon div.publisher p,
    #sbo-rt-content div.colophon div.publisher span.publishername {
      margin: 0;
      font-size: .8em
    }

    #sbo-rt-content div.legalnotice p,
    #sbo-rt-content div.timestamp p {
      font-size: .8em
    }

    #sbo-rt-content div.timestamp p {
      margin-top: 10px
    }

    #sbo-rt-content div.colophon[title="About the Author"] h1.title,
    #sbo-rt-content div.colophon[title="Colophon"] h1.title {
      font-size: 1.5em;
      margin: 0 !important;
      font-family: sans-serif !important
    }

    #sbo-rt-content section.chapter div.titlepage div.author {
      margin: 10px 0 10px 0
    }

    #sbo-rt-content section.chapter div.titlepage div.author div.affiliation {
      font-style: italic
    }

    #sbo-rt-content div.attribution {
      margin: 5px 0 0 50px !important
    }

    #sbo-rt-content h3.author span.orgname {
      display: none
    }

    #sbo-rt-content div.epigraph {
      margin: 10px 0 10px 20px !important;
      page-break-inside: avoid;
      font-size: 90%
    }

    #sbo-rt-content div.epigraph p {
      font-style: italic
    }

    #sbo-rt-content blockquote,
    #sbo-rt-content div.blockquote {
      margin: 10px !important;
      page-break-inside: avoid;
      font-size: 95%
    }

    #sbo-rt-content blockquote p,
    #sbo-rt-content div.blockquote p {
      font-style: italic;
      margin: .75em 0 0 !important
    }

    #sbo-rt-content blockquote div.attribution,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      margin: 5px 0 10px 30px !important;
      text-align: right;
      width: 80%
    }

    #sbo-rt-content blockquote div.attribution p,
    #sbo-rt-content blockquote p[data-type="attribution"] {
      font-style: normal;
      margin-top: 5px
    }

    #sbo-rt-content blockquote div.attribution p:before,
    #sbo-rt-content blockquote p[data-type="attribution"]:before {
      font-style: normal;
      content: "—";
      -webkit-hyphens: none;
      hyphens: none;
      adobe-hyphenate: none
    }

    #sbo-rt-content p.right {
      text-align: right;
      margin: 0
    }

    #sbo-rt-content div[data-type="footnotes"] {
      border-top: 1px solid black;
      margin-top: 2em
    }

    #sbo-rt-content sub,
    #sbo-rt-content sup {
      font-size: 75%;
      line-height: 0;
      position: relative
    }

    #sbo-rt-content sup {
      top: -.5em
    }

    #sbo-rt-content sub {
      bottom: -.25em
    }

    #sbo-rt-content p[data-type="footnote"] {
      font-size: 90% !important;
      line-height: 1.2em !important;
      margin-left: 2.5em !important;
      text-indent: -2.3em !important
    }

    #sbo-rt-content p[data-type="footnote"] sup {
      display: inline-block !important;
      position: static !important;
      width: 2em !important;
      text-align: right !important;
      font-size: 100% !important;
      padding-right: .5em !important
    }

    #sbo-rt-content p[data-type="footnote"] a[href$="-marker"] {
      font-family: sans-serif !important;
      font-size: 90% !important;
      color: #8e0012 !important
    }

    #sbo-rt-content a[data-type="noteref"] {
      font-family: sans-serif !important;
      color: #8e0012;
      margin-left: 0;
      padding-left: 0
    }

    #sbo-rt-content div.refentry p.refname {
      font-size: 1em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold;
      margin-bottom: 5px;
      overflow: auto;
      width: 100%
    }

    #sbo-rt-content div.refentry {
      width: 100%;
      display: block;
      margin-top: 2em
    }

    #sbo-rt-content div.refsynopsisdiv {
      display: block;
      clear: both
    }

    #sbo-rt-content div.refentry header {
      page-break-inside: avoid !important;
      display: block;
      break-inside: avoid !important;
      padding-top: 0;
      border-bottom: 1px solid #000
    }

    #sbo-rt-content div.refsect1 h6 {
      font-size: .9em;
      font-family: sans-serif, "DejaVuSans";
      font-weight: bold
    }

    #sbo-rt-content div.refsect1 {
      margin-top: 3em
    }

    #sbo-rt-content dt {
      padding-top: 10px !important;
      padding-bottom: 0 !important
    }

    #sbo-rt-content dd {
      margin-left: 1.5em !important;
      margin-bottom: .25em
    }

    #sbo-rt-content dd ol,
    #sbo-rt-content dd ul {
      padding-left: 1em
    }

    #sbo-rt-content dd li {
      margin-top: 0;
      margin-bottom: 0
    }

    #sbo-rt-content dd,
    #sbo-rt-content li {
      text-align: left
    }

    #sbo-rt-content ul,
    #sbo-rt-content ul&gt;
    li,
    #sbo-rt-content ol ul,
    #sbo-rt-content ol ul&gt;
    li,
    #sbo-rt-content ul ol ul,
    #sbo-rt-content ul ol ul&gt;

    li {
      list-style-type: disc
    }

    #sbo-rt-content ul ul,
    #sbo-rt-content ul ul&gt;

    li {
      list-style-type: square
    }

    #sbo-rt-content ul ul ul,
    #sbo-rt-content ul ul ul&gt;

    li {
      list-style-type: circle
    }

    #sbo-rt-content ol,
    #sbo-rt-content ol&gt;
    li,
    #sbo-rt-content ol ul ol,
    #sbo-rt-content ol ul ol&gt;
    li,
    #sbo-rt-content ul ol,
    #sbo-rt-content ul ol&gt;

    li {
      list-style-type: decimal
    }

    #sbo-rt-content ol ol,
    #sbo-rt-content ol ol&gt;

    li {
      list-style-type: lower-alpha
    }

    #sbo-rt-content ol ol ol,
    #sbo-rt-content ol ol ol&gt;

    li {
      list-style-type: lower-roman
    }

    #sbo-rt-content ol,
    #sbo-rt-content ul {
      list-style-position: outside;
      margin: 15px 0 15px 1.25em;
      padding-left: 2.25em
    }

    #sbo-rt-content ol li,
    #sbo-rt-content ul li {
      margin: .5em 0 .65em;
      line-height: 125%
    }

    #sbo-rt-content div.orderedlistalpha {
      list-style-type: upper-alpha
    }

    #sbo-rt-content table.simplelist,
    #sbo-rt-content ul.simplelist {
      margin: 15px 0 15px 20px !important
    }

    #sbo-rt-content ul.simplelist li {
      list-style-type: none;
      padding: 5px 0
    }

    #sbo-rt-content table.simplelist td {
      border: none
    }

    #sbo-rt-content table.simplelist tr {
      border-bottom: none
    }

    #sbo-rt-content table.simplelist tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content dl.calloutlist p:first-child {
      margin-top: -25px !important
    }

    #sbo-rt-content dl.calloutlist dd {
      padding-left: 0;
      margin-top: -25px
    }

    #sbo-rt-content dl.calloutlist img,
    #sbo-rt-content a.co img {
      padding: 0
    }

    #sbo-rt-content div.toc ol {
      margin-top: 8px !important;
      margin-bottom: 8px !important;
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol ol {
      margin-left: 30px !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.toc ol li {
      list-style-type: none
    }

    #sbo-rt-content div.toc a {
      color: #8e0012
    }

    #sbo-rt-content div.toc ol a {
      font-size: 1em;
      font-weight: bold
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;

    ol a {
      font-weight: bold;
      font-size: 1em
    }

    #sbo-rt-content div.toc ol&gt;
    li&gt;
    ol&gt;
    li&gt;

    ol a {
      text-decoration: none;
      font-weight: normal;
      font-size: 1em
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"],
    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      margin: 30px !important;
      font-size: 90%;
      padding: 10px 8px 20px 8px !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.tip ol,
    #sbo-rt-content div.tip ul,
    #sbo-rt-content div[data-type="tip"] ol,
    #sbo-rt-content div[data-type="tip"] ul,
    #sbo-rt-content div.note ol,
    #sbo-rt-content div.note ul,
    #sbo-rt-content div[data-type="note"] ol,
    #sbo-rt-content div[data-type="note"] ul,
    #sbo-rt-content div.warning ol,
    #sbo-rt-content div.warning ul,
    #sbo-rt-content div[data-type="warning"] ol,
    #sbo-rt-content div[data-type="warning"] ul,
    #sbo-rt-content div[data-type="caution"] ol,
    #sbo-rt-content div[data-type="caution"] ul,
    #sbo-rt-content div[data-type="important"] ol,
    #sbo-rt-content div[data-type="important"] ul {
      margin-left: 1.5em !important
    }

    #sbo-rt-content div.tip,
    #sbo-rt-content div[data-type="tip"],
    #sbo-rt-content div.note,
    #sbo-rt-content div[data-type="note"] {
      border: 1px solid #BEBEBE;
      background-color: transparent
    }

    #sbo-rt-content div.warning,
    #sbo-rt-content div[data-type="warning"],
    #sbo-rt-content div[data-type="caution"],
    #sbo-rt-content div[data-type="important"] {
      border: 1px solid #BC8F8F
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="note"] h1,
    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1,
    #sbo-rt-content div[data-type="important"] h6 {
      font-weight: bold;
      font-size: 110%;
      font-family: sans-serif !important;
      text-transform: uppercase;
      letter-spacing: 1px;
      text-align: center;
      margin: 4px 0 6px !important
    }

    #sbo-rt-content div[data-type="tip"] figure h6,
    #sbo-rt-content div[data-type="note"] figure h6,
    #sbo-rt-content div[data-type="warning"] figure h6,
    #sbo-rt-content div[data-type="caution"] figure h6,
    #sbo-rt-content div[data-type="important"] figure h6 {
      font-family: serif !important
    }

    #sbo-rt-content div.tip h3,
    #sbo-rt-content div[data-type="tip"] h6,
    #sbo-rt-content div.note h3,
    #sbo-rt-content div[data-type="note"] h6,
    #sbo-rt-content div[data-type="tip"] h1,
    #sbo-rt-content div[data-type="note"] h1 {
      color: #737373
    }

    #sbo-rt-content div.warning h3,
    #sbo-rt-content div[data-type="warning"] h6,
    #sbo-rt-content div[data-type="caution"] h6,
    #sbo-rt-content div[data-type="important"] h6,
    #sbo-rt-content div[data-type="warning"] h1,
    #sbo-rt-content div[data-type="caution"] h1,
    #sbo-rt-content div[data-type="important"] h1 {
      color: #C67171
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note,
    #sbo-rt-content div.safarienabled {
      background-color: transparent;
      margin: 8px 0 0 !important;
      border: 0 solid #BEBEBE;
      font-size: 100%;
      padding: 0 !important;
      page-break-inside: avoid
    }

    #sbo-rt-content div.sect1[title="Safari® Books Online"] div.note h3,
    #sbo-rt-content div.safarienabled h6 {
      display: none
    }

    #sbo-rt-content div.table,
    #sbo-rt-content table {
      margin: 15px 0 30px 0 !important;
      max-width: 95%;
      border: none !important;
      background: none;
      display: table !important
    }

    #sbo-rt-content div.table,
    #sbo-rt-content div.informaltable,
    #sbo-rt-content table {
      page-break-inside: avoid
    }

    #sbo-rt-content tr,
    #sbo-rt-content tr td {
      border-bottom: 1px solid #c3c3c3
    }

    #sbo-rt-content thead td,
    #sbo-rt-content thead th {
      border-bottom: #9d9d9d 1px solid !important;
      border-top: #9d9d9d 1px solid !important
    }

    #sbo-rt-content tr:nth-of-type(even) {
      background-color: #f1f6fc
    }

    #sbo-rt-content thead {
      font-family: sans-serif;
      font-weight: bold
    }

    #sbo-rt-content td,
    #sbo-rt-content th {
      display: table-cell;
      padding: .3em;
      text-align: left;
      vertical-align: middle;
      font-size: 80%
    }

    #sbo-rt-content div.informaltable table {
      margin: 10px auto !important
    }

    #sbo-rt-content div.informaltable table tr {
      border-bottom: none
    }

    #sbo-rt-content div.informaltable table tr:nth-of-type(even) {
      background-color: transparent
    }

    #sbo-rt-content div.informaltable td,
    #sbo-rt-content div.informaltable th {
      border: #9d9d9d 1px solid
    }

    #sbo-rt-content div.table-title,
    #sbo-rt-content table caption {
      font-weight: normal;
      font-style: italic;
      font-family: serif;
      font-size: 1em;
      margin: 10px 0 10px 0 !important;
      padding: 0;
      page-break-after: avoid;
      text-align: left !important
    }

    #sbo-rt-content table code {
      font-size: smaller
    }

    #sbo-rt-content table.border tbody&gt;
    tr:last-child&gt;

    td {
      border-bottom: transparent
    }

    #sbo-rt-content div.equation,
    #sbo-rt-content div[data-type="equation"] {
      margin: 10px 0 15px 0 !important
    }

    #sbo-rt-content div.equation-title,
    #sbo-rt-content div[data-type="equation"] h5 {
      font-style: italic;
      font-weight: normal;
      font-family: serif !important;
      font-size: 90%;
      margin: 20px 0 10px 0 !important;
      page-break-after: avoid
    }

    #sbo-rt-content div.equation-contents {
      margin-left: 20px
    }

    #sbo-rt-content div[data-type="equation"] math {
      font-size: calc(.35em + 1vw)
    }

    #sbo-rt-content span.inlinemediaobject {
      height: .85em;
      display: inline-block;
      margin-bottom: .2em
    }

    #sbo-rt-content span.inlinemediaobject img {
      margin: 0;
      height: .85em
    }

    #sbo-rt-content div.informalequation {
      margin: 20px 0 20px 20px;
      width: 75%
    }

    #sbo-rt-content div.informalequation img {
      width: 75%
    }

    #sbo-rt-content div.index {
      text-indent: 0
    }

    #sbo-rt-content div.index h3 {
      padding: .25em;
      margin-top: 1em !important;
      background-color: #F0F0F0
    }

    #sbo-rt-content div.index li {
      line-height: 130%;
      list-style-type: none
    }

    #sbo-rt-content div.index a.indexterm {
      color: #8e0012 !important
    }

    #sbo-rt-content div.index ul {
      margin-left: 0 !important;
      padding-left: 0 !important
    }

    #sbo-rt-content div.index ul ul {
      margin-left: 1em !important;
      margin-top: 0 !important
    }

    #sbo-rt-content code.boolean,
    #sbo-rt-content .navy {
      color: rgb(0, 0, 128);
    }

    #sbo-rt-content code.character,
    #sbo-rt-content .olive {
      color: rgb(128, 128, 0);
    }

    #sbo-rt-content code.comment,
    #sbo-rt-content .blue {
      color: rgb(0, 0, 255);
    }

    #sbo-rt-content code.conditional,
    #sbo-rt-content .limegreen {
      color: rgb(50, 205, 50);
    }

    #sbo-rt-content code.constant,
    #sbo-rt-content .darkorange {
      color: rgb(255, 140, 0);
    }

    #sbo-rt-content code.debug,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.define,
    #sbo-rt-content .darkgoldenrod,
    #sbo-rt-content .gold {
      color: rgb(184, 134, 11);
    }

    #sbo-rt-content code.delimiter,
    #sbo-rt-content .dimgray {
      color: rgb(105, 105, 105);
    }

    #sbo-rt-content code.error,
    #sbo-rt-content .red {
      color: rgb(255, 0, 0);
    }

    #sbo-rt-content code.exception,
    #sbo-rt-content .salmon {
      color: rgb(250, 128, 11);
    }

    #sbo-rt-content code.float,
    #sbo-rt-content .steelblue {
      color: rgb(70, 130, 180);
    }

    #sbo-rt-content pre code.function,
    #sbo-rt-content .green {
      color: rgb(0, 128, 0);
    }

    #sbo-rt-content code.identifier,
    #sbo-rt-content .royalblue {
      color: rgb(65, 105, 225);
    }

    #sbo-rt-content code.ignore,
    #sbo-rt-content .gray {
      color: rgb(128, 128, 128);
    }

    #sbo-rt-content code.include,
    #sbo-rt-content .purple {
      color: rgb(128, 0, 128);
    }

    #sbo-rt-content code.keyword,
    #sbo-rt-content .sienna {
      color: rgb(160, 82, 45);
    }

    #sbo-rt-content code.label,
    #sbo-rt-content .deeppink {
      color: rgb(255, 20, 147);
    }

    #sbo-rt-content code.macro,
    #sbo-rt-content .orangered {
      color: rgb(255, 69, 0);
    }

    #sbo-rt-content code.number,
    #sbo-rt-content .brown {
      color: rgb(165, 42, 42);
    }

    #sbo-rt-content code.operator,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.preCondit,
    #sbo-rt-content .teal {
      color: rgb(0, 128, 128);
    }

    #sbo-rt-content code.preProc,
    #sbo-rt-content .fuschia {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.repeat,
    #sbo-rt-content .indigo {
      color: rgb(75, 0, 130);
    }

    #sbo-rt-content code.special,
    #sbo-rt-content .saddlebrown {
      color: rgb(139, 69, 19);
    }

    #sbo-rt-content code.specialchar,
    #sbo-rt-content .magenta {
      color: rgb(255, 0, 255);
    }

    #sbo-rt-content code.specialcomment,
    #sbo-rt-content .seagreen {
      color: rgb(46, 139, 87);
    }

    #sbo-rt-content code.statement,
    #sbo-rt-content .forestgreen {
      color: rgb(34, 139, 34);
    }

    #sbo-rt-content code.storageclass,
    #sbo-rt-content .plum {
      color: rgb(221, 160, 221);
    }

    #sbo-rt-content code.string,
    #sbo-rt-content .darkred {
      color: rgb(139, 0, 0);
    }

    #sbo-rt-content code.structure,
    #sbo-rt-content .chocolate {
      color: rgb(210, 106, 30);
    }

    #sbo-rt-content code.tag,
    #sbo-rt-content .darkcyan {
      color: rgb(0, 139, 139);
    }

    #sbo-rt-content code.todo,
    #sbo-rt-content .black {
      color: #000;
    }

    #sbo-rt-content code.type,
    #sbo-rt-content .mediumslateblue {
      color: rgb(123, 104, 238);
    }

    #sbo-rt-content code.typedef,
    #sbo-rt-content .darkgreen {
      color: rgb(0, 100, 0);
    }

    #sbo-rt-content code.underlined {
      text-decoration: underline;
    }

    #sbo-rt-content pre code.hll {
      background-color: #ffc
    }

    #sbo-rt-content pre code.c {
      color: #09F;
      font-style: italic
    }

    #sbo-rt-content pre code.err {
      color: #A00
    }

    #sbo-rt-content pre code.k {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.o {
      color: #555
    }

    #sbo-rt-content pre code.cm {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cp {
      color: #099
    }

    #sbo-rt-content pre code.c1 {
      color: #35586C;
      font-style: italic
    }

    #sbo-rt-content pre code.cs {
      color: #35586C;
      font-weight: bold;
      font-style: italic
    }

    #sbo-rt-content pre code.gd {
      background-color: #FCC
    }

    #sbo-rt-content pre code.ge {
      font-style: italic
    }

    #sbo-rt-content pre code.gr {
      color: #F00
    }

    #sbo-rt-content pre code.gh {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gi {
      background-color: #CFC
    }

    #sbo-rt-content pre code.go {
      color: #000
    }

    #sbo-rt-content pre code.gp {
      color: #009;
      font-weight: bold
    }

    #sbo-rt-content pre code.gs {
      font-weight: bold
    }

    #sbo-rt-content pre code.gu {
      color: #030;
      font-weight: bold
    }

    #sbo-rt-content pre code.gt {
      color: #9C6
    }

    #sbo-rt-content pre code.kc {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kd {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kn {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kp {
      color: #069
    }

    #sbo-rt-content pre code.kr {
      color: #069;
      font-weight: bold
    }

    #sbo-rt-content pre code.kt {
      color: #078;
      font-weight: bold
    }

    #sbo-rt-content pre code.m {
      color: #F60
    }

    #sbo-rt-content pre code.s {
      color: #C30
    }

    #sbo-rt-content pre code.na {
      color: #309
    }

    #sbo-rt-content pre code.nb {
      color: #366
    }

    #sbo-rt-content pre code.nc {
      color: #0A8;
      font-weight: bold
    }

    #sbo-rt-content pre code.no {
      color: #360
    }

    #sbo-rt-content pre code.nd {
      color: #99F
    }

    #sbo-rt-content pre code.ni {
      color: #999;
      font-weight: bold
    }

    #sbo-rt-content pre code.ne {
      color: #C00;
      font-weight: bold
    }

    #sbo-rt-content pre code.nf {
      color: #C0F
    }

    #sbo-rt-content pre code.nl {
      color: #99F
    }

    #sbo-rt-content pre code.nn {
      color: #0CF;
      font-weight: bold
    }

    #sbo-rt-content pre code.nt {
      color: #309;
      font-weight: bold
    }

    #sbo-rt-content pre code.nv {
      color: #033
    }

    #sbo-rt-content pre code.ow {
      color: #000;
      font-weight: bold
    }

    #sbo-rt-content pre code.w {
      color: #bbb
    }

    #sbo-rt-content pre code.mf {
      color: #F60
    }

    #sbo-rt-content pre code.mh {
      color: #F60
    }

    #sbo-rt-content pre code.mi {
      color: #F60
    }

    #sbo-rt-content pre code.mo {
      color: #F60
    }

    #sbo-rt-content pre code.sb {
      color: #C30
    }

    #sbo-rt-content pre code.sc {
      color: #C30
    }

    #sbo-rt-content pre code.sd {
      color: #C30;
      font-style: italic
    }

    #sbo-rt-content pre code.s2 {
      color: #C30
    }

    #sbo-rt-content pre code.se {
      color: #C30;
      font-weight: bold
    }

    #sbo-rt-content pre code.sh {
      color: #C30
    }

    #sbo-rt-content pre code.si {
      color: #A00
    }

    #sbo-rt-content pre code.sx {
      color: #C30
    }

    #sbo-rt-content pre code.sr {
      color: #3AA
    }

    #sbo-rt-content pre code.s1 {
      color: #C30
    }

    #sbo-rt-content pre code.ss {
      color: #A60
    }

    #sbo-rt-content pre code.bp {
      color: #366
    }

    #sbo-rt-content pre code.vc {
      color: #033
    }

    #sbo-rt-content pre code.vg {
      color: #033
    }

    #sbo-rt-content pre code.vi {
      color: #033
    }

    #sbo-rt-content pre code.il {
      color: #F60
    }

    #sbo-rt-content pre code.g {
      color: #050
    }

    #sbo-rt-content pre code.l {
      color: #C60
    }

    #sbo-rt-content pre code.l {
      color: #F90
    }

    #sbo-rt-content pre code.n {
      color: #008
    }

    #sbo-rt-content pre code.nx {
      color: #008
    }

    #sbo-rt-content pre code.py {
      color: #96F
    }

    #sbo-rt-content pre code.p {
      color: #000
    }

    #sbo-rt-content pre code.x {
      color: #F06
    }

    #sbo-rt-content div.blockquote_sampler_toc {
      width: 95%;
      margin: 5px 5px 5px 10px !important
    }

    #sbo-rt-content div {
      font-family: serif;
      text-align: left
    }

    #sbo-rt-content .gray-background,
    #sbo-rt-content .reverse-video {
      background: #2E2E2E;
      color: #FFF
    }

    #sbo-rt-content .light-gray-background {
      background: #A0A0A0
    }

    #sbo-rt-content .preserve-whitespace {
      white-space: pre-wrap
    }

    #sbo-rt-content span.gray {
      color: #4C4C4C
    }

    #sbo-rt-content .width-10 {
      width: 10vw !important
    }

    #sbo-rt-content .width-20 {
      width: 20vw !important
    }

    #sbo-rt-content .width-30 {
      width: 30vw !important
    }

    #sbo-rt-content .width-40 {
      width: 40vw !important
    }

    #sbo-rt-content .width-50 {
      width: 50vw !important
    }

    #sbo-rt-content .width-60 {
      width: 60vw !important
    }

    #sbo-rt-content .width-70 {
      width: 70vw !important
    }

    #sbo-rt-content .width-80 {
      width: 80vw !important
    }

    #sbo-rt-content .width-90 {
      width: 90vw !important
    }

    #sbo-rt-content .width-full,
    #sbo-rt-content .width-100 {
      width: 100vw !important
    }

    #sbo-rt-content div[data-type="equation"].fifty-percent img {
      width: 50%
    }
  </style>
  <style type="text/css" id="font-styles">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-size: &lt;
      %=font_size %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="font-family">
    #sbo-rt-content,
    #sbo-rt-content p,
    #sbo-rt-content div {
      font-family: &lt;
      %=font_family %&gt;
      !important;
    }
  </style>
  <style type="text/css" id="column-width">
    #sbo-rt-content {
      max-width: &lt;
      %=column_width %&gt;
      % !important;
      margin: 0 auto !important;
    }
  </style>

  <style type="text/css">
    body {
      background-color: #fbfbfb !important;
      margin: 1em;
    }

    #sbo-rt-content * {
      text-indent: 0pt !important;
    }

    #sbo-rt-content .bq {
      margin-right: 1em !important;
    }

    #sbo-rt-content * {
      word-wrap: break-word !important;
      word-break: break-word !important;
    }

    #sbo-rt-content table,
    #sbo-rt-content pre {
      overflow-x: unset !important;
      overflow: unset !important;
      overflow-y: unset !important;
      white-space: pre-wrap !important;
    }
  </style>
</head>

<body>
  <div id="sbo-rt-content">
    <section data-type="chapter" epub:type="chapter" data-pdf-bookmark="Chapter 8. Dimensionality Reduction">
      <div class="chapter" id="dimensionality_chapter">
        <h1><span class="label">Chapter 8. </span>Dimensionality Reduction</h1>


        <p>Many <a data-type="indexterm" data-primary="dimensionality reduction" id="dr8" />Machine Learning problems
          involve thousands or even millions of features for each training instance. Not only does this make training
          extremely slow, it can also make it much harder to find a good solution, as we will see. This problem is often
          referred to as <a data-type="indexterm" data-primary="curse of dimensionality"
            data-seealso="dimensionality reduction" id="cod8" /><a data-type="indexterm"
            data-primary="dimensionality reduction" data-secondary="curse of dimensionality" id="dr8cod" />the <em>curse
            of dimensionality</em>.</p>

        <p>Fortunately, in real-world problems, it is often possible to reduce the number of features considerably,
          turning an intractable problem into a tractable one. For example, consider the MNIST images (introduced in <a
            data-type="xref" href="ch03.xhtml#classification_chapter">Chapter 3</a>): the pixels on the image borders
          are almost always white, so you could completely drop these pixels from the training set without losing much
          information. <a data-type="xref" href="ch07.xhtml#mnist_feature_importance_plot">Figure 7-6</a> confirms that
          these pixels are utterly unimportant for the classification task. Moreover, two neighboring pixels are often
          highly correlated: if you merge them into a single pixel (e.g., by taking the mean of the two pixel
          intensities), you will not lose much information.</p>
        <div data-type="warning" epub:type="warning">
          <h6>Warning</h6>
          <p>Reducing dimensionality does lose some information (just like compressing an image to JPEG can degrade its
            quality), so even though it will speed up training, it may also make your system perform slightly worse. It
            also makes your pipelines a bit more complex and thus harder to maintain. So you should first try to train
            your system with the original data before considering using dimensionality reduction if training is too
            slow. In some cases, however, reducing the dimensionality of the training data may filter out some noise and
            unnecessary details and thus result in higher performance (but in general it won’t; it will just speed up
            training).</p>
        </div>

        <p>Apart from speeding up training, <a data-type="indexterm" data-primary="dimensionality reduction"
            data-secondary="and data visualization" data-secondary-sortas="data"
            id="idm139656373319728" />dimensionality reduction is also extremely useful for data visualization (or
          <em>DataViz</em>). Reducing the number of dimensions down to two (or three) makes it possible to plot a
          condensed view of a high-dimensional training set on a graph and often gain some important insights by
          visually detecting patterns, such as clusters. Moreover, DataViz is essential to communicate your conclusions
          to people who are not data scientists, in particular decision makers who will use your results.</p>

        <p>In this chapter we will discuss the curse of dimensionality and get a sense of what goes on in
          high-dimensional space. Then, we will present the two main approaches to dimensionality reduction (projection
          and Manifold Learning), and we will go through three of the most popular dimensionality reduction techniques:
          PCA, Kernel PCA, and LLE.</p>






        <section data-type="sect1" data-pdf-bookmark="The Curse of Dimensionality">
          <div class="sect1" id="idm139656373316592">
            <h1>The Curse of Dimensionality</h1>

            <p>We are so used to living in three dimensions<sup><a data-type="noteref" id="idm139656373315056-marker"
                  href="ch08.xhtml#idm139656373315056">1</a></sup> that our intuition fails us when we try to imagine a
              high-dimensional space. Even a basic 4D hypercube is incredibly hard to picture in our mind (see <a
                data-type="xref" href="#hypercube_wikipedia">Figure 8-1</a>), let alone a 200-dimensional ellipsoid bent
              in a 1,000-dimensional space.</p>

            <figure>
              <div id="hypercube_wikipedia" class="figure">
                <img src="mlst_0801.png" alt="mlst 0801" width="1153" height="381" />
                <h6><span class="label">Figure 8-1. </span>Point, segment, square, cube, and tesseract (0D to 4D
                  hypercubes)<sup><a data-type="noteref" id="idm139656373311600-marker"
                      href="ch08.xhtml#idm139656373311600">2</a></sup></h6>
              </div>
            </figure>

            <p>It turns out that many things behave very differently in high-dimensional space. For example, if you pick
              a random point in a unit square (a 1 × 1 square), it will have only about a 0.4% chance of being located
              less than 0.001 from a border (in other words, it is very unlikely that a random point will be “extreme”
              along any dimension). But in a 10,000-dimensional unit hypercube (a 1 × 1 × ⋯ × 1 cube, with ten thousand
              1s), this probability is greater than 99.999999%. Most points in a high-dimensional hypercube are very
              close to the border.<sup><a data-type="noteref" id="idm139656373306608-marker"
                  href="ch08.xhtml#idm139656373306608">3</a></sup></p>

            <p>Here is a more troublesome difference: if you pick two points randomly in a unit square, the distance
              between these two points will be, on average, roughly 0.52. If you pick two random points in a unit 3D
              cube, the average distance will be roughly 0.66. But what about two points picked randomly in a
              1,000,000-dimensional hypercube? Well, the average distance, believe it or not, will be about 408.25
              (roughly <math xmlns="http://www.w3.org/1998/Math/MathML"
                alttext="StartRoot 1 comma 000 comma 000 slash 6 EndRoot">
                <msqrt>
                  <mrow>
                    <mn>1</mn>
                    <mo>,</mo>
                    <mn>000</mn>
                    <mo>,</mo>
                    <mn>000</mn>
                    <mo>/</mo>
                    <mn>6</mn>
                  </mrow>
                </msqrt>
              </math>)! This is quite counterintuitive: how can two points be so far apart when they both lie within the
              same unit hypercube? This fact implies that high-dimensional datasets are at risk of being very sparse:
              most training instances are likely to be far away from each other. Of course, this also means that a new
              instance will likely be far away from any training instance, making predictions much less reliable than in
              lower dimensions, since they will be based on much larger extrapolations. In short, the more dimensions
              the training set has, the greater the risk of overfitting it.</p>

            <p>In theory, one solution to the curse of dimensionality could be to increase the size of the training set
              to reach a sufficient density of training instances. Unfortunately, in practice, the number of training
              instances required to reach a given density grows exponentially with the number of dimensions. With just
              100 features (much less than in the MNIST problem), you would need more training instances than atoms in
              the observable universe in order for training instances to be within 0.1 of each other on average,
              assuming they were spread out uniformly <a data-type="indexterm" data-primary="curse of dimensionality"
                data-startref="cod8" id="idm139656373298544" /><a data-type="indexterm"
                data-primary="dimensionality reduction" data-secondary="curse of dimensionality" data-startref="dr8cod"
                id="idm139656373297568" />across all dimensions.</p>
          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Main Approaches for Dimensionality Reduction">
          <div class="sect1" id="idm139656373296032">
            <h1>Main Approaches for Dimensionality Reduction</h1>

            <p>Before we dive into specific dimensionality reduction algorithms, let’s take a look at the two main
              approaches to reducing dimensionality: projection and Manifold Learning.</p>








            <section data-type="sect2" data-pdf-bookmark="Projection">
              <div class="sect2" id="idm139656373294416">
                <h2>Projection</h2>

                <p>In most <a data-type="indexterm" data-primary="dimensionality reduction"
                    data-secondary="approaches to" data-tertiary="projection" id="dr8atp" /><a data-type="indexterm"
                    data-primary="projection" id="p8" />real-world problems, training instances are <em>not</em> spread
                  out uniformly across all dimensions. Many features are almost constant, while others are highly
                  correlated (as discussed earlier for MNIST). As a result, all training instances actually lie within
                  (or close to) a much lower-dimensional <em>subspace</em> of the high-dimensional space. This sounds
                  very abstract, so let’s look at an example. In <a data-type="xref"
                    href="#dataset_3d_plot">Figure 8-2</a> you can see a 3D dataset represented by the circles.</p>

                <figure class="smallerseventyfive">
                  <div id="dataset_3d_plot" class="figure">
                    <img src="mlst_0802.png" alt="mlst 0802" width="1565" height="1018" />
                    <h6><span class="label">Figure 8-2. </span>A 3D dataset lying close to a 2D subspace</h6>
                  </div>
                </figure>

                <p>Notice that all training instances lie close to a plane: this is a lower-dimensional (2D) subspace of
                  the high-dimensional (3D) space. Now if we project every training instance perpendicularly onto this
                  subspace (as represented by the short lines connecting the instances to the plane), we get the new 2D
                  dataset shown in <a data-type="xref" href="#dataset_2d_plot">Figure 8-3</a>. Ta-da! We have just
                  reduced the dataset’s dimensionality from 3D to 2D. Note that the axes correspond to new features
                  <em>z</em><sub>1</sub> and <em>z</em><sub>2</sub> (the coordinates of the projections on the plane).
                </p>

                <figure class="smallersixty">
                  <div id="dataset_2d_plot" class="figure">
                    <img src="mlst_0803.png" alt="mlst 0803" width="1278" height="1084" />
                    <h6><span class="label">Figure 8-3. </span>The new 2D dataset after projection</h6>
                  </div>
                </figure>

                <p>However, projection is not always the best approach to dimensionality reduction. In many cases the
                  subspace may twist and turn, such as in the famous <em>Swiss roll</em> toy dataset represented in <a
                    data-type="xref" href="#swiss_roll_plot">Figure 8-4</a>.</p>

                <figure class="smallerseventyfive">
                  <div id="swiss_roll_plot" class="figure">
                    <img src="mlst_0804.png" alt="mlst 0804" width="1423" height="1096" />
                    <h6><span class="label">Figure 8-4. </span>Swiss roll dataset</h6>
                  </div>
                </figure>

                <p>Simply projecting onto a plane (e.g., by dropping <em>x</em><sub>3</sub>) would squash different
                  layers of the Swiss roll together, as shown on the left of <a data-type="xref"
                    href="#squished_swiss_roll_plot">Figure 8-5</a>. However, what you really want is to unroll the
                  Swiss roll to obtain the 2D dataset on the <a data-type="indexterm"
                    data-primary="dimensionality reduction" data-secondary="approaches to" data-tertiary="projection"
                    data-startref="dr8atp" id="idm139656373165088" /><a data-type="indexterm" data-primary="projection"
                    data-startref="p8" id="idm139656373163584" />right of <a data-type="xref"
                    href="#squished_swiss_roll_plot">Figure 8-5</a>.</p>

                <figure>
                  <div id="squished_swiss_roll_plot" class="figure">
                    <img src="mlst_0805.png" alt="mlst 0805" width="3195" height="1084" />
                    <h6><span class="label">Figure 8-5. </span>Squashing by projecting onto a plane (left) versus
                      unrolling the Swiss roll (right)</h6>
                  </div>
                </figure>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Manifold Learning">
              <div class="sect2" id="idm139656373159728">
                <h2>Manifold Learning</h2>

                <p>The <a data-type="indexterm" data-primary="dimensionality reduction" data-secondary="approaches to"
                    data-tertiary="Manifold Learning" id="idm139656373158224" /><a data-type="indexterm"
                    data-primary="Manifold Learning" data-seealso="LLE (Locally Linear Embedding"
                    id="idm139656373156592" />Swiss roll is an example of a 2D <em>manifold</em>. Put simply, a 2D
                  manifold is a 2D shape that can be bent and twisted in a higher-dimensional space. More generally, a
                  <em>d</em>-dimensional manifold is a part of an <em>n</em>-dimensional space (where <em>d</em> &lt;
                  <em>n</em>) that locally resembles a <em>d</em>-dimensional <a data-type="indexterm"
                    data-primary="hyperplane" id="hyperplanech8" />hyperplane. In the case of the Swiss roll, <em>d</em>
                  = 2 and <em>n</em> = 3: it locally resembles a 2D plane, but it is rolled in the third dimension.</p>

                <p>Many dimensionality reduction algorithms work by modeling the <em>manifold</em> on which the training
                  instances lie; this is called <em>Manifold Learning</em>. It relies on the <em>manifold
                    assumption</em>, also called the <em>manifold hypothesis</em>, which <a data-type="indexterm"
                    data-primary="manifold assumption/hypothesis" id="idm139656373148624" /><a data-type="indexterm"
                    data-primary="hypothesis" data-secondary="manifold" id="idm139656373147872" />holds that most
                  real-world high-dimensional datasets lie close to a much lower-dimensional manifold. This assumption
                  is very often empirically observed.</p>

                <p>Once again, think about the MNIST dataset: all handwritten digit images have some similarities. They
                  are made of connected lines, the borders are white, they are more or less centered, and so on. If you
                  randomly generated images, only a ridiculously tiny fraction of them would look like handwritten
                  digits. In other words, the degrees of freedom available to you if you try to create a digit image are
                  dramatically lower than the degrees of freedom you would have if you were allowed to generate any
                  image you wanted. These constraints tend to squeeze the dataset into a lower-dimensional manifold.</p>

                <p>The manifold assumption is often accompanied by another implicit assumption: that the task at hand
                  (e.g., classification or regression) will be simpler if expressed in the lower-dimensional space of
                  the manifold. For example, in the top row of <a data-type="xref"
                    href="#manifold_decision_boundary_plot">Figure 8-6</a> the Swiss roll is split into two classes: in
                  the 3D space (on the left), the decision boundary would be fairly complex, but in the 2D unrolled
                  manifold space (on the right), the decision boundary is a simple straight line.</p>

                <p>However, this assumption does not always hold. For example, in the bottom row of <a data-type="xref"
                    href="#manifold_decision_boundary_plot">Figure 8-6</a>, the decision boundary is located at
                  <em>x</em><sub>1</sub> = 5. This decision boundary looks very simple in the original 3D space (a
                  vertical plane), but it looks more complex in the unrolled manifold (a collection of four independent
                  line segments).</p>

                <p>In short, if you reduce the dimensionality of your training set before training a model, it will
                  usually speed up training, but it may not always lead to a better or simpler solution; it all depends
                  on the dataset.</p>

                <p>Hopefully you now have a good sense of what the curse of dimensionality is and how dimensionality
                  reduction algorithms can fight it, especially when the manifold assumption holds. The rest of this
                  chapter will go through some of the most popular algorithms.</p>

                <figure>
                  <div id="manifold_decision_boundary_plot" class="figure">
                    <img src="mlst_0806.png" alt="mlst 0806" width="2128" height="1603" />
                    <h6><span class="label">Figure 8-6. </span>The decision boundary may not always be simpler with
                      lower dimensions</h6>
                  </div>
                </figure>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="PCA">
          <div class="sect1" id="idm139656373138128">
            <h1>PCA</h1>

            <p><em>Principal Component Analysis</em> (PCA) <a data-type="indexterm"
                data-primary="Principal Component Analysis (PCA)" id="pca8" /><a data-type="indexterm"
                data-primary="dimensionality reduction" data-secondary="PCA (Principal Component Analysis)"
                id="dr8pca" />is by far the most popular dimensionality reduction algorithm. First it identifies the <a
                data-type="indexterm" data-primary="hyperplane" data-startref="hyperplanech8"
                id="idm139656373133808" />hyperplane that lies closest to the data, and then it projects the data onto
              it, just like in <a data-type="xref" href="#dataset_3d_plot">Figure 8-2</a>.</p>








            <section data-type="sect2" data-pdf-bookmark="Preserving the Variance">
              <div class="sect2" id="idm139656373131696">
                <h2>Preserving the Variance</h2>

                <p>Before <a data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-secondary="variance, preserving" id="pca8vp" /><a data-type="indexterm"
                    data-primary="variance preservation" id="vp8" />you can project the training set onto a
                  lower-dimensional hyperplane, you first need to choose the right hyperplane. For example, a simple 2D
                  dataset is represented on the left of <a data-type="xref" href="#pca_best_projection">Figure 8-7</a>,
                  along with three different axes (i.e., one-dimensional hyperplanes). On the right is the result of the
                  projection of the dataset onto each of these axes. As you can see, the projection onto the solid line
                  preserves the maximum variance, while the projection onto the dotted line preserves very little
                  variance, and the projection onto the dashed line preserves an intermediate amount of variance.</p>

                <figure>
                  <div id="pca_best_projection" class="figure">
                    <img src="mlst_0807.png" alt="mlst 0807" width="2304" height="1084" />
                    <h6><span class="label">Figure 8-7. </span>Selecting the subspace onto which to project</h6>
                  </div>
                </figure>

                <p>It seems reasonable to select the axis that preserves the maximum amount of variance, as it will most
                  likely lose less information than the other projections. Another way to justify this choice is that it
                  is the axis that minimizes the mean squared distance between the original dataset and its projection
                  onto that axis. This is the rather simple idea <a data-type="indexterm"
                    data-primary="Principal Component Analysis (PCA)" data-secondary="variance, preserving"
                    data-startref="pca8vp" id="idm139656373123712" /><a data-type="indexterm"
                    data-primary="variance preservation" data-startref="vp8" id="idm139656373122448" />behind <a
                    href="https://homl.info/31">PCA</a>.<sup><a data-type="noteref" id="idm139656373120688-marker"
                      href="ch08.xhtml#idm139656373120688">4</a></sup></p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Principal Components">
              <div class="sect2" id="idm139656373119728">
                <h2>Principal Components</h2>

                <p>PCA identifies <a data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-secondary="finding principal components" id="pca8fpc" />the axis that accounts for the largest
                  amount of variance in the training set. In <a data-type="xref"
                    href="#pca_best_projection">Figure 8-7</a>, it is the solid line. It also finds a second axis,
                  orthogonal to the first one, that accounts for the largest amount of remaining variance. In this 2D
                  example there is no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA would
                  also find a third axis, orthogonal to both previous axes, and a fourth, a fifth, and so on—as many
                  axes as the number of dimensions in the dataset.</p>

                <p>The unit vector that defines the i<sup>th</sup> axis is called <a data-type="indexterm"
                    data-primary="principal component" id="idm139656373114416" />the i<sup>th</sup> <em>principal
                    component</em> (PC). In <a data-type="xref" href="#pca_best_projection">Figure 8-7</a>, the
                  1<sup>st</sup> PC is <strong>c</strong><sub><strong>1</strong></sub> and the 2<sup>nd</sup> PC is
                  <strong>c</strong><sub><strong>2</strong></sub>. In <a data-type="xref"
                    href="#dataset_3d_plot">Figure 8-2</a> the first two PCs are represented by the orthogonal arrows in
                  the plane, and the third PC would be orthogonal to the plane (pointing up or down).</p>
                <div data-type="note" epub:type="note">
                  <h6>Note</h6>
                  <p>The direction of the principal components is not stable: if you perturb the training set slightly
                    and run PCA again, some of the new PCs may point in the opposite direction of the original PCs.
                    However, they will generally still lie on the same axes. In some cases, a pair of PCs may even
                    rotate or swap, but the plane they define will generally remain the same.</p>
                </div>

                <p>So how can you find the principal components of a training set? Luckily, there is a standard matrix
                  factorization technique <a data-type="indexterm" data-primary="Singular Value Decomposition (SVD)"
                    id="idm139656373106224" />called <em>Singular Value Decomposition</em> (SVD) that can decompose the
                  training set matrix <strong>X</strong> into the matrix multiplication of three matrices
                  <strong>U</strong> Σ <strong>V</strong><sup><em>T</em></sup>, where <strong>V</strong> contains all
                  the principal components that we are looking for, as shown in <a data-type="xref"
                    href="#principal_components_matrix">Equation 8-1</a>.</p>
                <div class="fifty-percent" id="principal_components_matrix" data-type="equation">
                  <h5><span class="label">Equation 8-1. </span>Principal components matrix</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <mi mathvariant="bold">V</mi>
                      <mo>=</mo>
                      <mfenced open="(" close=")">
                        <mtable>
                          <mtr>
                            <mtd>
                              <mo>∣</mo>
                            </mtd>
                            <mtd>
                              <mo>∣</mo>
                            </mtd>
                            <mtd />
                            <mtd>
                              <mo>∣</mo>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <msub>
                                <mi mathvariant="bold">c</mi>
                                <mn mathvariant="bold">1</mn>
                              </msub>
                            </mtd>
                            <mtd>
                              <msub>
                                <mi mathvariant="bold">c</mi>
                                <mn mathvariant="bold">2</mn>
                              </msub>
                            </mtd>
                            <mtd>
                              <mo>⋯</mo>
                            </mtd>
                            <mtd>
                              <msub>
                                <mi mathvariant="bold">c</mi>
                                <mi mathvariant="bold">n</mi>
                              </msub>
                            </mtd>
                          </mtr>
                          <mtr>
                            <mtd>
                              <mo>∣</mo>
                            </mtd>
                            <mtd>
                              <mo>∣</mo>
                            </mtd>
                            <mtd />
                            <mtd>
                              <mo>∣</mo>
                            </mtd>
                          </mtr>
                        </mtable>
                      </mfenced>
                    </mrow>
                  </math>
                </div>

                <p>The following Python code uses NumPy’s <code>svd()</code> function <a data-type="indexterm"
                    data-primary="svd()" id="idm139656373080288" />to obtain all the principal components of the
                  training set, then extracts the first two PCs:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">X_centered</code> <code class="o">=</code> <code class="n">X</code> <code class="o">-</code> <code class="n">X</code><code class="o">.</code><code class="n">mean</code><code class="p">(</code><code class="n">axis</code><code class="o">=</code><code class="mi">0</code><code class="p">)</code>
<code class="n">U</code><code class="p">,</code> <code class="n">s</code><code class="p">,</code> <code class="n">Vt</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">linalg</code><code class="o">.</code><code class="n">svd</code><code class="p">(</code><code class="n">X_centered</code><code class="p">)</code>
<code class="n">c1</code> <code class="o">=</code> <code class="n">Vt</code><code class="o">.</code><code class="n">T</code><code class="p">[:,</code> <code class="mi">0</code><code class="p">]</code>
<code class="n">c2</code> <code class="o">=</code> <code class="n">Vt</code><code class="o">.</code><code class="n">T</code><code class="p">[:,</code> <code class="mi">1</code><code class="p">]</code></pre>
                <div data-type="warning" epub:type="warning">
                  <h6>Warning</h6>
                  <p>PCA assumes that the dataset is centered around the origin. As we will see, Scikit-Learn’s PCA
                    classes take care of centering the data for you. However, if you implement PCA yourself (as in the
                    preceding example), or if you use other libraries, don’t forget to center the <a
                      data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                      data-secondary="finding principal components" data-startref="pca8fpc"
                      id="idm139656373035616" />data first.</p>
                </div>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Projecting Down to d Dimensions">
              <div class="sect2" id="idm139656373034112">
                <h2>Projecting Down to <em>d</em> Dimensions</h2>

                <p>Once <a data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-secondary="projecting down to d dimensions" id="pca8pdtdd" />you have identified all the
                  principal components, you can reduce the dimensionality of the dataset down to <em>d</em> dimensions
                  by projecting it onto the <a data-type="indexterm" data-primary="hyperplane"
                    id="idm139656373030016" />hyperplane defined by the first <em>d</em> principal components. Selecting
                  this hyperplane ensures that the projection will preserve as much variance as possible. For example,
                  in <a data-type="xref" href="#dataset_3d_plot">Figure 8-2</a> the 3D dataset is projected down to the
                  2D plane defined by the first two principal components, preserving a large part of the dataset’s
                  variance. As a result, the 2D projection looks very much like the original 3D dataset.</p>

                <p>To project the training set onto the hyperplane, you can simply compute the matrix multiplication of
                  the training set matrix <strong>X</strong> by the matrix <strong>W</strong><sub><em>d</em></sub>,
                  defined as the matrix containing the first <em>d</em> principal components (i.e., the matrix composed
                  of the first <em>d</em> columns of <strong>V</strong>), as shown in <a data-type="xref"
                    href="#pca_projection">Equation 8-2</a>.</p>
                <div class="fifty-percent" id="pca_projection" data-type="equation">
                  <h5><span class="label">Equation 8-2. </span>Projecting the training set down to <em>d</em> dimensions
                  </h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <msub>
                        <mi mathvariant="bold">X</mi>
                        <mrow>
                          <mi>d</mi>
                          <mtext>-proj</mtext>
                        </mrow>
                      </msub>
                      <mo>=</mo>
                      <mi mathvariant="bold">X</mi>
                      <msub>
                        <mi mathvariant="bold">W</mi>
                        <mi>d</mi>
                      </msub>
                    </mrow>
                  </math>
                </div>

                <p>The following Python code projects the training set onto the plane defined by the first two principal
                  components:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">W2</code> <code class="o">=</code> <code class="n">Vt</code><code class="o">.</code><code class="n">T</code><code class="p">[:,</code> <code class="p">:</code><code class="mi">2</code><code class="p">]</code>
<code class="n">X2D</code> <code class="o">=</code> <code class="n">X_centered</code><code class="o">.</code><code class="n">dot</code><code class="p">(</code><code class="n">W2</code><code class="p">)</code></pre>

                <p>There you have it! You now know how to reduce the dimensionality of any dataset down to any number of
                  dimensions, while preserving as much variance as possible.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Using Scikit-Learn">
              <div class="sect2" id="idm139656373003872">
                <h2>Using Scikit-Learn</h2>

                <p>Scikit-Learn’s <code>PCA</code> class <a data-type="indexterm"
                    data-primary="Principal Component Analysis (PCA)" data-secondary="Scikit Learn for"
                    id="idm139656372896592" /><a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="PCA implementation" id="idm139656372895616" /><a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="sklearn.decomposition.PCA"
                    id="idm139656372894672" />implements PCA using SVD decomposition just like we did before. The
                  following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note
                  that it automatically takes care of centering the data):</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">PCA</code>

<code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code> <code class="o">=</code> <code class="mi">2</code><code class="p">)</code>
<code class="n">X2D</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

                <p>After fitting the <code>PCA</code> transformer to the dataset, you can access the principal
                  components using the <code>components_</code> <a data-type="indexterm" data-primary="components_"
                    id="idm139656372883808" />variable (note that it contains the PCs as horizontal vectors, so, for
                  example, the first principal component is equal to <code>pca.components_.T[:, 0]</code>).</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Explained Variance Ratio">
              <div class="sect2" id="idm139656372882224">
                <h2>Explained Variance Ratio</h2>

                <p>Another <a data-type="indexterm" data-primary="explained variance ratio" id="idm139656372851856" /><a
                    data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-secondary="explained variance ratios" id="idm139656372851152" />very useful piece of
                  information is the <em>explained variance ratio</em> of each principal component, available via the
                  <code>explained_variance_ratio_</code> variable. It indicates the proportion of the dataset’s variance
                  that lies along the axis of each principal component. For example, let’s look at the explained
                  variance ratios of the first two components of the 3D dataset represented in <a data-type="xref"
                    href="#dataset_3d_plot">Figure 8-2</a>:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code>
<code class="go">array([0.84248607, 0.14631839])</code></pre>

                <p>This tells you that 84.2% of the dataset’s variance lies along the first axis, and 14.6% lies along
                  the second axis. This leaves less than 1.2% for the third axis, so it is reasonable to assume that it
                  probably carries little information.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Choosing the Right Number of Dimensions">
              <div class="sect2" id="idm139656372940992">
                <h2>Choosing the Right Number of Dimensions</h2>

                <p>Instead <a data-type="indexterm" data-primary="dimensionality reduction"
                    data-secondary="choosing the right number of dimensions" id="idm139656372942896" />of arbitrarily
                  choosing the number of dimensions to reduce down to, it is generally preferable to choose the number
                  of dimensions that add up to a sufficiently large portion of the variance (e.g., 95%). Unless, of
                  course, you are reducing dimensionality for data visualization—in that case you will generally want to
                  reduce the dimensionality down to 2 or 3.</p>

                <p>The following code computes PCA without reducing dimensionality, then computes the minimum number of
                  dimensions required to preserve 95% of the training set’s variance:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">()</code>
<code class="n">pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">cumsum</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">cumsum</code><code class="p">(</code><code class="n">pca</code><code class="o">.</code><code class="n">explained_variance_ratio_</code><code class="p">)</code>
<code class="n">d</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">argmax</code><code class="p">(</code><code class="n">cumsum</code> <code class="o">&gt;=</code> <code class="mf">0.95</code><code class="p">)</code> <code class="o">+</code> <code class="mi">1</code></pre>

                <p>You could then set <code>n_components=d</code> and run PCA again. However, there is a much better
                  option: instead of specifying the number of principal components you want to preserve, you <a
                    data-type="indexterm" data-primary="n_components" id="idm139656372975008" />can set
                  <code>n_components</code> to be a float between <code>0.0</code> and <code>1.0</code>, indicating the
                  ratio of variance you wish to preserve:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mf">0.95</code><code class="p">)</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code></pre>

                <p>Yet another option is to plot the <a data-type="indexterm" data-primary="explained variance"
                    id="idm139656372708464" />explained variance as a function of the number of dimensions (simply plot
                  <code>cumsum</code>; see <a data-type="xref" href="#explained_variance_plot">Figure 8-8</a>). There
                  will usually be an elbow in the curve, where the explained variance stops growing fast. You can think
                  of this as the intrinsic dimensionality of the dataset. In this case, you can see that reducing the
                  dimensionality down to about 100 dimensions wouldn’t lose too much explained variance.</p>

                <figure class="smallersixty">
                  <div id="explained_variance_plot" class="figure">
                    <img src="mlst_0808.png" alt="mlst 0808" width="1707" height="1089" />
                    <h6><span class="label">Figure 8-8. </span>Explained variance as a function of the number of
                      dimensions</h6>
                  </div>
                </figure>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="PCA for Compression">
              <div class="sect2" id="idm139656372800528">
                <h2>PCA for Compression</h2>

                <p>Obviously <a data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-secondary="for compression" id="pca8fc" />after dimensionality reduction, the training set
                  takes up much less space. For example, try applying PCA to the MNIST dataset while preserving 95% of
                  its variance. You should find that each instance will have just over 150 features, instead of the
                  original 784 features. So while most of the variance is preserved, the dataset is now less than 20% of
                  its original size! This is a reasonable compression ratio, and you can see how this can speed up a
                  classification algorithm (such as an SVM classifier) tremendously.</p>

                <p>It is also possible to decompress the reduced dataset back to 784 dimensions by applying the inverse
                  transformation of the PCA projection. Of course this won’t give you back the original data, since the
                  projection lost a bit of information (within the 5% variance that was dropped), but it will likely be
                  quite close to the original data. The mean squared distance between the original data and the
                  reconstructed data (compressed and then decompressed) is called the <em>reconstruction error</em>. <a
                    data-type="indexterm" data-primary="reconstruction error" id="idm139656372795776" />For example, the
                  following code compresses the MNIST dataset down to 154 dimensions, then uses the
                  <code>inverse_transform()</code> method to decompress it back to 784 dimensions. <a data-type="xref"
                    href="#mnist_compression_plot">Figure 8-9</a> shows a few digits from the original training set (on
                  the left), and the corresponding digits after compression and decompression. You can see that there is
                  a slight image quality loss, but the digits are still mostly intact.</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code> <code class="o">=</code> <code class="mi">154</code><code class="p">)</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code>
<code class="n">X_recovered</code> <code class="o">=</code> <code class="n">pca</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">X_reduced</code><code class="p">)</code></pre>

                <figure class="smallereighty">
                  <div id="mnist_compression_plot" class="figure">
                    <img src="mlst_0809.png" alt="mlst 0809" width="1861" height="935" />
                    <h6><span class="label">Figure 8-9. </span>MNIST compression preserving 95% of the variance</h6>
                  </div>
                </figure>

                <p>The equation of the inverse transformation is shown in <a data-type="xref"
                    href="#inverse_pca">Equation 8-3</a>.</p>
                <div class="fifty-percent" id="inverse_pca" data-type="equation">
                  <h5><span class="label">Equation 8-3. </span>PCA inverse transformation, back to the original number
                    of dimensions</h5>
                  <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                    <mrow>
                      <msub>
                        <mi mathvariant="bold">X</mi>
                        <mtext>recovered</mtext>
                      </msub>
                      <mo>=</mo>
                      <msub>
                        <mi mathvariant="bold">X</mi>
                        <mrow>
                          <mi>d</mi>
                          <mtext>-proj</mtext>
                        </mrow>
                      </msub>
                      <msup>
                        <mrow>
                          <msub>
                            <mi mathvariant="bold">W</mi>
                            <mi>d</mi>
                          </msub>
                        </mrow>
                        <mi>T</mi>
                      </msup>
                    </mrow>
                  </math>
                </div>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Randomized PCA">
              <div class="sect2" id="idm139656372750352">
                <h2>Randomized PCA</h2>

                <p>If you set the <code>svd_solver</code> hyperparameter to <code>"randomized"</code>, Scikit-Learn uses
                  a stochastic algorithm <a data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-secondary="Incremental PCA" data-startref="pca8ipca" id="idm139656372747328" /><a
                    data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-secondary="Randomized PCA" id="idm139656372691456" /><a data-type="indexterm"
                    data-primary="Randomized PCA" id="idm139656372690544" /><a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="Randomized PCA" id="idm139656372689872" />called
                  <em>Randomized PCA</em> that quickly finds an approximation of the first <em>d</em> principal
                  components. Its computational complexity is <em>O</em>(<em>m</em> × <em>d</em><sup>2</sup>) +
                  <em>O</em>(<em>d</em><sup>3</sup>), instead of <em>O</em>(<em>m</em> × <em>n</em><sup>2</sup>) +
                  <em>O</em>(<em>n</em><sup>3</sup>) for the full SVD approach, so it is dramatically faster than full
                  SVD when <em>d</em> is much <a data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-startref="pca8" id="idm139656372681888" /><a data-type="indexterm"
                    data-primary="dimensionality reduction" data-secondary="PCA (Principal Component Analysis)"
                    data-startref="dr8pca" id="idm139656372680912" />smaller than <em>n</em>:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">rnd_pca</code> <code class="o">=</code> <code class="n">PCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">154</code><code class="p">,</code> <code class="n">svd_solver</code><code class="o">=</code><code class="s2">"randomized"</code><code class="p">)</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">rnd_pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code></pre>

                <p>By default, <code>svd_solver</code> is actually set to <code>"auto"</code>: Scikit-Learn
                  automatically uses the randomized PCA algorithm if <em>m</em> or <em>n</em> is greater than 500 and
                  <em>d</em> is less than 80% of <em>m</em> or <em>n</em>, or else it uses the full SVD approach. If you
                  want to force Scikit-Learn to use full SVD, you can set the <code>svd_solver</code> hyperparameter to
                  <code>"full"</code>.</p>
              </div>
            </section>













            <section data-type="sect2" data-pdf-bookmark="Incremental PCA">
              <div class="sect2" id="idm139656372665872">
                <h2>Incremental PCA</h2>

                <p>One <a data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-secondary="for compression" data-startref="pca8fc" id="idm139656372600832" /><a
                    data-type="indexterm" data-primary="Principal Component Analysis (PCA)"
                    data-secondary="Incremental PCA" id="pca8ipca" />problem with the preceding implementations of PCA
                  is that they require the whole training set to fit in memory in order for the algorithm to run.
                  Fortunately, <em>Incremental PCA</em> (IPCA) algorithms have been developed: you can split the
                  training set into mini-batches and feed an IPCA algorithm one mini-batch at a time. This is useful for
                  large training sets, and also to apply PCA online (i.e., on the fly, as new instances arrive).</p>

                <p>The following code splits the MNIST dataset into 100 mini-batches (using NumPy’s
                  <code>array_split()</code> function) <a data-type="indexterm" data-primary="array_split()"
                    id="idm139656372596384" />and feeds them to Scikit-Learn’s <a
                    href="https://homl.info/32"><code>IncrementalPCA</code> class</a><sup><a data-type="noteref"
                      id="idm139656372594704-marker" href="ch08.xhtml#idm139656372594704">5</a></sup> to <a
                    data-type="indexterm" data-primary="incremental learning" id="idm139656372593872" />reduce the
                  dimensionality of the MNIST dataset down to 154 dimensions (just like before). Note that you must call
                  the <code>partial_fit()</code> <a data-type="indexterm" data-primary="partial_fit()"
                    id="idm139656372592528" />method with each mini-batch rather than <a data-type="indexterm"
                    data-primary="fit()" id="idm139656372591664" />the <code>fit()</code> method with the whole <a
                    data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.decomposition.IncrementalPCA" id="idm139656372590448" />training set:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">IncrementalPCA</code>

<code class="n">n_batches</code> <code class="o">=</code> <code class="mi">100</code>
<code class="n">inc_pca</code> <code class="o">=</code> <code class="n">IncrementalPCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">154</code><code class="p">)</code>
<code class="k">for</code> <code class="n">X_batch</code> <code class="ow">in</code> <code class="n">np</code><code class="o">.</code><code class="n">array_split</code><code class="p">(</code><code class="n">X_train</code><code class="p">,</code> <code class="n">n_batches</code><code class="p">):</code>
    <code class="n">inc_pca</code><code class="o">.</code><code class="n">partial_fit</code><code class="p">(</code><code class="n">X_batch</code><code class="p">)</code>

<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">inc_pca</code><code class="o">.</code><code class="n">transform</code><code class="p">(</code><code class="n">X_train</code><code class="p">)</code></pre>

                <p>Alternatively, you can use NumPy’s <code>memmap</code> <a data-type="indexterm" data-primary="memmap"
                    id="idm139656372642752" />class, which allows you to manipulate a large array stored in a binary
                  file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when
                  it needs it. Since the <code>IncrementalPCA</code> class uses only a small part of the array at any
                  given time, the memory usage remains under control. This makes it possible to call the usual
                  <code>fit()</code> method, as you can see in the following code:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">X_mm</code> <code class="o">=</code> <code class="n">np</code><code class="o">.</code><code class="n">memmap</code><code class="p">(</code><code class="n">filename</code><code class="p">,</code> <code class="n">dtype</code><code class="o">=</code><code class="s2">"float32"</code><code class="p">,</code> <code class="n">mode</code><code class="o">=</code><code class="s2">"readonly"</code><code class="p">,</code> <code class="n">shape</code><code class="o">=</code><code class="p">(</code><code class="n">m</code><code class="p">,</code> <code class="n">n</code><code class="p">))</code>

<code class="n">batch_size</code> <code class="o">=</code> <code class="n">m</code> <code class="o">//</code> <code class="n">n_batches</code>
<code class="n">inc_pca</code> <code class="o">=</code> <code class="n">IncrementalPCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">154</code><code class="p">,</code> <code class="n">batch_size</code><code class="o">=</code><code class="n">batch_size</code><code class="p">)</code>
<code class="n">inc_pca</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X_mm</code><code class="p">)</code></pre>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Kernel PCA">
          <div class="sect1" id="idm139656373137536">
            <h1>Kernel PCA</h1>

            <p>In <a data-type="xref" href="ch05.xhtml#svm_chapter">Chapter 5</a> <a data-type="indexterm"
                data-primary="Principal Component Analysis (PCA)" data-secondary="Kernel PCA (kPCA)" id="pca8kpca" /><a
                data-type="indexterm" data-primary="Kernel PCA (kPCA)" id="kpca8" />we discussed the <a
                data-type="indexterm" data-primary="kernel trick" id="idm139656372488768" />kernel trick, a mathematical
              technique that implicitly maps instances into a very <a data-type="indexterm" data-primary="feature space"
                id="idm139656372487936" />high-dimensional space (called the <em>feature space</em>), enabling nonlinear
              classification and regression with Support Vector Machines. Recall that a linear decision boundary in the
              high-dimensional feature space corresponds to a complex nonlinear decision boundary in the <em>original
                space</em>.</p>

            <p>It turns out that the same trick can be applied to PCA, making it possible to perform complex nonlinear
              projections for dimensionality reduction. This is called <a href="https://homl.info/33"><em>Kernel
                  PCA</em> (kPCA)</a>.<sup><a data-type="noteref" id="idm139656372484624-marker"
                  href="ch08.xhtml#idm139656372484624">6</a></sup> It is often good at preserving clusters of instances
              after projection, or sometimes even unrolling datasets that lie close to a twisted manifold.</p>

            <p>For example, <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.decomposition.KernelPCA" id="skldecomkpcach8" />the following code uses
              Scikit-Learn’s <code>KernelPCA</code> class to perform kPCA with an RBF kernel (see <a data-type="xref"
                href="ch05.xhtml#svm_chapter">Chapter 5</a> for more details about the RBF kernel and the other
              kernels):</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.decomposition</code> <code class="kn">import</code> <code class="n">KernelPCA</code>

<code class="n">rbf_pca</code> <code class="o">=</code> <code class="n">KernelPCA</code><code class="p">(</code><code class="n">n_components</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code> <code class="n">kernel</code><code class="o">=</code><code class="s2">"rbf"</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">0.04</code><code class="p">)</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">rbf_pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

            <p><a data-type="xref" href="#kernel_pca_plot">Figure 8-10</a> shows the Swiss roll, reduced to two
              dimensions using a linear kernel (equivalent to simply using the <code>PCA</code> class), an RBF kernel,
              and a sigmoid kernel (Logistic).</p>

            <figure>
              <div id="kernel_pca_plot" class="figure">
                <img src="mlst_0810.png" alt="mlst 0810" width="3157" height="1087" />
                <h6><span class="label">Figure 8-10. </span>Swiss roll reduced to 2D using kPCA with various kernels
                </h6>
              </div>
            </figure>








            <section data-type="sect2" data-pdf-bookmark="Selecting a Kernel and Tuning Hyperparameters">
              <div class="sect2" id="idm139656372339840">
                <h2>Selecting a Kernel and Tuning Hyperparameters</h2>

                <p>As kPCA is an unsupervised learning algorithm, there is no obvious performance measure to help you
                  select the best kernel and hyperparameter values. However, dimensionality reduction is often a
                  preparation step for a supervised learning task (e.g., classification), so you can simply use grid
                  search to select the kernel and hyperparameters that lead to the best performance on that task. For
                  example, the following code creates a two-step pipeline, first reducing dimensionality to two
                  dimensions using kPCA, then applying Logistic Regression for classification. Then it uses
                  <code>GridSearchCV</code> <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.model_selection.GridSearchCV" id="idm139656372337136" /><a
                    data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.linear_model.LogisticRegression" id="idm139656372336160" /><a
                    data-type="indexterm" data-primary="Scikit-Learn" data-secondary="sklearn.pipeline.Pipeline"
                    id="idm139656372335248" /><a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.decomposition.KernelPCA" data-startref="skldecomkpcach8"
                    id="idm139656372334336" />to find the best kernel and gamma value for kPCA in order to get the best
                  classification accuracy at the end of the pipeline:</p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.model_selection</code> <code class="kn">import</code> <code class="n">GridSearchCV</code>
<code class="kn">from</code> <code class="nn">sklearn.linear_model</code> <code class="kn">import</code> <code class="n">LogisticRegression</code>
<code class="kn">from</code> <code class="nn">sklearn.pipeline</code> <code class="kn">import</code> <code class="n">Pipeline</code>

<code class="n">clf</code> <code class="o">=</code> <code class="n">Pipeline</code><code class="p">([</code>
        <code class="p">(</code><code class="s2">"kpca"</code><code class="p">,</code> <code class="n">KernelPCA</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">)),</code>
        <code class="p">(</code><code class="s2">"log_reg"</code><code class="p">,</code> <code class="n">LogisticRegression</code><code class="p">())</code>
    <code class="p">])</code>

<code class="n">param_grid</code> <code class="o">=</code> <code class="p">[{</code>
        <code class="s2">"kpca__gamma"</code><code class="p">:</code> <code class="n">np</code><code class="o">.</code><code class="n">linspace</code><code class="p">(</code><code class="mf">0.03</code><code class="p">,</code> <code class="mf">0.05</code><code class="p">,</code> <code class="mi">10</code><code class="p">),</code>
        <code class="s2">"kpca__kernel"</code><code class="p">:</code> <code class="p">[</code><code class="s2">"rbf"</code><code class="p">,</code> <code class="s2">"sigmoid"</code><code class="p">]</code>
    <code class="p">}]</code>

<code class="n">grid_search</code> <code class="o">=</code> <code class="n">GridSearchCV</code><code class="p">(</code><code class="n">clf</code><code class="p">,</code> <code class="n">param_grid</code><code class="p">,</code> <code class="n">cv</code><code class="o">=</code><code class="mi">3</code><code class="p">)</code>
<code class="n">grid_search</code><code class="o">.</code><code class="n">fit</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">y</code><code class="p">)</code></pre>

                <p>The best kernel and hyperparameters are then available through the <code>best_params_</code>
                  variable:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="k">print</code><code class="p">(</code><code class="n">grid_search</code><code class="o">.</code><code class="n">best_params_</code><code class="p">)</code>
<code class="go">{'kpca__gamma': 0.043333333333333335, 'kpca__kernel': 'rbf'}</code></pre>

                <p>Another approach, this time entirely unsupervised, is to select the kernel and hyperparameters that
                  yield the lowest reconstruction error. However, reconstruction is not as easy as with linear PCA.
                  Here’s why. <a data-type="xref" href="#kernel_pca_diagram">Figure 8-11</a> shows the original Swiss
                  roll 3D dataset (top left), and the resulting 2D dataset after kPCA is applied using an RBF kernel
                  (top right). Thanks to the kernel trick, this is mathematically equivalent to mapping the training set
                  to an infinite-dimensional <a data-type="indexterm" data-primary="feature space"
                    id="idm139656372267072" />feature space (bottom right) using <a data-type="indexterm"
                    data-primary="feature maps" id="idm139656372266336" />the <em>feature map</em> φ, then projecting
                  the transformed training set down to 2D using linear PCA. Notice that if we could invert the linear
                  PCA step for a given instance in the reduced space, the reconstructed point would lie in feature
                  space, not in the original space (e.g., like the one represented by an x in the diagram). Since the
                  feature space is infinite-dimensional, we cannot compute the reconstructed point, and therefore we
                  cannot compute the true reconstruction error. Fortunately, it is possible to find a point in the
                  original space that would map close to the reconstructed point. This is called the <a
                    data-type="indexterm" data-primary="reconstruction pre-image"
                    id="idm139656372264544" />reconstruction <em>pre-image</em>. Once you have this pre-image, you can
                  measure its squared distance to the original instance. You can then select the kernel and
                  hyperparameters that minimize this reconstruction pre-image error.</p>

                <figure>
                  <div id="kernel_pca_diagram" class="figure">
                    <img src="mlst_0811.png" alt="mlst 0811" width="1746" height="1404" />
                    <h6><span class="label">Figure 8-11. </span>Kernel PCA and the reconstruction pre-image error</h6>
                  </div>
                </figure>

                <p>You may be wondering how to perform this reconstruction. One solution is to train a supervised
                  regression model, with the projected instances as the training set and the original instances as the
                  targets. Scikit-Learn will do this automatically if you <a data-type="indexterm"
                    data-primary="Scikit-Learn" data-secondary="sklearn.decomposition.KernelPCA"
                    id="idm139656372155792" /><a data-type="indexterm" data-primary="fit_inverse_transform="
                    id="idm139656372154848" />set <code>fit_inverse_transform=True</code>, as shown in the following
                  code:<sup><a data-type="noteref" id="idm139656372153632-marker"
                      href="ch08.xhtml#idm139656372153632">7</a></sup></p>

                <pre data-type="programlisting"
                  data-code-language="python"><code class="n">rbf_pca</code> <code class="o">=</code> <code class="n">KernelPCA</code><code class="p">(</code><code class="n">n_components</code> <code class="o">=</code> <code class="mi">2</code><code class="p">,</code> <code class="n">kernel</code><code class="o">=</code><code class="s2">"rbf"</code><code class="p">,</code> <code class="n">gamma</code><code class="o">=</code><code class="mf">0.0433</code><code class="p">,</code>
                    <code class="n">fit_inverse_transform</code><code class="o">=</code><code class="bp">True</code><code class="p">)</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">rbf_pca</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code>
<code class="n">X_preimage</code> <code class="o">=</code> <code class="n">rbf_pca</code><code class="o">.</code><code class="n">inverse_transform</code><code class="p">(</code><code class="n">X_reduced</code><code class="p">)</code></pre>
                <div data-type="note" epub:type="note">
                  <h6>Note</h6>
                  <p>By default, <code>fit_inverse_transform=False</code> and <code>KernelPCA</code> has no
                    <code>inverse_transform()</code> method. <a data-type="indexterm" data-primary="inverse_transform()"
                      id="idm139656372229152" />This method only gets created when you set
                    <code>fit_inverse_transform=True</code>.</p>
                </div>

                <p>You can then compute the reconstruction <a data-type="indexterm" data-primary="Scikit-Learn"
                    data-secondary="sklearn.metrics.mean_squared_error()" id="idm139656372227424" />pre-image error:</p>

                <pre data-type="programlisting" data-code-language="pycon"><code class="gp">&gt;&gt;&gt; </code><code class="kn">from</code> <code class="nn">sklearn.metrics</code> <code class="kn">import</code> <code class="n">mean_squared_error</code>
<code class="gp">&gt;&gt;&gt; </code><code class="n">mean_squared_error</code><code class="p">(</code><code class="n">X</code><code class="p">,</code> <code class="n">X_preimage</code><code class="p">)</code>
<code class="go">32.786308795766132</code></pre>

                <p>Now you can use grid search with cross-validation to find the kernel and hyperparameters that
                  minimize this pre-image reconstruction <a data-type="indexterm"
                    data-primary="Principal Component Analysis (PCA)" data-secondary="Kernel PCA"
                    data-startref="pca8kpca" id="idm139656372214976" /><a data-type="indexterm"
                    data-primary="Kernel PCA (kPCA)" data-startref="kpca8" id="idm139656372213888" />error.</p>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="LLE">
          <div class="sect1" id="idm139656372339216">
            <h1>LLE</h1>

            <p><a href="https://homl.info/35"><em>Locally Linear Embedding</em></a> (LLE)<sup><a data-type="noteref"
                  id="idm139656372210064-marker" href="ch08.xhtml#idm139656372210064">8</a></sup> is <a
                data-type="indexterm" data-primary="dimensionality reduction"
                data-secondary="LLE (Locally Linear Embedding)" id="dr8lle" /><a data-type="indexterm"
                data-primary="LLE (Locally Linear Embedding)" id="lle8" />another very powerful <em>nonlinear
                dimensionality reduction</em> (NLDR) <a data-type="indexterm"
                data-primary="nonlinear dimensionality reduction (NLDR)"
                data-seealso="Kernel PCA; LLE (Locally Linear Embedding)" id="idm139656372206656" />technique. It is a
              <a data-type="indexterm" data-primary="Manifold Learning" id="idm139656372205552" />Manifold Learning
              technique that does not rely on projections like the previous algorithms. In a nutshell, LLE works by
              first measuring how each training instance linearly relates to its closest neighbors (c.n.), and then
              looking for a low-dimensional representation of the training set where these local relationships are best
              preserved (more details shortly). This makes it particularly good at unrolling twisted manifolds,
              especially when there is not too much noise.</p>

            <p>For example, the following code uses Scikit-Learn’s <code>LocallyLinearEmbedding</code> class <a
                data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.manifold.LocallyLinearEmbedding" id="sklmllech11" />to unroll the Swiss roll.
              The resulting 2D dataset is shown in <a data-type="xref" href="#lle_unrolling_plot">Figure 8-12</a>. As
              you can see, the Swiss roll is completely unrolled and the distances between instances are locally well
              preserved. However, distances are not preserved on a larger scale: the left part of the unrolled Swiss
              roll is stretched, while the right part is squeezed. Nevertheless, LLE did a pretty good job at modeling
              the manifold.</p>

            <pre data-type="programlisting"
              data-code-language="python"><code class="kn">from</code> <code class="nn">sklearn.manifold</code> <code class="kn">import</code> <code class="n">LocallyLinearEmbedding</code>

<code class="n">lle</code> <code class="o">=</code> <code class="n">LocallyLinearEmbedding</code><code class="p">(</code><code class="n">n_components</code><code class="o">=</code><code class="mi">2</code><code class="p">,</code> <code class="n">n_neighbors</code><code class="o">=</code><code class="mi">10</code><code class="p">)</code>
<code class="n">X_reduced</code> <code class="o">=</code> <code class="n">lle</code><code class="o">.</code><code class="n">fit_transform</code><code class="p">(</code><code class="n">X</code><code class="p">)</code></pre>

            <figure class="smallerninety">
              <div id="lle_unrolling_plot" class="figure">
                <img src="mlst_0812.png" alt="mlst 0812" width="1652" height="1096" />
                <h6><span class="label">Figure 8-12. </span>Unrolled Swiss roll using LLE</h6>
              </div>
            </figure>

            <p>Here’s how <a data-type="indexterm" data-primary="Scikit-Learn"
                data-secondary="sklearn.manifold.LocallyLinearEmbedding" data-startref="sklmllech11"
                id="idm139656372110368" />LLE works: first, for each training instance
              <strong>x</strong><sup>(<em>i</em>)</sup>, the algorithm identifies its <em>k</em> closest neighbors (in
              the preceding code <em>k</em> = 10), then tries to reconstruct <strong>x</strong><sup>(<em>i</em>)</sup>
              as a linear function of these neighbors. More specifically, it finds the weights
              <em>w</em><sub><em>i,j</em></sub> such that the squared distance between
              <strong>x</strong><sup>(<em>i</em>)</sup> and <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                  <msubsup>
                    <mo>∑</mo>
                    <mrow>
                      <mi>j</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mi>m</mi>
                  </msubsup>
                  <mrow>
                    <msub>
                      <mi>w</mi>
                      <mrow>
                        <mi>i</mi>
                        <mo>,</mo>
                        <mi>j</mi>
                      </mrow>
                    </msub>
                    <msup>
                      <mi mathvariant="bold">x</mi>
                      <mrow>
                        <mo>(</mo>
                        <mi>j</mi>
                        <mo>)</mo>
                      </mrow>
                    </msup>
                  </mrow>
                </mrow>
              </math> is as small as possible, assuming <em>w</em><sub><em>i,j</em></sub> = 0 if
              <strong>x</strong><sup>(<em>j</em>)</sup> is not one of the <em>k</em> closest neighbors of
              <strong>x</strong><sup>(<em>i</em>)</sup>. Thus the first step of LLE is the constrained optimization
              problem described in <a data-type="xref" href="#lle_first_step">Equation 8-4</a>, where <strong>W</strong>
              is the weight matrix containing all the weights <em>w</em><sub><em>i,j</em></sub>. The second constraint
              simply normalizes the weights for each training instance <strong>x</strong><sup>(<em>i</em>)</sup>.</p>
            <div class="pagebreak-before less_space" id="lle_first_step" data-type="equation">
              <h5><span class="label">Equation 8-4. </span>LLE step 1: linearly modeling local relationships</h5>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mtable displaystyle="true">
                  <mtr>
                    <mtd />
                    <mtd columnalign="left">
                      <mrow>
                        <mover accent="true">
                          <mi mathvariant="bold">W</mi>
                          <mo>^</mo>
                        </mover>
                        <mo>=</mo>
                        <munder>
                          <mo form="prefix">argmin</mo>
                          <mi mathvariant="bold">W</mi>
                        </munder>
                        <mstyle scriptlevel="0" displaystyle="true">
                          <munderover>
                            <mo>∑</mo>
                            <mrow>
                              <mi>i</mi>
                              <mo>=</mo>
                              <mn>1</mn>
                            </mrow>
                            <mi>m</mi>
                          </munderover>
                        </mstyle>
                        <msup>
                          <mfenced separators="" open="(" close=")">
                            <msup>
                              <mi mathvariant="bold">x</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>i</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                            <mo>-</mo>
                            <munderover>
                              <mo>∑</mo>
                              <mrow>
                                <mi>j</mi>
                                <mo>=</mo>
                                <mn>1</mn>
                              </mrow>
                              <mi>m</mi>
                            </munderover>
                            <msub>
                              <mi>w</mi>
                              <mrow>
                                <mi>i</mi>
                                <mo>,</mo>
                                <mi>j</mi>
                              </mrow>
                            </msub>
                            <msup>
                              <mi mathvariant="bold">x</mi>
                              <mrow>
                                <mo>(</mo>
                                <mi>j</mi>
                                <mo>)</mo>
                              </mrow>
                            </msup>
                          </mfenced>
                          <mn>2</mn>
                        </msup>
                      </mrow>
                    </mtd>
                  </mtr>
                  <mtr>
                    <mtd />
                    <mtd columnalign="left">
                      <mrow>
                        <mtext>subject</mtext>
                        <mspace width="4.pt" />
                        <mtext>to</mtext>
                        <mspace width="4.pt" />
                        <mfenced separators="" open="{" close="">
                          <mtable>
                            <mtr>
                              <mtd columnalign="left">
                                <mrow>
                                  <msub>
                                    <mi>w</mi>
                                    <mrow>
                                      <mi>i</mi>
                                      <mo>,</mo>
                                      <mi>j</mi>
                                    </mrow>
                                  </msub>
                                  <mo>=</mo>
                                  <mn>0</mn>
                                </mrow>
                              </mtd>
                              <mtd columnalign="left">
                                <mrow>
                                  <mtext>if</mtext>
                                  <mspace width="4.pt" />
                                  <msup>
                                    <mi mathvariant="bold">x</mi>
                                    <mrow>
                                      <mo>(</mo>
                                      <mi>j</mi>
                                      <mo>)</mo>
                                    </mrow>
                                  </msup>
                                  <mspace width="4.pt" />
                                  <mtext>is</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>not</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>one</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>of</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>the</mtext>
                                  <mspace width="4.pt" />
                                  <mi>k</mi>
                                  <mspace width="4.pt" />
                                  <mtext>c.n.</mtext>
                                  <mspace width="4.pt" />
                                  <mtext>of</mtext>
                                  <mspace width="4.pt" />
                                  <msup>
                                    <mi mathvariant="bold">x</mi>
                                    <mrow>
                                      <mo>(</mo>
                                      <mi>i</mi>
                                      <mo>)</mo>
                                    </mrow>
                                  </msup>
                                </mrow>
                              </mtd>
                            </mtr>
                            <mtr>
                              <mtd columnalign="left">
                                <mrow>
                                  <munderover>
                                    <mo>∑</mo>
                                    <mrow>
                                      <mi>j</mi>
                                      <mo>=</mo>
                                      <mn>1</mn>
                                    </mrow>
                                    <mi>m</mi>
                                  </munderover>
                                  <msub>
                                    <mi>w</mi>
                                    <mrow>
                                      <mi>i</mi>
                                      <mo>,</mo>
                                      <mi>j</mi>
                                    </mrow>
                                  </msub>
                                  <mo>=</mo>
                                  <mn>1</mn>
                                </mrow>
                              </mtd>
                              <mtd columnalign="left">
                                <mrow>
                                  <mtext>for</mtext>
                                  <mspace width="4.pt" />
                                  <mi>i</mi>
                                  <mo>=</mo>
                                  <mn>1</mn>
                                  <mo>,</mo>
                                  <mn>2</mn>
                                  <mo>,</mo>
                                  <mo>⋯</mo>
                                  <mo>,</mo>
                                  <mi>m</mi>
                                </mrow>
                              </mtd>
                            </mtr>
                          </mtable>
                        </mfenced>
                      </mrow>
                    </mtd>
                  </mtr>
                </mtable>
              </math>
            </div>

            <p>After this step, the weight matrix <math xmlns="http://www.w3.org/1998/Math/MathML">
                <mover accent="true">
                  <mi mathvariant="bold">W</mi>
                  <mo>^</mo>
                </mover>
              </math> (containing the weights <math xmlns="http://www.w3.org/1998/Math/MathML"
                alttext="ModifyingAbove w With caret Subscript i comma j">
                <msub>
                  <mover accent="true">
                    <mi>w</mi>
                    <mo>^</mo>
                  </mover>
                  <mrow>
                    <mi>i</mi>
                    <mo>,</mo>
                    <mi>j</mi>
                  </mrow>
                </msub>
              </math>) encodes the local linear relationships between the training instances. Now the second step is to
              map the training instances into a <em>d</em>-dimensional space (where <em>d</em> &lt; <em>n</em>) while
              preserving these local relationships as much as possible. If <strong>z</strong><sup>(<em>i</em>)</sup> is
              the image of <strong>x</strong><sup>(<em>i</em>)</sup> in this <em>d</em>-dimensional space, then we want
              the squared distance between <strong>z</strong><sup>(<em>i</em>)</sup> and <math
                xmlns="http://www.w3.org/1998/Math/MathML">
                <mrow>
                  <msubsup>
                    <mo>∑</mo>
                    <mrow>
                      <mi>j</mi>
                      <mo>=</mo>
                      <mn>1</mn>
                    </mrow>
                    <mi>m</mi>
                  </msubsup>
                  <mrow>
                    <msub>
                      <mover accent="true">
                        <mi>w</mi>
                        <mo>^</mo>
                      </mover>
                      <mrow>
                        <mi>i</mi>
                        <mo>,</mo>
                        <mi>j</mi>
                      </mrow>
                    </msub>
                    <msup>
                      <mi mathvariant="bold">z</mi>
                      <mrow>
                        <mo>(</mo>
                        <mi>j</mi>
                        <mo>)</mo>
                      </mrow>
                    </msup>
                  </mrow>
                </mrow>
              </math> to be as small as possible. This idea leads to the unconstrained optimization problem described in
              <a data-type="xref" href="#lle_second_step">Equation 8-5</a>. It looks very similar to the first step, but
              instead of keeping the instances fixed and finding the optimal weights, we are doing the reverse: keeping
              the weights fixed and finding the optimal position of the instances’ images in the low-dimensional space.
              Note that <strong>Z</strong> is the matrix containing all <strong>z</strong><sup>(<em>i</em>)</sup>.</p>
            <div id="lle_second_step" data-type="equation">
              <h5><span class="label">Equation 8-5. </span>LLE step 2: reducing dimensionality while preserving
                relationships</h5>
              <math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
                <mrow>
                  <mover accent="true">
                    <mi mathvariant="bold">Z</mi>
                    <mo>^</mo>
                  </mover>
                  <mo>=</mo>
                  <munder>
                    <mo form="prefix">argmin</mo>
                    <mi mathvariant="bold">Z</mi>
                  </munder>
                  <mstyle scriptlevel="0" displaystyle="true">
                    <munderover>
                      <mo>∑</mo>
                      <mrow>
                        <mi>i</mi>
                        <mo>=</mo>
                        <mn>1</mn>
                      </mrow>
                      <mi>m</mi>
                    </munderover>
                  </mstyle>
                  <msup>
                    <mfenced separators="" open="(" close=")">
                      <msup>
                        <mi mathvariant="bold">z</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>i</mi>
                          <mo>)</mo>
                        </mrow>
                      </msup>
                      <mo>-</mo>
                      <munderover>
                        <mo>∑</mo>
                        <mrow>
                          <mi>j</mi>
                          <mo>=</mo>
                          <mn>1</mn>
                        </mrow>
                        <mi>m</mi>
                      </munderover>
                      <msub>
                        <mover accent="true">
                          <mi>w</mi>
                          <mo>^</mo>
                        </mover>
                        <mrow>
                          <mi>i</mi>
                          <mo>,</mo>
                          <mi>j</mi>
                        </mrow>
                      </msub>
                      <msup>
                        <mi mathvariant="bold">z</mi>
                        <mrow>
                          <mo>(</mo>
                          <mi>j</mi>
                          <mo>)</mo>
                        </mrow>
                      </msup>
                    </mfenced>
                    <mn>2</mn>
                  </msup>
                </mrow>
              </math>
            </div>

            <p>Scikit-Learn’s LLE implementation has the following computational complexity: <span
                class="keep-together"><em>O</em>(<em>m</em> log(<em>m</em>)<em>n</em> log(<em>k</em>))</span> for
              finding the <em>k</em> nearest neighbors, <em>O</em>(<em>mnk</em><sup>3</sup>) for optimizing the weights,
              and <em>O</em>(<em>dm</em><sup>2</sup>) for constructing the low-dimensional representations.
              Unfortunately, the <em>m</em><sup>2</sup> in the last term makes this algorithm <a data-type="indexterm"
                data-primary="dimensionality reduction" data-secondary="LLE (Locally Linear Embedding)"
                data-startref="dr8lle" id="idm139656371915216" /><a data-type="indexterm"
                data-primary="LLE (Locally Linear Embedding)" data-startref="lle8" id="idm139656371913888" />scale
              poorly to very large datasets.</p>








            <section data-type="sect2" data-pdf-bookmark="Other Dimensionality Reduction Techniques">
              <div class="sect2" id="idm139656371921536">
                <h2>Other Dimensionality Reduction Techniques</h2>

                <p>There are many other dimensionality reduction techniques, several of which are available in
                  Scikit-Learn. Here are some of the most popular:</p>

                <ul>
                  <li>
                    <p><em>Multidimensional Scaling</em> (MDS) <a data-type="indexterm"
                        data-primary="Multidimensional Scaling (MDS)" id="idm139656371910128" /><a data-type="indexterm"
                        data-primary="dimensionality reduction" data-secondary="Multidimensional Scaling"
                        id="dimredmultscalch8" />reduces dimensionality while trying to preserve the distances between
                      the instances (see <a data-type="xref" href="#other_dim_reduction_plot">Figure 8-13</a>).</p>
                  </li>
                  <li>
                    <p><em>Isomap</em> creates <a data-type="indexterm" data-primary="dimensionality reduction"
                        data-secondary="Isomap" id="idm139656371905904" /><a data-type="indexterm" data-primary="Isomap"
                        id="idm139656371904880" />a graph by connecting each instance to its nearest neighbors, then
                      reduces dimensionality while trying to <a data-type="indexterm" data-primary="geodesic distance"
                        id="idm139656371903952" />preserve the <em>geodesic distances</em><sup><a data-type="noteref"
                          id="idm139656371902864-marker" href="ch08.xhtml#idm139656371902864">9</a></sup> between the
                      instances.</p>
                  </li>
                  <li>
                    <p><em>t-Distributed Stochastic Neighbor Embedding</em> (t-SNE) <a data-type="indexterm"
                        data-primary="dimensionality reduction"
                        data-secondary="t-Distributed Stochastic Neighbor Embedding (t-SNE)"
                        id="idm139656371900832" /><a data-type="indexterm"
                        data-primary="t-Distributed Stochastic Neighbor Embedding (t-SNE)"
                        id="idm139656371899744" />reduces dimensionality while trying to keep similar instances close
                      and dissimilar instances apart. It is mostly used for visualization, in particular to visualize
                      clusters of instances in high-dimensional space (e.g., to visualize the MNIST images in 2D).</p>
                  </li>
                  <li>
                    <p><em>Linear Discriminant Analysis</em> (LDA) is <a data-type="indexterm"
                        data-primary="dimensionality reduction" data-secondary="Multidimensional Scaling"
                        data-startref="dimredmultscalch8" id="idm139656371897552" /><a data-type="indexterm"
                        data-primary="Linear Discriminant Analysis (LDA)" id="idm139656371896464" />actually a
                      classification algorithm, but during training it learns the most discriminative axes between the
                      classes, and these axes can then be used to define a <a data-type="indexterm"
                        data-primary="hyperplane" id="idm139656371895552" />hyperplane onto which to project the data.
                      The benefit is that the projection will keep classes as far apart as possible, so LDA is a good
                      technique to reduce dimensionality before running another classification algorithm such as an SVM
                      classifier.</p>
                  </li>
                </ul>

                <figure>
                  <div id="other_dim_reduction_plot" class="figure">
                    <img src="mlst_0813.png" alt="mlst 0813" width="3196" height="1095" />
                    <h6><span class="label">Figure 8-13. </span>Reducing the Swiss roll to 2D using various techniques
                    </h6>
                  </div>
                </figure>
              </div>
            </section>





          </div>
        </section>













        <section data-type="sect1" data-pdf-bookmark="Exercises">
          <div class="sect1" id="idm139656372212112">
            <h1>Exercises</h1>
            <ol>
              <li>
                <p>What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?
                </p>
              </li>
              <li>
                <p>What is the curse of dimensionality?</p>
              </li>
              <li>
                <p>Once a dataset’s dimensionality has been reduced, is it possible to reverse the operation? If so,
                  how? If not, why?</p>
              </li>
              <li>
                <p>Can PCA be used to reduce the dimensionality of a highly nonlinear dataset?</p>
              </li>
              <li>
                <p>Suppose you perform PCA on a 1,000-dimensional dataset, setting the explained variance ratio to 95%.
                  How many dimensions will the resulting dataset have?</p>
              </li>
              <li>
                <p>In what cases would you use vanilla PCA, Incremental PCA, Randomized PCA, or Kernel PCA?</p>
              </li>
              <li>
                <p>How can you evaluate the performance of a dimensionality reduction algorithm on your dataset?</p>
              </li>
              <li>
                <p>Does it make any sense to chain two different dimensionality reduction algorithms?</p>
              </li>
              <li>
                <p>Load the MNIST dataset (introduced in <a data-type="xref"
                    href="ch03.xhtml#classification_chapter">Chapter 3</a>) and split it into a training set and a test
                  set (take the first 60,000 instances for training, and the remaining 10,000 for testing). Train a
                  Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model
                  on the test set. Next, use PCA to reduce the dataset’s dimensionality, with an explained variance
                  ratio of 95%. Train a new Random Forest classifier on the reduced dataset and see how long it takes.
                  Was training much faster? Next evaluate the classifier on the test set: how does it compare to the
                  previous classifier?</p>
              </li>
              <li>
                <p>Use t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib.
                  You can use a scatterplot using 10 different colors to represent each image’s target class.
                  Alternatively, you can write colored digits at the location of each instance, or even plot scaled-down
                  versions of the digit images themselves (if you plot all digits, the visualization will be too
                  cluttered, so you should either draw a random sample or plot an instance only if no other instance has
                  already been plotted at a close distance). You should get a nice visualization with well-separated
                  clusters of digits. Try using other dimensionality reduction algorithms such as PCA, LLE, or MDS and
                  compare the resulting visualizations.</p>
              </li>

            </ol>

            <p>Solutions to these exercises <a data-type="indexterm" data-primary="dimensionality reduction"
                data-startref="dr8" id="idm139656371879232" />are available in Appendix A.</p>
          </div>
        </section>







        <div data-type="footnotes">
          <p data-type="footnote" id="idm139656373315056"><sup><a
                href="ch08.xhtml#idm139656373315056-marker">1</a></sup> Well, four dimensions if you count time, and a
            few more if you are a string theorist.</p>
          <p data-type="footnote" id="idm139656373311600"><sup><a
                href="ch08.xhtml#idm139656373311600-marker">2</a></sup> Watch a rotating tesseract projected into 3D
            space at <a href="https://homl.info/30"><em class="hyperlink">https://homl.info/30</em></a>. Image by
            Wikipedia user NerdBoy1392 (<a href="https://creativecommons.org/licenses/by-sa/3.0/">Creative Commons BY-SA
              3.0</a>). Reproduced from <a href="https://en.wikipedia.org/wiki/Tesseract"><em
                class="hyperlink">https://en.wikipedia.org/wiki/Tesseract</em></a>.</p>
          <p data-type="footnote" id="idm139656373306608"><sup><a
                href="ch08.xhtml#idm139656373306608-marker">3</a></sup> Fun fact: anyone you know is probably an
            extremist in at least one dimension (e.g., how much sugar they put in their coffee), if you consider enough
            dimensions.</p>
          <p data-type="footnote" id="idm139656373120688"><sup><a
                href="ch08.xhtml#idm139656373120688-marker">4</a></sup> “On Lines and Planes of Closest Fit to Systems
            of Points in Space,” K. Pearson (1901).</p>
          <p data-type="footnote" id="idm139656372594704"><sup><a
                href="ch08.xhtml#idm139656372594704-marker">5</a></sup> Scikit-Learn uses the algorithm described in
            “Incremental Learning for Robust Visual Tracking,” D. Ross et al. (2007).</p>
          <p data-type="footnote" id="idm139656372484624"><sup><a
                href="ch08.xhtml#idm139656372484624-marker">6</a></sup> “Kernel Principal Component Analysis,” B.
            Schölkopf, A. Smola, K. Müller (1999).</p>
          <p data-type="footnote" id="idm139656372153632"><sup><a
                href="ch08.xhtml#idm139656372153632-marker">7</a></sup> Scikit-Learn uses the algorithm based on Kernel
            Ridge Regression described in Gokhan H. Bakır, Jason Weston, and Bernhard Scholkopf, <a
              href="https://homl.info/34">“Learning to Find Pre-images”</a> (Tubingen, Germany: Max Planck Institute for
            Biological Cybernetics, 2004).</p>
          <p data-type="footnote" id="idm139656372210064"><sup><a
                href="ch08.xhtml#idm139656372210064-marker">8</a></sup> “Nonlinear Dimensionality Reduction by Locally
            Linear Embedding,” S. Roweis, L. Saul (2000).</p>
          <p data-type="footnote" id="idm139656371902864"><sup><a
                href="ch08.xhtml#idm139656371902864-marker">9</a></sup> The geodesic distance between two nodes in a
            graph is the number of nodes on the shortest path between these nodes.</p>
        </div>
      </div>
    </section>
  </div>



</body>

</html>